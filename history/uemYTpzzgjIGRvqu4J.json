[{
  "history_id" : "3o2m07nkjlh",
  "history_input" : "import sys\nfrom pathlib import Path\nfrom datetime import date\n\nsys.path.append(\"..\")\n\nfrom src_exporters_geowiki import *\nfrom src_exporters_sentinel_geowiki import *\nfrom src_exporters_sentinel_pv_kenya import *\nfrom src_exporters_sentinel_kenya_non_crop import *\nfrom src_exporters_sentinel_region import *\nfrom src_exporters_sentinel_utils import *\n\nfrom scripts_process import *\n\ndef export_geowiki():\n    exporter = GeoWikiExporter(Path(\"../data\"))\n    exporter.export()\n\n\ndef export_geowiki_sentinel_ee():\n    exporter = GeoWikiSentinelExporter(Path(\"../data\"))\n    exporter.export_for_labels(\n        num_labelled_points=None, monitor=False, checkpoint=True)\n\n\ndef export_plant_village_sentinel_ee():\n    exporter = KenyaPVSentinelExporter(Path(\"../data\"))\n    exporter.export_for_labels(\n        num_labelled_points=None, monitor=False, checkpoint=True)\n\n\ndef export_kenya_non_crop():\n    exporter = KenyaNonCropSentinelExporter(Path(\"../data\"))\n    exporter.export_for_labels(\n        num_labelled_points=None, monitor=False, checkpoint=True)\n\n\ndef export_region():\n    exporter = RegionalExporter(Path(\"../data\"))\n    exporter.export_for_region(\n        region_name=\"Busia\",\n        end_date=date(2020, 9, 13),\n        num_timesteps=5,\n        monitor=False,\n        checkpoint=True,\n        metres_per_polygon=None,\n        fast=False,\n    )\n\n\nif __name__ == \"__main__\":\n\tprint(\"starting export_geowiki()...\")\n\texport_geowiki()\n\tprint(\"Done export_geowiki()!\")\n\tprint(\"starting process_geowiki()...\")\n\tprocess_geowiki()\n\tprint(\"Done process_geowiki()!\")\n\tprint(\"starting export_geowiki_sentinel_ee()...this could take a while\")\n\texport_geowiki_sentinel_ee()\n\tprint(\"Done export_geowiki_sentinel_ee()!\")\n\t#export_plant_village_sentinel_ee()\n\t#export_kenya_non_crop()\n\t#export_region()\n",
  "history_output" : "0it [00:00, ?it/s]\n1259it [00:00, 12585.09it/s]\n2750it [00:00, 13951.16it/s]\n4158it [00:00, 14009.69it/s]\n5591it [00:00, 14134.01it/s]\n7005it [00:00, 13050.93it/s]\n8324it [00:00, 12722.50it/s]\n9755it [00:00, 13211.85it/s]\n11220it [00:00, 13651.76it/s]\n12899it [00:00, 14606.46it/s]\n14639it [00:01, 15452.72it/s]\n16192it [00:01, 14000.03it/s]\n17745it [00:01, 14428.02it/s]\n19475it [00:01, 15248.91it/s]\n21022it [00:01, 14009.72it/s]\n22456it [00:01, 13973.85it/s]\n23910it [00:01, 14129.78it/s]\n25340it [00:01, 11828.85it/s]\n26594it [00:01, 11934.83it/s]\n27839it [00:02, 11824.51it/s]\n29243it [00:02, 12423.38it/s]\n30964it [00:02, 13757.52it/s]\n32683it [00:02, 14733.87it/s]\n34405it [00:02, 15449.10it/s]\n35866it [00:02, 13964.36it/s]starting export_geowiki()...\ncrop_all.zip already exists! Skipping\ncrop_con.zip already exists! Skipping\ncrop_exp.zip already exists! Skipping\nloc_all.zip already exists! Skipping\nloc_all_2.zip already exists! Skipping\nloc_con.zip already exists! Skipping\nloc_exp.zip already exists! Skipping\nDone export_geowiki()!\nstarting process_geowiki()...\nDone process_geowiki()!\nstarting export_geowiki_sentinel_ee()...this could take a while\nExporting image for polygon 0 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 1 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 2 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 3 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 4 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 5 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 6 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 7 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 8 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 9 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 10 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 11 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 12 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 13 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 14 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 15 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 16 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 17 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 18 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 19 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 20 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 21 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 22 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 23 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 24 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 25 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 26 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 27 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 28 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 29 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 30 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 31 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 32 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 33 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 34 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 35 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 36 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 37 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 38 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 39 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 40 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 41 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 42 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 43 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 44 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 45 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 46 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 47 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 48 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 49 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 50 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 51 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 52 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 53 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 54 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 55 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 56 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 57 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 58 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 59 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 60 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 61 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 62 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 63 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 64 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 65 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 66 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 67 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 68 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 69 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 70 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 71 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 72 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 73 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 74 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 75 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 76 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 77 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 78 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 79 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 80 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 81 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 82 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 83 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 84 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 85 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 86 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 87 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 88 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 89 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 90 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 91 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 92 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 93 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 94 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 95 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 96 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 97 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 98 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 99 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 100 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 101 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 102 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 103 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 104 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 105 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 106 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 107 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 108 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 109 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 110 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 111 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 112 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 113 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 114 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 115 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 116 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 117 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 118 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 119 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 120 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 121 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 122 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 123 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 124 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 125 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 126 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 127 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 128 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 129 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 130 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 131 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 132 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 133 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 134 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 135 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 136 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 137 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 138 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 139 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 140 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 141 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 142 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 143 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 144 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 145 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 146 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 147 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 148 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 149 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 150 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 151 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 152 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 153 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 154 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 155 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 156 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 157 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 158 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 159 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 160 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 161 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 162 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 163 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 164 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 165 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 166 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 167 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 168 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 169 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 170 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 171 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 172 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 173 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 174 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 175 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 176 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 177 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 178 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 179 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 180 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 181 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 182 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 183 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 184 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 185 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 186 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 187 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 188 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 189 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 190 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 191 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 192 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 193 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 194 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 195 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 196 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 197 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 198 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 199 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 200 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 201 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 202 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 203 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 204 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 205 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 206 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 207 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 208 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 209 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 210 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 211 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 212 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 213 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 214 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 215 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 216 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 217 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 218 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 219 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 220 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 221 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 222 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 223 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 224 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 225 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 226 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 227 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 228 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 229 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 230 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 231 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 232 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 233 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 234 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 235 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 236 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 237 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 238 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 239 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 240 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 241 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 242 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 243 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 244 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 245 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 246 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 247 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 248 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 249 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 250 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 251 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 252 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 253 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 254 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 255 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 256 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 257 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 258 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 259 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 260 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 261 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 262 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 263 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 264 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 265 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 266 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 267 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 268 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 269 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 270 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 271 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 272 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 273 from aggregated images between 2017-03-28 and 2018-03-28\nExporting image for polygon 274 from aggregated images between 2017-03-28 and 2018-03-28/opt/anaconda3/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n  warnings.warn('resource_tracker: There appear to be %d '\n",
  "history_begin_time" : 1645393369118,
  "history_end_time" : 1645393667324,
  "history_notes" : null,
  "history_process" : "gpetwx",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "flt5rpox22e",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_processors_geowiki import *\nfrom src_processors_kenya_non_crop import *\nfrom src_processors_pv_kenya import *\n\ndef process_geowiki():\n    processor = GeoWikiProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_plantvillage():\n    processor = KenyaPVProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_kenya_noncrop():\n    processor = KenyaNonCropProcessor(Path(\"../data\"))\n    processor.process()\n\n\nif __name__ == \"__main__\":\n    process_geowiki()\n    process_plantvillage()\n    process_kenya_noncrop()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"scripts_process.py\", line 27, in <module>\n    process_plantvillage()\n  File \"scripts_process.py\", line 16, in process_plantvillage\n    processor = KenyaPVProcessor(Path(\"../data\"))\n  File \"/Users/uhhmed/gw-workspace/C2ej3sSc5n8u92TFGIdpirlJRP/src_processors_base.py\", line 20, in __init__\n    assert self.raw_folder.exists(), f\"{self.raw_folder} does not exist!\"\nAssertionError: ../data/raw/plant_village_kenya does not exist!\n",
  "history_begin_time" : 1645393668431,
  "history_end_time" : 1645393671812,
  "history_notes" : null,
  "history_process" : "iticjd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5s41jw92llj",
  "history_input" : "from pathlib import Path\n\nfrom typing import Any, Dict\n\n\nclass BaseExporter:\n    r\"\"\"Base for all exporter classes. It creates the appropriate\n    directory in the data dir (``data_dir/raw/{dataset}``).\n\n    All classes which extend this should implement an export function.\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n    default_args_dict: Dict[str, Any] = {}\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n\n        self.data_folder = data_folder\n\n        self.raw_folder = self.data_folder / \"raw\"\n        self.output_folder = self.raw_folder / self.dataset\n        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
  "history_output" : "",
  "history_begin_time" : 1645393670352,
  "history_end_time" : 1645393670483,
  "history_notes" : null,
  "history_process" : "4q2yxd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1qtmze3zc1c",
  "history_input" : "from pathlib import Path\nimport urllib.request\nimport zipfile\n\nfrom src_exporters_base import BaseExporter\n\n\nclass GeoWikiExporter(BaseExporter):\n    r\"\"\"\n    Download the GeoWiki labels\n    \"\"\"\n\n    dataset = \"geowiki_landcover_2017\"\n\n    download_urls = [\n        \"http://store.pangaea.de/Publications/See_2017/crop_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_exp.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all_2.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_exp.zip\",\n    ]\n\n    @staticmethod\n    def download_file(url: str, output_folder: Path, remove_zip: bool = True) -> None:\n\n        filename = url.split(\"/\")[-1]\n        output_path = output_folder / filename\n\n        if output_path.exists():\n            print(f\"{filename} already exists! Skipping\")\n            return None\n\n        print(f\"Downloading {url}\")\n        urllib.request.urlretrieve(url, output_path)\n\n        if filename.endswith(\"zip\"):\n\n            print(f\"Downloaded! Unzipping to {output_folder}\")\n            with zipfile.ZipFile(output_path, \"r\") as zip_file:\n                zip_file.extractall(output_folder)\n\n            if remove_zip:\n                print(\"Deleting zip file\")\n                (output_path).unlink()\n\n    def export(self, remove_zip: bool = False) -> None:\n        r\"\"\"\n        Download the GeoWiki labels\n        :param remove_zip: Whether to remove the zip file once it has been expanded\n        \"\"\"\n        for file_url in self.download_urls:\n            self.download_file(file_url, self.output_folder, remove_zip)\n",
  "history_output" : "",
  "history_begin_time" : 1645393669019,
  "history_end_time" : 1645393669177,
  "history_notes" : null,
  "history_process" : "jonz77",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b9as7xm9w94",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import date\nfrom math import cos, radians\nimport ee\n\nfrom typing import List, Tuple, Union\n\nfrom src_utils import BoundingBox\n\n\ndef date_overlap(start1: date, end1: date, start2: date, end2: date) -> int:\n    overlaps = start1 <= end2 and end1 >= start2\n    if not overlaps:\n        return 0\n    return (min(end1, end2) - max(start1, start2)).days\n\n\ndef metre_per_degree(mid_lat: float) -> Tuple[float, float]:\n    # https://gis.stackexchange.com/questions/75528/understanding-terms-in-length-of-degree-formula\n    # see the link above to explain the magic numbers\n    m_per_deg_lat = 111132.954 - 559.822 * cos(2.0 * mid_lat) + 1.175 * cos(radians(4.0 * mid_lat))\n    m_per_deg_lon = (3.14159265359 / 180) * 6367449 * cos(radians(mid_lat))\n\n    return m_per_deg_lat, m_per_deg_lon\n\n\n@dataclass\nclass EEBoundingBox(BoundingBox):\n    r\"\"\"\n    A bounding box with additional earth-engine specific\n    functionality\n    \"\"\"\n\n    def to_ee_polygon(self) -> ee.Geometry.Polygon:\n        return ee.Geometry.Polygon(\n            [\n                [\n                    [self.min_lon, self.min_lat],\n                    [self.min_lon, self.max_lat],\n                    [self.max_lon, self.max_lat],\n                    [self.max_lon, self.min_lat],\n                ]\n            ]\n        )\n\n    def to_metres(self) -> Tuple[float, float]:\n        r\"\"\"\n        :return: [lat metres, lon metres]\n        \"\"\"\n        # https://gis.stackexchange.com/questions/75528/understanding-terms-in-length-of-degree-formula\n        mid_lat = (self.min_lat + self.max_lat) / 2.0\n        m_per_deg_lat, m_per_deg_lon = metre_per_degree(mid_lat)\n\n        delta_lat = self.max_lat - self.min_lat\n        delta_lon = self.max_lon - self.min_lon\n\n        return delta_lat * m_per_deg_lat, delta_lon * m_per_deg_lon\n\n    def to_polygons(self, metres_per_patch: int = 3300) -> List[ee.Geometry.Polygon]:\n\n        lat_metres, lon_metres = self.to_metres()\n\n        num_cols = int(lon_metres / metres_per_patch)\n        num_rows = int(lat_metres / metres_per_patch)\n\n        print(f\"Splitting into {num_cols} columns and {num_rows} rows\")\n\n        lon_size = (self.max_lon - self.min_lon) / num_cols\n        lat_size = (self.max_lat - self.min_lat) / num_rows\n\n        output_polygons: List[ee.Geometry.Polygon] = []\n\n        cur_lon = self.min_lon\n        while cur_lon < self.max_lon:\n            cur_lat = self.min_lat\n            while cur_lat < self.max_lat:\n                output_polygons.append(\n                    ee.Geometry.Polygon(\n                        [\n                            [\n                                [cur_lon, cur_lat],\n                                [cur_lon, cur_lat + lat_size],\n                                [cur_lon + lon_size, cur_lat + lat_size],\n                                [cur_lon + lon_size, cur_lat],\n                            ]\n                        ]\n                    )\n                )\n                cur_lat += lat_size\n            cur_lon += lon_size\n\n        return output_polygons\n\n\ndef bounding_box_from_centre(\n    mid_lat: float, mid_lon: float, surrounding_metres: Union[int, Tuple[int, int]]\n) -> EEBoundingBox:\n\n    m_per_deg_lat, m_per_deg_lon = metre_per_degree(mid_lat)\n\n    if isinstance(surrounding_metres, int):\n        surrounding_metres = (surrounding_metres, surrounding_metres)\n\n    surrounding_lat, surrounding_lon = surrounding_metres\n\n    deg_lat = surrounding_lat / m_per_deg_lat\n    deg_lon = surrounding_lon / m_per_deg_lon\n\n    max_lat, min_lat = mid_lat + deg_lat, mid_lat - deg_lat\n    max_lon, min_lon = mid_lon + deg_lon, mid_lon - deg_lon\n\n    return EEBoundingBox(max_lon=max_lon, min_lon=min_lon, max_lat=max_lat, min_lat=min_lat)\n\n\ndef bounding_box_to_earth_engine_bounding_box(bounding_box: BoundingBox,) -> EEBoundingBox:\n    return EEBoundingBox(\n        max_lat=bounding_box.max_lat,\n        min_lat=bounding_box.min_lat,\n        max_lon=bounding_box.max_lon,\n        min_lon=bounding_box.min_lon,\n    )\n\n\ndef cancel_all_tasks() -> None:\n\n    ee.Initialize()\n\n    tasks = ee.batch.Task.list()\n    print(f\"Cancelling up to {len(tasks)} tasks\")\n    # Cancel running and ready tasks\n    for task in tasks:\n        task_id = task.status()[\"id\"]\n        task_state = task.status()[\"state\"]\n        if task_state == \"RUNNING\" or task_state == \"READY\":\n            task.cancel()\n            print(f\"Task {task_id} cancelled\")\n        else:\n            print(f\"Task {task_id} state is {task_state}\")\n",
  "history_output" : "",
  "history_begin_time" : 1645393671195,
  "history_end_time" : 1645393671816,
  "history_notes" : null,
  "history_process" : "dmf4zo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "uk34wegt1jn",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudFreeKeepThresh,\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(calcCloudStats)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef calcCloudStats(img):\n    imgPoly = ee.Algorithms.GeometryConstructors.Polygon(\n        ee.Geometry(img.get(\"system:footprint\")).coordinates()\n    )\n\n    roi = ee.Geometry(img.get(\"ROI\"))\n\n    intersection = roi.intersection(imgPoly, ee.ErrorMargin(0.5))\n    cloudMask = img.select([\"cloudScore\"]).gt(cloudThresh).clip(roi).rename(\"cloudMask\")\n\n    cloudAreaImg = cloudMask.multiply(ee.Image.pixelArea())\n\n    stats = cloudAreaImg.reduceRegion(\n        **{\"reducer\": ee.Reducer.sum(), \"geometry\": roi, \"scale\": 10, \"maxPixels\": 1e12}\n    )\n\n    cloudPercent = ee.Number(stats.get(\"cloudMask\")).divide(imgPoly.area()).multiply(100)\n    coveragePercent = ee.Number(intersection.area()).divide(roi.area()).multiply(100)\n    cloudPercentROI = ee.Number(stats.get(\"cloudMask\")).divide(roi.area()).multiply(100)\n\n    img = img.set(\"CLOUDY_PERCENTAGE\", cloudPercent)\n    img = img.set(\"ROI_COVERAGE_PERCENT\", coveragePercent)\n    img = img.set(\"CLOUDY_PERCENTAGE_ROI\", cloudPercentROI)\n\n    return img\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n    score = (\n        score.reproject(\"EPSG:4326\", None, 20)\n        .focal_min(**{\"radius\": erodePixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .focal_max(**{\"radius\": dilationPixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .reproject(\"EPSG:4326\", None, 20)\n    )\n\n    return score\n\n\ndef mergeCollection(imgC):\n    # Select the best images, which are below the cloud free threshold, sort them in reverse order\n    # (worst on top) for mosaicing\n    best = imgC.filterMetadata(\"CLOUDY_PERCENTAGE\", \"less_than\", cloudFreeKeepThresh).sort(\n        \"CLOUDY_PERCENTAGE\", False\n    )\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n\n    # Add the quality mosaic to fill in any missing areas of the ROI which aren't covered by good\n    # images\n    newC = ee.ImageCollection.fromImages([filtered, best.mosaic()])\n\n    return ee.Image(newC.mosaic())\n",
  "history_output" : "",
  "history_begin_time" : 1645393369118,
  "history_end_time" : 1645393371507,
  "history_notes" : null,
  "history_process" : "nph7xo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5po0tf4nab9",
  "history_input" : "# These are algorithm settings for the cloud filtering algorithm\nimage_collection = \"COPERNICUS/S2\"\n\n# Ranges from 0-1.Lower value will mask more pixels out.\n# Generally 0.1-0.3 works well with 0.2 being used most commonly\ncloudThresh = 0.2\n# Height of clouds to use to project cloud shadows\ncloudHeights = [200, 10000, 250]\n# Sum of IR bands to include as shadows within TDOM and the\n# shadow shift method (lower number masks out less)\nirSumThresh = 0.3\nndviThresh = -0.1\n# Pixels to reduce cloud mask and dark shadows by to reduce inclusion\n# of single-pixel comission errors\nerodePixels = 1.5\ndilationPixels = 3\n\n# images with less than this many cloud pixels will be used with normal\n# mosaicing (most recent on top)\ncloudFreeKeepThresh = 3\n\nBANDS = [\n    \"B1\",\n    \"B2\",\n    \"B3\",\n    \"B4\",\n    \"B5\",\n    \"B6\",\n    \"B7\",\n    \"B8\",\n    \"B8A\",\n    \"B9\",\n    \"B10\",\n    \"B11\",\n    \"B12\",\n]\n",
  "history_output" : "",
  "history_begin_time" : 1645393369146,
  "history_end_time" : 1645393369259,
  "history_notes" : null,
  "history_process" : "jsnayl",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "oh45lf4h6o6",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PIXEL_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5), optimization=\"boxcar\"\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5))\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n\n    def erode(img, distance):\n        d = (\n            img.Not()\n            .unmask(1)\n            .fastDistanceTransform(30)\n            .sqrt()\n            .multiply(ee.Image.pixelArea().sqrt())\n        )\n        return img.updateMask(d.gt(distance))\n\n    def dilate(img, distance):\n        d = img.fastDistanceTransform(30).sqrt().multiply(ee.Image.pixelArea().sqrt())\n        return d.lt(distance)\n\n    score = score.reproject(\"EPSG:4326\", None, 20)\n    score = erode(score, erodePixels)\n    score = dilate(score, dilationPixels)\n\n    return score.reproject(\"EPSG:4326\", None, 20)\n\n\ndef mergeCollection(imgC):\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n    return filtered\n",
  "history_output" : "",
  "history_begin_time" : 1645393369118,
  "history_end_time" : 1645393371513,
  "history_notes" : null,
  "history_process" : "yqt708",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "npv2v3hddb8",
  "history_input" : "r\"\"\"\nFunctions shared by both the fast and slow\ncloudfree algorithm\n\"\"\"\nimport ee\nfrom datetime import date\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\n\nfrom typing import Union\n\n\ndef combine_bands(current, previous):\n    # Transforms an Image Collection with 1 band per Image into a single Image with items as bands\n    # Author: Jamie Vleeshouwer\n\n    # Rename the band\n    previous = ee.Image(previous)\n    current = current.select(BANDS)\n    # Append it to the result (Note: only return current item on first element/iteration)\n    return ee.Algorithms.If(\n        ee.Algorithms.IsEqual(previous, None), current, previous.addBands(ee.Image(current)),\n    )\n\n\ndef export(\n    image: ee.Image, region: ee.Geometry, filename: str, drive_folder: str, monitor: bool = False,\n) -> ee.batch.Export:\n\n    task = ee.batch.Export.image(\n        image.clip(region),\n        filename,\n        {\"scale\": 10, \"region\": region, \"maxPixels\": 1e13, \"driveFolder\": drive_folder},\n    )\n\n    try:\n        task.start()\n    except ee.ee_exception.EEException as e:\n        print(f\"Task not started! Got exception {e}\")\n        return task\n\n    if monitor:\n        monitor_task(task)\n\n    return task\n\n\ndef date_to_string(input_date: Union[date, str]) -> str:\n    if isinstance(input_date, str):\n        return input_date\n    else:\n        assert isinstance(input_date, date)\n        return input_date.strftime(\"%Y-%m-%d\")\n\n\ndef monitor_task(task: ee.batch.Export) -> None:\n\n    while task.status()[\"state\"] in [\"READY\", \"RUNNING\"]:\n        print(task.status())\n        # print(f\"Running: {task.status()['state']}\")\n\n\ndef rescale(img, exp, thresholds):\n    return (\n        img.expression(exp, {\"img\": img})\n        .subtract(thresholds[0])\n        .divide(thresholds[1] - thresholds[0])\n    )\n",
  "history_output" : "",
  "history_begin_time" : 1645393368826,
  "history_end_time" : 1645393371506,
  "history_notes" : null,
  "history_process" : "q5a232",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1vlzi8ns9ce",
  "history_input" : "from pathlib import Path\nfrom src_utils import set_seed\n\n\nclass BaseProcessor:\n    r\"\"\"Base for all processor classes. It creates the appropriate\n    directory in the data dir (``data_dir/processed/{dataset}``).\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n\n    def __init__(self, data_folder: Path) -> None:\n\n        set_seed()\n        self.data_folder = data_folder\n        self.raw_folder = self.data_folder / \"raw\" / self.dataset\n        assert self.raw_folder.exists(), f\"{self.raw_folder} does not exist!\"\n\n        self.output_folder = self.data_folder / \"processed\" / self.dataset\n        self.output_folder.mkdir(exist_ok=True, parents=True)\n",
  "history_output" : "",
  "history_begin_time" : 1645393668754,
  "history_end_time" : 1645393669738,
  "history_notes" : null,
  "history_process" : "6nnond",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7zgo62wocqp",
  "history_input" : "import pandas as pd\n\nfrom src_processors_base import BaseProcessor\n\n\nclass GeoWikiProcessor(BaseProcessor):\n\n    dataset = \"geowiki_landcover_2017\"\n\n    def load_raw_data(self, participants: str) -> pd.DataFrame:\n\n        participants_to_file_labels = {\n            \"all\": \"all\",\n            \"students\": \"con\",\n            \"experts\": \"exp\",\n        }\n\n        file_label = participants_to_file_labels.get(participants, participants)\n        assert (\n            file_label in participants_to_file_labels.values()\n        ), f\"Unknown participant {file_label}\"\n\n        return pd.read_csv(\n            self.raw_folder / f\"loc_{file_label}{'_2' if file_label == 'all' else ''}.txt\",\n            sep=\"\\t\",\n        )\n\n    def process(self, participants: str = \"all\") -> None:\n\n        location_data = self.load_raw_data(participants)\n\n        # first, we find the mean sumcrop calculated per location\n        mean_per_location = (\n            location_data[[\"location_id\", \"sumcrop\", \"loc_cent_X\", \"loc_cent_Y\"]]\n            .groupby(\"location_id\")\n            .mean()\n        )\n\n        # then, we rename the columns\n        mean_per_location = mean_per_location.rename(\n            {\"loc_cent_X\": \"lon\", \"loc_cent_Y\": \"lat\", \"sumcrop\": \"mean_sumcrop\"},\n            axis=\"columns\",\n            errors=\"raise\",\n        )\n        # then, we turn it into an xarray with x and y as indices\n        output_xr = (\n            mean_per_location.reset_index().set_index([\"lon\", \"lat\"])[\"mean_sumcrop\"].to_xarray()\n        )\n\n        # and save\n        output_xr.to_netcdf(self.output_folder / \"data.nc\")\n",
  "history_output" : "",
  "history_begin_time" : 1645393670426,
  "history_end_time" : 1645393670486,
  "history_notes" : null,
  "history_process" : "m6v1cg",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6b99mwbjrn2",
  "history_input" : "# The probability threshold to use to label GeoWiki\n# instances as crop / not_crop (since the GeoWiki labels are a mean crop probability, as\n# assigned by several labellers). In addition, this is the threshold used when calculating\n# metrics which require binary predictions, such as accuracy score\nPROBABILITY_THRESHOLD = 0.5\n",
  "history_output" : "",
  "history_begin_time" : 1645393373273,
  "history_end_time" : 1645393373361,
  "history_notes" : null,
  "history_process" : "nt17bz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5mglq1c4m5l",
  "history_input" : "import torch\nimport numpy as np\nimport random\n\nfrom dataclasses import dataclass\n\n\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n\n@dataclass\nclass BoundingBox:\n\n    min_lon: float\n    max_lon: float\n    min_lat: float\n    max_lat: float\n\n\nSTR2BB = {\n    \"Kenya\": BoundingBox(min_lon=33.501, max_lon=42.283, min_lat=-5.202, max_lat=6.002),\n    \"Busia\": BoundingBox(\n        min_lon=33.88389587402344,\n        min_lat=-0.04119872691853491,\n        max_lon=34.44007873535156,\n        max_lat=0.7779454563313616,\n    ),\n}\n",
  "history_output" : "",
  "history_begin_time" : 1645393373516,
  "history_end_time" : 1645393374683,
  "history_notes" : null,
  "history_process" : "o5t3jb",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gdzfahzzjiv",
  "history_input" : "import pandas as pd\nimport xarray as xr\nfrom datetime import date\nfrom tqdm import tqdm\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass GeoWikiSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_geowiki\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        geowiki = self.data_folder / \"processed\" / GeoWikiExporter.dataset / \"data.nc\"\n        assert geowiki.exists(), \"GeoWiki processor must be run to load labels\"\n        return xr.open_dataset(geowiki).to_dataframe().dropna().reset_index()\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        start_date: date = date(2017, 3, 28),\n        end_date: date = date(2018, 3, 28),\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        Run the GeoWiki exporter. For each label, the exporter will export\n        int( (end_date - start_date).days / days_per_timestep) timesteps of data,\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param start_date: The start data of the data export\n        :param end_date: The end date of the data export\n        :param num_labelled_points: (Optional) The number of labelled points to export.\n        :param surrounding_metres: The number of metres surrounding each labelled point to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        assert start_date >= self.min_date, f\"Sentinel data does not exist before {self.min_date}\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        for idx, bounding_box in enumerate(bounding_boxes_to_download):\n            self._export_for_polygon(\n                polygon=bounding_box.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1645393375698,
  "history_end_time" : 1645393381027,
  "history_notes" : null,
  "history_process" : "mw544v",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1m8spss6xpm",
  "history_input" : "from abc import ABC, abstractmethod\nfrom datetime import date, timedelta\nfrom pathlib import Path\nimport pandas as pd\nimport ee\n\nfrom src_exporters_sentinel_cloudfree_cloudfree import *\nfrom src_exporters_sentinel_cloudfree_fast import get_single_image as get_single_image_fast\nfrom src_exporters_base import BaseExporter\nfrom src_exporters_sentinel_cloudfree_utils import *\n\n\nfrom typing import List, Union\n\n\nclass BaseSentinelExporter(BaseExporter, ABC):\n\n    r\"\"\"\n    Download cloud free sentinel data for countries,\n    where countries are defined by the simplified large scale\n    international boundaries.\n    \"\"\"\n\n    dataset: str\n    min_date = date(2017, 3, 28)\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n        super().__init__(data_folder)\n        try:\n            ee.Initialize()\n        except Exception:\n            print(\"This code doesn't work unless you have authenticated your earthengine account\")\n\n        self.labels = self.load_labels()\n\n    @abstractmethod\n    def load_labels(self) -> pd.DataFrame:\n        raise NotImplementedError\n\n    def _export_for_polygon(\n        self,\n        polygon: ee.Geometry.Polygon,\n        polygon_identifier: Union[int, str],\n        start_date: date,\n        end_date: date,\n        days_per_timestep: int,\n        checkpoint: bool,\n        monitor: bool,\n        fast: bool,\n    ) -> None:\n\n        if fast:\n            export_func = get_single_image_fast\n        else:\n            export_func = get_single_image\n\n        cur_date = start_date\n        cur_end_date = cur_date + timedelta(days=days_per_timestep)\n\n        image_collection_list: List[ee.Image] = []\n\n        print(\n            f\"Exporting image for polygon {polygon_identifier} from \"\n            f\"aggregated images between {str(cur_date)} and {str(end_date)}\"\n        )\n        filename = f\"{polygon_identifier}_{str(cur_date)}_{str(end_date)}\"\n\n        if checkpoint and (self.output_folder / f\"{filename}.tif\").exists():\n            print(\"File already exists! Skipping\")\n            return None\n\n        while cur_end_date <= end_date:\n\n            image_collection_list.append(\n                export_func(region=polygon, start_date=cur_date, end_date=cur_end_date)\n            )\n            cur_date += timedelta(days=days_per_timestep)\n            cur_end_date += timedelta(days=days_per_timestep)\n\n        # now, we want to take our image collection and append the bands into a single image\n        imcoll = ee.ImageCollection(image_collection_list)\n        img = ee.Image(imcoll.iterate(combine_bands))\n\n        # and finally, export the image\n        export(\n            image=img,\n            region=polygon,\n            filename=filename,\n            drive_folder=self.dataset,\n            monitor=monitor,\n        )\n",
  "history_output" : "",
  "history_begin_time" : 1645393373524,
  "history_end_time" : 1645393375114,
  "history_notes" : null,
  "history_process" : "vxuj3q",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "if6swqb7q2r",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nfrom datetime import timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_kenya_non_crop import KenyaNonCropProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass KenyaNonCropSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_kenya_non_crop\"\n\n    # data collection date\n    data_date = date(2020, 4, 16)\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        non_crop = self.data_folder / \"processed\" / KenyaNonCropProcessor.dataset / \"data.geojson\"\n        assert non_crop.exists(), \"Kenya non crop processor must be run to load labels\"\n        return geopandas.read_file(non_crop)[[\"lat\", \"lon\"]]\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                ),\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        start_date = self.data_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            self._export_for_polygon(\n                polygon=bounding_info.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=self.data_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1645393379534,
  "history_end_time" : 1645393379707,
  "history_notes" : null,
  "history_process" : "nlb6f5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tc03bv74s8p",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nimport numpy as np\nfrom datetime import datetime, timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_pv_kenya import KenyaPVProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre, date_overlap\n\nfrom typing import Dict, Optional, List, Tuple\n\n\nclass KenyaPVSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_plant_village_kenya\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        plantvillage = self.data_folder / \"processed\" / KenyaPVProcessor.dataset / \"data.geojson\"\n        assert plantvillage.exists(), \"Plant Village processor must be run to load labels\"\n        return geopandas.read_file(plantvillage)[\n            [\"lat\", \"lon\", \"index\", \"planting_date\", \"harvest_date\"]\n        ]\n\n    @staticmethod\n    def overlapping_year(\n        end_month: int, num_days: int, harvest_date: date, planting_date: date\n    ) -> Tuple[Optional[int], Optional[int]]:\n        r\"\"\"\n        Return the end_year of the most overlapping years\n        \"\"\"\n        harvest_year = harvest_date.year\n\n        overlap_dict: Dict[int, int] = {}\n\n        for diff in range(-1, 2):\n            end_date = date(harvest_year + diff, end_month, 1)\n\n            if end_date > datetime.now().date():\n                continue\n            else:\n                overlap_dict[harvest_year + diff] = date_overlap(\n                    planting_date, harvest_date, end_date - timedelta(days=num_days), end_date,\n                )\n        if len(overlap_dict) > 0:\n            return max(overlap_dict.items(), key=lambda x: x[1])\n        else:\n            # sometimes the harvest date is in the future? in which case\n            # we will just skip the datapoint for now\n            return None, None\n\n    def labels_to_bounding_boxes(\n        self,\n        num_labelled_points: Optional[int],\n        surrounding_metres: int,\n        end_month_day: Optional[Tuple[int, int]],\n        num_days: int,\n    ) -> List[Tuple[int, EEBoundingBox, date, Optional[int]]]:\n\n        output: List[Tuple[int, EEBoundingBox, date, Optional[int]]] = []\n\n        if end_month_day is not None:\n            end_month: Optional[int]\n            end_day: Optional[int]\n            end_month, end_day = end_month_day\n        else:\n            end_month = end_day = None\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            try:\n                harvest_date = datetime.strptime(row[\"harvest_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n            except ValueError:\n                continue\n\n            # this is only used if end_month is not None\n            overlapping_days: Optional[int] = 0\n            if end_month is not None:\n                planting_date = datetime.strptime(row[\"planting_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n\n                end_year, overlapping_days = self.overlapping_year(\n                    end_month, num_days, harvest_date, planting_date\n                )\n\n                if end_year is None:\n                    continue\n\n                if end_day is None:\n                    # if no end_day is passed, we will take the first month\n                    end_day = 1\n                harvest_date = date(end_year, end_month, end_day)\n\n            output.append(\n                (\n                    row[\"index\"],\n                    bounding_box_from_centre(\n                        mid_lat=row[\"lat\"],\n                        mid_lon=row[\"lon\"],\n                        surrounding_metres=surrounding_metres,\n                    ),\n                    harvest_date,\n                    overlapping_days,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def get_start_and_end_dates(\n        self, harvest_date: date, days_per_timestep: int, num_timesteps: int\n    ) -> Optional[Tuple[date, date]]:\n\n        if harvest_date < self.min_date:\n            print(\"Harvest date < min date - skipping\")\n            return None\n        else:\n            start_date = max(\n                harvest_date - timedelta(days_per_timestep * num_timesteps), self.min_date,\n            )\n            end_date = start_date + timedelta(days_per_timestep * num_timesteps)\n\n            return start_date, end_date\n\n    def export_for_labels(\n        self,\n        end_month_day: Optional[Tuple[int, int]] = (4, 16),\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param end_month_day: The final month-day to use. If None is passed, the harvest date\n            will be used. Default = (4, 16)\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points,\n            surrounding_metres=surrounding_metres,\n            end_month_day=end_month_day,\n            num_days=days_per_timestep * num_timesteps,\n        )\n\n        if end_month_day is not None:\n            print(\n                f\"Average overlapping days between planting to harvest and \"\n                f\"export dates: {np.mean([x[3] for x in bounding_boxes_to_download])}\"\n            )\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            harvest_date = bounding_info[-2]\n\n            dates = self.get_start_and_end_dates(harvest_date, days_per_timestep, num_timesteps)\n\n            if dates is not None:\n\n                self._export_for_polygon(\n                    polygon=bounding_info[1].to_ee_polygon(),\n                    polygon_identifier=bounding_info[0],\n                    start_date=dates[0],\n                    end_date=dates[1],\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n",
  "history_output" : "",
  "history_begin_time" : 1645393381531,
  "history_end_time" : 1645393384297,
  "history_notes" : null,
  "history_process" : "i4s7l1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dpeoijk8gkc",
  "history_input" : "from datetime import date, timedelta\nimport pandas as pd\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_sentinel_utils import bounding_box_to_earth_engine_bounding_box\nfrom src_utils import STR2BB\n\nfrom typing import Optional\n\n\nclass RegionalExporter(BaseSentinelExporter):\n    r\"\"\"\n    This is useful if you are trying to export\n    full regions for predictions\n    \"\"\"\n\n    dataset = \"earth_engine_region_busia_partial_slow_cloudfree\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # We don't need any labels for this exporter,\n        # so we can return an empty dataframe\n        return pd.DataFrame()\n\n    def export_for_region(\n        self,\n        region_name: str,\n        end_date: date,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        checkpoint: bool = True,\n        monitor: bool = True,\n        metres_per_polygon: Optional[int] = 10000,\n        fast: bool = True,\n    ):\n        r\"\"\"\n        Run the regional exporter. For each label, the exporter will export\n        data from (end_date - timedelta(days=days_per_timestep * num_timesteps)) to end_date\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param region_name: The name of the region to export. This must be defined in\n            src.utils.STR2BB\n        :param end_date: The end date of the data export\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param num_timesteps: The number of timesteps to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param metres_per_polygon: Whether to split the export of a large region into smaller\n            boxes of (max) area metres_per_polygon * metres_per_polygon. It is better to instead\n            split the area once it has been exported\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        start_date = end_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        region = bounding_box_to_earth_engine_bounding_box(STR2BB[region_name])\n\n        if metres_per_polygon is not None:\n\n            regions = region.to_polygons(metres_per_patch=metres_per_polygon)\n\n            for idx, region in enumerate(regions):\n                self._export_for_polygon(\n                    polygon=region,\n                    polygon_identifier=f\"{idx}-{region_name}\",\n                    start_date=start_date,\n                    end_date=end_date,\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n        else:\n            self._export_for_polygon(\n                polygon=region.to_ee_polygon(),\n                polygon_identifier=region_name,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1645393375374,
  "history_end_time" : 1645393378513,
  "history_notes" : null,
  "history_process" : "9c0ch9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8kbnptfxnzb",
  "history_input" : "import geopandas\nfrom pathlib import Path\nimport pandas as pd\nfrom pyproj import Transformer\n\nfrom src_processors_base import BaseProcessor\n\nfrom typing import List\n\n\nclass KenyaNonCropProcessor(BaseProcessor):\n\n    dataset = \"kenya_non_crop\"\n\n    @staticmethod\n    def process_set(filepath: Path, latlon: bool, reversed: bool) -> geopandas.GeoDataFrame:\n        df = geopandas.read_file(filepath)\n\n        x, y = df.geometry.centroid.x.values, df.geometry.centroid.y.values\n\n        if reversed:\n            x, y = y, x\n\n        if not latlon:\n\n            transformer = Transformer.from_crs(crs_from=32636, crs_to=4326)\n\n            lat, lon = transformer.transform(xx=x, yy=y)\n            df[\"lat\"] = lat\n            df[\"lon\"] = lon\n        else:\n            df[\"lat\"] = x\n            df[\"lon\"] = y\n\n        df[\"index\"] = df.index\n\n        return df\n\n    def process(self) -> None:\n\n        filepaths = [\n            (self.raw_folder / \"noncrop_labels_v2\", False, False),\n            (self.raw_folder / \"noncrop_labels_set2\", False, False),\n            (self.raw_folder / \"2019_gepro_noncrop\", True, True),\n            (self.raw_folder / \"noncrop_water_kenya_gt\", True, True),\n            (self.raw_folder / \"noncrop_kenya_gt\", True, True),\n        ]\n\n        dfs: List[geopandas.GeoDataFrame] = []\n        for filepath, is_latlon, is_reversed in filepaths:\n            dfs.append(self.process_set(filepath, is_latlon, is_reversed))\n\n        df = pd.concat(dfs)\n        df.to_file(self.output_folder / \"data.geojson\", driver=\"GeoJSON\")\n",
  "history_output" : "",
  "history_begin_time" : 1645393374526,
  "history_end_time" : 1645393374685,
  "history_notes" : null,
  "history_process" : "9x9elz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1qop76tj1yw",
  "history_input" : "import geopandas\nimport pandas as pd\nimport numpy as np\n\nfrom src_processors_base import BaseProcessor\n\nfrom typing import List\n\n\nclass KenyaPVProcessor(BaseProcessor):\n    dataset = \"plant_village_kenya\"\n\n    def process(self) -> None:\n\n        subfolders = [f\"ref_african_crops_kenya_01_labels_0{i}\" for i in [0, 1, 2]]\n\n        dfs: List[geopandas.GeoDataFrame] = []\n        for subfolder in subfolders:\n            df = geopandas.read_file(\n                self.raw_folder / \"ref_african_crops_kenya_01_labels\" / subfolder / \"labels.geojson\"\n            )\n            df = df.rename(\n                columns={\n                    \"Latitude\": \"lat\",\n                    \"Longitude\": \"lon\",\n                    \"Planting Date\": \"planting_date\",\n                    \"Estimated Harvest Date\": \"harvest_date\",\n                    \"Crop1\": \"label\",\n                    \"Survey Date\": \"collection_date\",\n                }\n            )\n            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"]).dt.to_pydatetime()\n            df[\"harvest_date\"] = pd.to_datetime(df[\"harvest_date\"]).dt.to_pydatetime()\n            df[\"collection_date\"] = pd.to_datetime(df[\"collection_date\"]).dt.to_pydatetime()\n            df[\"is_crop\"] = np.where((df[\"label\"] == \"Fallowland\"), 0, 1)\n            df = df.to_crs(\"EPSG:4326\")\n            dfs.append(df)\n\n        df = pd.concat(dfs)\n        df = df.reset_index(drop=True)\n        df[\"index\"] = df.index\n        df.to_file(self.output_folder / \"data.geojson\", driver=\"GeoJSON\")\n",
  "history_output" : "",
  "history_begin_time" : 1645393376691,
  "history_end_time" : 1645393378524,
  "history_notes" : null,
  "history_process" : "m9myzm",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5r6g8cr3evj",
  "history_input" : "from pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cartopy.crs as ccrs\nfrom datetime import datetime\nimport xarray as xr\n\n\nfrom src_engineer_base import BaseEngineer\n\n\ndef sentinel_as_tci(sentinel_ds: xr.DataArray, scale: bool = True) -> xr.DataArray:\n    r\"\"\"\n    Get a True Colour Image from Sentinel data exported from Earth Engine\n    :param sentinel_ds: The sentinel data, exported from Earth Engine\n    :param scale: Whether or not to add the factor 10,000 scale\n    :return: A dataframe with true colour bands\n    \"\"\"\n\n    band2idx = {band: idx for idx, band in enumerate(sentinel_ds.attrs[\"band_descriptions\"])}\n\n    tci_bands = [\"B4\", \"B3\", \"B2\"]\n    tci_indices = [band2idx[band] for band in tci_bands]\n    if scale:\n        return sentinel_ds.isel(band=tci_indices) / 10000 * 2.5\n    else:\n        return sentinel_ds.isel(band=tci_indices) * 2.5\n\n\ndef plot_results(model_preds: xr.Dataset, tci_path: Path, savepath: Path, prefix: str = \"\") -> None:\n\n    multi_output = len(model_preds.data_vars) > 1\n\n    tci = sentinel_as_tci(\n        BaseEngineer.load_tif(tci_path, start_date=datetime(2020, 1, 1), days_per_timestep=30),\n        scale=False,\n    ).isel(time=-1)\n\n    tci = tci.sortby(\"x\").sortby(\"y\")\n    model_preds = model_preds.sortby(\"lat\").sortby(\"lon\")\n\n    plt.clf()\n    fig, ax = plt.subplots(1, 3, figsize=(20, 7.5), subplot_kw={\"projection\": ccrs.PlateCarree()})\n\n    fig.suptitle(\n        f\"Model results for tile with bottom left corner:\"\n        f\"\\nat latitude {float(model_preds.lat.min())}\"\n        f\"\\n and longitude {float(model_preds.lon.min())}\",\n        fontsize=15,\n    )\n    # ax 1 - original\n    img_extent_1 = (tci.x.min(), tci.x.max(), tci.y.min(), tci.y.max())\n    img = np.clip(np.moveaxis(tci.values, 0, -1), 0, 1)\n\n    ax[0].set_title(\"True colour image\")\n    ax[0].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict = {\n        \"origin\": \"upper\",\n        \"extent\": img_extent_1,\n        \"transform\": ccrs.PlateCarree(),\n    }\n\n    if multi_output:\n        mask = np.argmax(model_preds.to_array().values, axis=0)\n\n        # currently, we have 10 classes (at most). It seems unlikely we will go\n        # above 20\n        args_dict[\"cmap\"] = plt.cm.get_cmap(\"tab20\", len(model_preds.data_vars))\n    else:\n        mask = model_preds.prediction_0\n        args_dict.update({\"vmin\": 0, \"vmax\": 1})\n\n    # ax 2 - mask\n    ax[1].set_title(\"Mask\")\n    im = ax[1].imshow(mask, **args_dict)\n\n    # finally, all together\n    ax[2].set_title(\"Mask on top of the true colour image\")\n    ax[2].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict[\"alpha\"] = 0.3\n    if not multi_output:\n        mask = mask > 0.5\n    ax[2].imshow(mask, **args_dict)\n\n    colorbar_args = {\n        \"ax\": ax.ravel().tolist(),\n    }\n\n    if multi_output:\n        # This function formatter will replace integers with target names\n        formatter = plt.FuncFormatter(lambda val, loc: list(model_preds.data_vars)[val])\n        colorbar_args.update({\"ticks\": range(len(model_preds.data_vars)), \"format\": formatter})\n\n    # We must be sure to specify the ticks matching our target names\n    fig.colorbar(im, **colorbar_args)\n\n    plt.savefig(savepath / f\"results_{prefix}{tci_path.name}.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_analysis.py\", line 4, in <module>\n    import cartopy.crs as ccrs\nModuleNotFoundError: No module named 'cartopy'\n",
  "history_begin_time" : 1645393379130,
  "history_end_time" : 1645393379703,
  "history_notes" : null,
  "history_process" : "qdzo28",
  "host_id" : "100001",
  "indicator" : "Done"
}]
