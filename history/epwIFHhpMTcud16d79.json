[{
  "history_id" : "q94arcbuyjo",
  "history_input" : "import sys\nfrom pathlib import Path\nfrom datetime import date\nimport os\n\nsys.path.append(\"..\")\n\nfrom src_exporters_geowiki import *\nfrom src_exporters_sentinel_geowiki import *\nfrom src_exporters_sentinel_pv_kenya import *\nfrom src_exporters_sentinel_kenya_non_crop import *\nfrom src_exporters_sentinel_region import *\nfrom src_exporters_sentinel_utils import *\n\n\n\n\ndef export_geowiki():\n    if len(os.listdir('../data/raw/geowiki_landcover_2017')) == 0:\n        exporter = GeoWikiExporter(Path(\"../data\"))\n        exporter.export()\n\n\ndef export_geowiki_sentinel_ee():\n    if len(os.listdir('../data/raw/earth_engine_geowiki')) == 0:\n        exporter = GeoWikiSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_plant_village_sentinel_ee():\n    if len(os.listdir('../data/raw/earth_engine_plant_village_kenya')) == 0:\n        exporter = KenyaPVSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_kenya_non_crop():\n    if len(os.listdir('../data/raw/earth_engine_kenya_non_crop')) == 0:\n        exporter = KenyaNonCropSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_region():\n    if len(os.listdir('../data/raw/earth_engine_region_busia_partial_slow_cloudfree')) == 0:\n        exporter = RegionalExporter(Path(\"../data\"))\n        exporter.export_for_region(\n            region_name=\"Busia\",\n            end_date=date(2020, 9, 13),\n            num_timesteps=5,\n            monitor=False,\n            checkpoint=True,\n            metres_per_polygon=None,\n            fast=False,\n        )\n\n\nif __name__ == \"__main__\":\n    print(\"starting export_geowiki()...\")\n    export_geowiki()\n    print(\"Done export_geowiki()!\")\n    print(\"starting process_geowiki()...\")\n    #process_geowiki()\n    print(\"Done process_geowiki()!\")\n    print(\"starting export_geowiki_sentinel_ee()...this could take a while\")\n    export_geowiki_sentinel_ee()\n    print(\"Done export_geowiki_sentinel_ee()!\")\n    print(\"starting process_plantvillage()...\")\n    #process_plantvillage()\n    print(\"Done process_plantvillage()!\")\n    print(\"starting export_plant_village_sentinel_ee()...\")\n    export_plant_village_sentinel_ee()\n    print(\"Done export_plant_village_sentinel_ee()!\")\n    print(\"starting process_kenya_noncrop()...\")\n    #process_kenya_noncrop()\n    print(\"Done process_kenya_noncrop()!\")\n    print(\"starting export_kenya_non_crop()...\")\n    #export_kenya_non_crop()\n    print(\"Done export_kenya_non_crop()!\")\n    print(\"starting export_region()...\")\n    #export_region()\n    print(\"Done export_region()!\")\n",
  "history_output" : "starting export_geowiki()...\nDone export_geowiki()!\nstarting process_geowiki()...\nDone process_geowiki()!\nstarting export_geowiki_sentinel_ee()...this could take a while\nDone export_geowiki_sentinel_ee()!\nstarting process_plantvillage()...\nDone process_plantvillage()!\nstarting export_plant_village_sentinel_ee()...\nDone export_plant_village_sentinel_ee()!\nstarting process_kenya_noncrop()...\nDone process_kenya_noncrop()!\nstarting export_kenya_non_crop()...\nDone export_kenya_non_crop()!\nstarting export_region()...\nDone export_region()!\n",
  "history_begin_time" : 1647347130223,
  "history_end_time" : 1647347134561,
  "history_notes" : null,
  "history_process" : "gpetwx",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "k6lyxiboljn",
  "history_input" : "from pathlib import Path\n\nfrom typing import Any, Dict\n\n\nclass BaseExporter:\n    r\"\"\"Base for all exporter classes. It creates the appropriate\n    directory in the data dir (``data_dir/raw/{dataset}``).\n\n    All classes which extend this should implement an export function.\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n    default_args_dict: Dict[str, Any] = {}\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n\n        self.data_folder = data_folder\n\n        self.raw_folder = self.data_folder / \"raw\"\n        self.output_folder = self.raw_folder / self.dataset\n        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
  "history_output" : "",
  "history_begin_time" : 1647347135033,
  "history_end_time" : 1647347135129,
  "history_notes" : null,
  "history_process" : "4q2yxd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bhgfqf9kbuc",
  "history_input" : "from pathlib import Path\nimport urllib.request\nimport zipfile\n\nfrom src_exporters_base import BaseExporter\n\n\nclass GeoWikiExporter(BaseExporter):\n    r\"\"\"\n    Download the GeoWiki labels\n    \"\"\"\n\n    dataset = \"geowiki_landcover_2017\"\n\n    download_urls = [\n        \"http://store.pangaea.de/Publications/See_2017/crop_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_exp.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all_2.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_exp.zip\",\n    ]\n\n    @staticmethod\n    def download_file(url: str, output_folder: Path, remove_zip: bool = True) -> None:\n\n        filename = url.split(\"/\")[-1]\n        output_path = output_folder / filename\n\n        if output_path.exists():\n            print(f\"{filename} already exists! Skipping\")\n            return None\n\n        print(f\"Downloading {url}\")\n        urllib.request.urlretrieve(url, output_path)\n\n        if filename.endswith(\"zip\"):\n\n            print(f\"Downloaded! Unzipping to {output_folder}\")\n            with zipfile.ZipFile(output_path, \"r\") as zip_file:\n                zip_file.extractall(output_folder)\n\n            if remove_zip:\n                print(\"Deleting zip file\")\n                (output_path).unlink()\n\n    def export(self, remove_zip: bool = False) -> None:\n        r\"\"\"\n        Download the GeoWiki labels\n        :param remove_zip: Whether to remove the zip file once it has been expanded\n        \"\"\"\n        for file_url in self.download_urls:\n            self.download_file(file_url, self.output_folder, remove_zip)\n",
  "history_output" : "",
  "history_begin_time" : 1647347135572,
  "history_end_time" : 1647347135734,
  "history_notes" : null,
  "history_process" : "jonz77",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "23gdpbsrpww",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import date\nfrom math import cos, radians\nimport ee\n\nfrom typing import List, Tuple, Union\n\nfrom src_utils import BoundingBox\n\n\ndef date_overlap(start1: date, end1: date, start2: date, end2: date) -> int:\n    overlaps = start1 <= end2 and end1 >= start2\n    if not overlaps:\n        return 0\n    return (min(end1, end2) - max(start1, start2)).days\n\n\ndef metre_per_degree(mid_lat: float) -> Tuple[float, float]:\n    # https://gis.stackexchange.com/questions/75528/understanding-terms-in-length-of-degree-formula\n    # see the link above to explain the magic numbers\n    m_per_deg_lat = 111132.954 - 559.822 * cos(2.0 * mid_lat) + 1.175 * cos(radians(4.0 * mid_lat))\n    m_per_deg_lon = (3.14159265359 / 180) * 6367449 * cos(radians(mid_lat))\n\n    return m_per_deg_lat, m_per_deg_lon\n\n\n@dataclass\nclass EEBoundingBox(BoundingBox):\n    r\"\"\"\n    A bounding box with additional earth-engine specific\n    functionality\n    \"\"\"\n\n    def to_ee_polygon(self) -> ee.Geometry.Polygon:\n        return ee.Geometry.Polygon(\n            [\n                [\n                    [self.min_lon, self.min_lat],\n                    [self.min_lon, self.max_lat],\n                    [self.max_lon, self.max_lat],\n                    [self.max_lon, self.min_lat],\n                ]\n            ]\n        )\n\n    def to_metres(self) -> Tuple[float, float]:\n        r\"\"\"\n        :return: [lat metres, lon metres]\n        \"\"\"\n        # https://gis.stackexchange.com/questions/75528/understanding-terms-in-length-of-degree-formula\n        mid_lat = (self.min_lat + self.max_lat) / 2.0\n        m_per_deg_lat, m_per_deg_lon = metre_per_degree(mid_lat)\n\n        delta_lat = self.max_lat - self.min_lat\n        delta_lon = self.max_lon - self.min_lon\n\n        return delta_lat * m_per_deg_lat, delta_lon * m_per_deg_lon\n\n    def to_polygons(self, metres_per_patch: int = 3300) -> List[ee.Geometry.Polygon]:\n\n        lat_metres, lon_metres = self.to_metres()\n\n        num_cols = int(lon_metres / metres_per_patch)\n        num_rows = int(lat_metres / metres_per_patch)\n\n        print(f\"Splitting into {num_cols} columns and {num_rows} rows\")\n\n        lon_size = (self.max_lon - self.min_lon) / num_cols\n        lat_size = (self.max_lat - self.min_lat) / num_rows\n\n        output_polygons: List[ee.Geometry.Polygon] = []\n\n        cur_lon = self.min_lon\n        while cur_lon < self.max_lon:\n            cur_lat = self.min_lat\n            while cur_lat < self.max_lat:\n                output_polygons.append(\n                    ee.Geometry.Polygon(\n                        [\n                            [\n                                [cur_lon, cur_lat],\n                                [cur_lon, cur_lat + lat_size],\n                                [cur_lon + lon_size, cur_lat + lat_size],\n                                [cur_lon + lon_size, cur_lat],\n                            ]\n                        ]\n                    )\n                )\n                cur_lat += lat_size\n            cur_lon += lon_size\n\n        return output_polygons\n\n\ndef bounding_box_from_centre(\n    mid_lat: float, mid_lon: float, surrounding_metres: Union[int, Tuple[int, int]]\n) -> EEBoundingBox:\n\n    m_per_deg_lat, m_per_deg_lon = metre_per_degree(mid_lat)\n\n    if isinstance(surrounding_metres, int):\n        surrounding_metres = (surrounding_metres, surrounding_metres)\n\n    surrounding_lat, surrounding_lon = surrounding_metres\n\n    deg_lat = surrounding_lat / m_per_deg_lat\n    deg_lon = surrounding_lon / m_per_deg_lon\n\n    max_lat, min_lat = mid_lat + deg_lat, mid_lat - deg_lat\n    max_lon, min_lon = mid_lon + deg_lon, mid_lon - deg_lon\n\n    return EEBoundingBox(max_lon=max_lon, min_lon=min_lon, max_lat=max_lat, min_lat=min_lat)\n\n\ndef bounding_box_to_earth_engine_bounding_box(bounding_box: BoundingBox,) -> EEBoundingBox:\n    return EEBoundingBox(\n        max_lat=bounding_box.max_lat,\n        min_lat=bounding_box.min_lat,\n        max_lon=bounding_box.max_lon,\n        min_lon=bounding_box.min_lon,\n    )\n\n\ndef cancel_all_tasks() -> None:\n\n    ee.Initialize()\n\n    tasks = ee.batch.Task.list()\n    print(f\"Cancelling up to {len(tasks)} tasks\")\n    # Cancel running and ready tasks\n    for task in tasks:\n        task_id = task.status()[\"id\"]\n        task_state = task.status()[\"state\"]\n        if task_state == \"RUNNING\" or task_state == \"READY\":\n            task.cancel()\n            print(f\"Task {task_id} cancelled\")\n        else:\n            print(f\"Task {task_id} state is {task_state}\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347136567,
  "history_end_time" : 1647347138230,
  "history_notes" : null,
  "history_process" : "dmf4zo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ibgr2jmyhca",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudFreeKeepThresh,\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(calcCloudStats)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef calcCloudStats(img):\n    imgPoly = ee.Algorithms.GeometryConstructors.Polygon(\n        ee.Geometry(img.get(\"system:footprint\")).coordinates()\n    )\n\n    roi = ee.Geometry(img.get(\"ROI\"))\n\n    intersection = roi.intersection(imgPoly, ee.ErrorMargin(0.5))\n    cloudMask = img.select([\"cloudScore\"]).gt(cloudThresh).clip(roi).rename(\"cloudMask\")\n\n    cloudAreaImg = cloudMask.multiply(ee.Image.pixelArea())\n\n    stats = cloudAreaImg.reduceRegion(\n        **{\"reducer\": ee.Reducer.sum(), \"geometry\": roi, \"scale\": 10, \"maxPixels\": 1e12}\n    )\n\n    cloudPercent = ee.Number(stats.get(\"cloudMask\")).divide(imgPoly.area()).multiply(100)\n    coveragePercent = ee.Number(intersection.area()).divide(roi.area()).multiply(100)\n    cloudPercentROI = ee.Number(stats.get(\"cloudMask\")).divide(roi.area()).multiply(100)\n\n    img = img.set(\"CLOUDY_PERCENTAGE\", cloudPercent)\n    img = img.set(\"ROI_COVERAGE_PERCENT\", coveragePercent)\n    img = img.set(\"CLOUDY_PERCENTAGE_ROI\", cloudPercentROI)\n\n    return img\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n    score = (\n        score.reproject(\"EPSG:4326\", None, 20)\n        .focal_min(**{\"radius\": erodePixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .focal_max(**{\"radius\": dilationPixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .reproject(\"EPSG:4326\", None, 20)\n    )\n\n    return score\n\n\ndef mergeCollection(imgC):\n    # Select the best images, which are below the cloud free threshold, sort them in reverse order\n    # (worst on top) for mosaicing\n    best = imgC.filterMetadata(\"CLOUDY_PERCENTAGE\", \"less_than\", cloudFreeKeepThresh).sort(\n        \"CLOUDY_PERCENTAGE\", False\n    )\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n\n    # Add the quality mosaic to fill in any missing areas of the ROI which aren't covered by good\n    # images\n    newC = ee.ImageCollection.fromImages([filtered, best.mosaic()])\n\n    return ee.Image(newC.mosaic())\n",
  "history_output" : "",
  "history_begin_time" : 1647347130445,
  "history_end_time" : 1647347132122,
  "history_notes" : null,
  "history_process" : "nph7xo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "q8wlpf71wot",
  "history_input" : "# These are algorithm settings for the cloud filtering algorithm\nimage_collection = \"COPERNICUS/S2\"\n\n# Ranges from 0-1.Lower value will mask more pixels out.\n# Generally 0.1-0.3 works well with 0.2 being used most commonly\ncloudThresh = 0.2\n# Height of clouds to use to project cloud shadows\ncloudHeights = [200, 10000, 250]\n# Sum of IR bands to include as shadows within TDOM and the\n# shadow shift method (lower number masks out less)\nirSumThresh = 0.3\nndviThresh = -0.1\n# Pixels to reduce cloud mask and dark shadows by to reduce inclusion\n# of single-pixel comission errors\nerodePixels = 1.5\ndilationPixels = 3\n\n# images with less than this many cloud pixels will be used with normal\n# mosaicing (most recent on top)\ncloudFreeKeepThresh = 3\n\nBANDS = [\n    \"B1\",\n    \"B2\",\n    \"B3\",\n    \"B4\",\n    \"B5\",\n    \"B6\",\n    \"B7\",\n    \"B8\",\n    \"B8A\",\n    \"B9\",\n    \"B10\",\n    \"B11\",\n    \"B12\",\n]\n",
  "history_output" : "",
  "history_begin_time" : 1647347130807,
  "history_end_time" : 1647347130921,
  "history_notes" : null,
  "history_process" : "jsnayl",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8yuhikxu08h",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PIXEL_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5), optimization=\"boxcar\"\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5))\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n\n    def erode(img, distance):\n        d = (\n            img.Not()\n            .unmask(1)\n            .fastDistanceTransform(30)\n            .sqrt()\n            .multiply(ee.Image.pixelArea().sqrt())\n        )\n        return img.updateMask(d.gt(distance))\n\n    def dilate(img, distance):\n        d = img.fastDistanceTransform(30).sqrt().multiply(ee.Image.pixelArea().sqrt())\n        return d.lt(distance)\n\n    score = score.reproject(\"EPSG:4326\", None, 20)\n    score = erode(score, erodePixels)\n    score = dilate(score, dilationPixels)\n\n    return score.reproject(\"EPSG:4326\", None, 20)\n\n\ndef mergeCollection(imgC):\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n    return filtered\n",
  "history_output" : "",
  "history_begin_time" : 1647347129974,
  "history_end_time" : 1647347132121,
  "history_notes" : null,
  "history_process" : "yqt708",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "88g6z0veh4t",
  "history_input" : "r\"\"\"\nFunctions shared by both the fast and slow\ncloudfree algorithm\n\"\"\"\nimport ee\nfrom datetime import date\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\n\nfrom typing import Union\n\n\ndef combine_bands(current, previous):\n    # Transforms an Image Collection with 1 band per Image into a single Image with items as bands\n    # Author: Jamie Vleeshouwer\n\n    # Rename the band\n    previous = ee.Image(previous)\n    current = current.select(BANDS)\n    # Append it to the result (Note: only return current item on first element/iteration)\n    return ee.Algorithms.If(\n        ee.Algorithms.IsEqual(previous, None), current, previous.addBands(ee.Image(current)),\n    )\n\n\ndef export(\n    image: ee.Image, region: ee.Geometry, filename: str, drive_folder: str, monitor: bool = False,\n) -> ee.batch.Export:\n\n    task = ee.batch.Export.image(\n        image.clip(region),\n        filename,\n        {\"scale\": 10, \"region\": region, \"maxPixels\": 1e13, \"driveFolder\": drive_folder},\n    )\n\n    try:\n        task.start()\n    except ee.ee_exception.EEException as e:\n        print(f\"Task not started! Got exception {e}\")\n        return task\n\n    if monitor:\n        monitor_task(task)\n\n    return task\n\n\ndef date_to_string(input_date: Union[date, str]) -> str:\n    if isinstance(input_date, str):\n        return input_date\n    else:\n        assert isinstance(input_date, date)\n        return input_date.strftime(\"%Y-%m-%d\")\n\n\ndef monitor_task(task: ee.batch.Export) -> None:\n\n    while task.status()[\"state\"] in [\"READY\", \"RUNNING\"]:\n        print(task.status())\n        # print(f\"Running: {task.status()['state']}\")\n\n\ndef rescale(img, exp, thresholds):\n    return (\n        img.expression(exp, {\"img\": img})\n        .subtract(thresholds[0])\n        .divide(thresholds[1] - thresholds[0])\n    )\n",
  "history_output" : "",
  "history_begin_time" : 1647347130493,
  "history_end_time" : 1647347132124,
  "history_notes" : null,
  "history_process" : "q5a232",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5tgygp7pnlh",
  "history_input" : "# The probability threshold to use to label GeoWiki\n# instances as crop / not_crop (since the GeoWiki labels are a mean crop probability, as\n# assigned by several labellers). In addition, this is the threshold used when calculating\n# metrics which require binary predictions, such as accuracy score\nPROBABILITY_THRESHOLD = 0.5\n",
  "history_output" : "",
  "history_begin_time" : 1647347132935,
  "history_end_time" : 1647347133024,
  "history_notes" : null,
  "history_process" : "nt17bz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "70a3322qff9",
  "history_input" : "import torch\nimport numpy as np\nimport random\n\nfrom dataclasses import dataclass\n\n\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n\n@dataclass\nclass BoundingBox:\n\n    min_lon: float\n    max_lon: float\n    min_lat: float\n    max_lat: float\n\n\nSTR2BB = {\n    \"Kenya\": BoundingBox(min_lon=33.501, max_lon=42.283, min_lat=-5.202, max_lat=6.002),\n    \"Busia\": BoundingBox(\n        min_lon=33.88389587402344,\n        min_lat=-0.04119872691853491,\n        max_lon=34.44007873535156,\n        max_lat=0.7779454563313616,\n    ),\n}\n",
  "history_output" : "",
  "history_begin_time" : 1647347133133,
  "history_end_time" : 1647347133949,
  "history_notes" : null,
  "history_process" : "o5t3jb",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "a70tjdhiq8e",
  "history_input" : "import pandas as pd\nimport xarray as xr\nfrom datetime import date\nfrom tqdm import tqdm\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass GeoWikiSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_geowiki\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        geowiki = self.data_folder / \"processed\" / GeoWikiExporter.dataset / \"data.nc\"\n        assert geowiki.exists(), \"GeoWiki processor must be run to load labels\"\n        return xr.open_dataset(geowiki).to_dataframe().dropna().reset_index()\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        start_date: date = date(2017, 3, 28),\n        end_date: date = date(2018, 3, 28),\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        Run the GeoWiki exporter. For each label, the exporter will export\n        int( (end_date - start_date).days / days_per_timestep) timesteps of data,\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param start_date: The start data of the data export\n        :param end_date: The end date of the data export\n        :param num_labelled_points: (Optional) The number of labelled points to export.\n        :param surrounding_metres: The number of metres surrounding each labelled point to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        assert start_date >= self.min_date, f\"Sentinel data does not exist before {self.min_date}\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        for idx, bounding_box in enumerate(bounding_boxes_to_download):\n            self._export_for_polygon(\n                polygon=bounding_box.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1647347137581,
  "history_end_time" : 1647347138236,
  "history_notes" : null,
  "history_process" : "mw544v",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "jqbr0eclpza",
  "history_input" : "from abc import ABC, abstractmethod\nfrom datetime import date, timedelta\nfrom pathlib import Path\nimport pandas as pd\nimport ee\n\nfrom src_exporters_sentinel_cloudfree_cloudfree import *\nfrom src_exporters_sentinel_cloudfree_fast import get_single_image as get_single_image_fast\nfrom src_exporters_base import BaseExporter\nfrom src_exporters_sentinel_cloudfree_utils import *\n\n\nfrom typing import List, Union\n\n\nclass BaseSentinelExporter(BaseExporter, ABC):\n\n    r\"\"\"\n    Download cloud free sentinel data for countries,\n    where countries are defined by the simplified large scale\n    international boundaries.\n    \"\"\"\n\n    dataset: str\n    min_date = date(2017, 3, 28)\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n        super().__init__(data_folder)\n        try:\n            ee.Initialize()\n        except Exception:\n            print(\"This code doesn't work unless you have authenticated your earthengine account\")\n\n        self.labels = self.load_labels()\n\n    @abstractmethod\n    def load_labels(self) -> pd.DataFrame:\n        raise NotImplementedError\n\n    def _export_for_polygon(\n        self,\n        polygon: ee.Geometry.Polygon,\n        polygon_identifier: Union[int, str],\n        start_date: date,\n        end_date: date,\n        days_per_timestep: int,\n        checkpoint: bool,\n        monitor: bool,\n        fast: bool,\n    ) -> None:\n\n        if fast:\n            export_func = get_single_image_fast\n        else:\n            export_func = get_single_image\n\n        cur_date = start_date\n        cur_end_date = cur_date + timedelta(days=days_per_timestep)\n\n        image_collection_list: List[ee.Image] = []\n\n        print(\n            f\"Exporting image for polygon {polygon_identifier} from \"\n            f\"aggregated images between {str(cur_date)} and {str(end_date)}\"\n        )\n        filename = f\"{polygon_identifier}_{str(cur_date)}_{str(end_date)}\"\n\n        if checkpoint and (self.output_folder / f\"{filename}.tif\").exists():\n            print(\"File already exists! Skipping\")\n            return None\n\n        while cur_end_date <= end_date:\n\n            image_collection_list.append(\n                export_func(region=polygon, start_date=cur_date, end_date=cur_end_date)\n            )\n            cur_date += timedelta(days=days_per_timestep)\n            cur_end_date += timedelta(days=days_per_timestep)\n\n        # now, we want to take our image collection and append the bands into a single image\n        imcoll = ee.ImageCollection(image_collection_list)\n        img = ee.Image(imcoll.iterate(combine_bands))\n\n        # and finally, export the image\n        export(\n            image=img,\n            region=polygon,\n            filename=filename,\n            drive_folder=self.dataset,\n            monitor=monitor,\n        )\n",
  "history_output" : "",
  "history_begin_time" : 1647347134129,
  "history_end_time" : 1647347135568,
  "history_notes" : null,
  "history_process" : "vxuj3q",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "aigzz06nwg7",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nfrom datetime import timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_kenya_non_crop import KenyaNonCropProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass KenyaNonCropSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_kenya_non_crop\"\n\n    # data collection date\n    data_date = date(2020, 4, 16)\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        non_crop = self.data_folder / \"processed\" / KenyaNonCropProcessor.dataset / \"data.geojson\"\n        assert non_crop.exists(), \"Kenya non crop processor must be run to load labels\"\n        return geopandas.read_file(non_crop)[[\"lat\", \"lon\"]]\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                ),\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        start_date = self.data_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            self._export_for_polygon(\n                polygon=bounding_info.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=self.data_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1647347150702,
  "history_end_time" : 1647347152591,
  "history_notes" : null,
  "history_process" : "nlb6f5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "p4akurbw6dl",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nimport numpy as np\nfrom datetime import datetime, timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_pv_kenya import KenyaPVProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre, date_overlap\n\nfrom typing import Dict, Optional, List, Tuple\n\n\nclass KenyaPVSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_plant_village_kenya\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        plantvillage = self.data_folder / \"processed\" / KenyaPVProcessor.dataset / \"data.geojson\"\n        assert plantvillage.exists(), \"Plant Village processor must be run to load labels\"\n        return geopandas.read_file(plantvillage)[\n            [\"lat\", \"lon\", \"index\", \"planting_date\", \"harvest_date\"]\n        ]\n\n    @staticmethod\n    def overlapping_year(\n        end_month: int, num_days: int, harvest_date: date, planting_date: date\n    ) -> Tuple[Optional[int], Optional[int]]:\n        r\"\"\"\n        Return the end_year of the most overlapping years\n        \"\"\"\n        harvest_year = harvest_date.year\n\n        overlap_dict: Dict[int, int] = {}\n\n        for diff in range(-1, 2):\n            end_date = date(harvest_year + diff, end_month, 1)\n\n            if end_date > datetime.now().date():\n                continue\n            else:\n                overlap_dict[harvest_year + diff] = date_overlap(\n                    planting_date, harvest_date, end_date - timedelta(days=num_days), end_date,\n                )\n        if len(overlap_dict) > 0:\n            return max(overlap_dict.items(), key=lambda x: x[1])\n        else:\n            # sometimes the harvest date is in the future? in which case\n            # we will just skip the datapoint for now\n            return None, None\n\n    def labels_to_bounding_boxes(\n        self,\n        num_labelled_points: Optional[int],\n        surrounding_metres: int,\n        end_month_day: Optional[Tuple[int, int]],\n        num_days: int,\n    ) -> List[Tuple[int, EEBoundingBox, date, Optional[int]]]:\n\n        output: List[Tuple[int, EEBoundingBox, date, Optional[int]]] = []\n\n        if end_month_day is not None:\n            end_month: Optional[int]\n            end_day: Optional[int]\n            end_month, end_day = end_month_day\n        else:\n            end_month = end_day = None\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            try:\n                harvest_date = datetime.strptime(row[\"harvest_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n            except ValueError:\n                continue\n\n            # this is only used if end_month is not None\n            overlapping_days: Optional[int] = 0\n            if end_month is not None:\n                planting_date = datetime.strptime(row[\"planting_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n\n                end_year, overlapping_days = self.overlapping_year(\n                    end_month, num_days, harvest_date, planting_date\n                )\n\n                if end_year is None:\n                    continue\n\n                if end_day is None:\n                    # if no end_day is passed, we will take the first month\n                    end_day = 1\n                harvest_date = date(end_year, end_month, end_day)\n\n            output.append(\n                (\n                    row[\"index\"],\n                    bounding_box_from_centre(\n                        mid_lat=row[\"lat\"],\n                        mid_lon=row[\"lon\"],\n                        surrounding_metres=surrounding_metres,\n                    ),\n                    harvest_date,\n                    overlapping_days,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def get_start_and_end_dates(\n        self, harvest_date: date, days_per_timestep: int, num_timesteps: int\n    ) -> Optional[Tuple[date, date]]:\n\n        if harvest_date < self.min_date:\n            print(\"Harvest date < min date - skipping\")\n            return None\n        else:\n            start_date = max(\n                harvest_date - timedelta(days_per_timestep * num_timesteps), self.min_date,\n            )\n            end_date = start_date + timedelta(days_per_timestep * num_timesteps)\n\n            return start_date, end_date\n\n    def export_for_labels(\n        self,\n        end_month_day: Optional[Tuple[int, int]] = (4, 16),\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param end_month_day: The final month-day to use. If None is passed, the harvest date\n            will be used. Default = (4, 16)\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points,\n            surrounding_metres=surrounding_metres,\n            end_month_day=end_month_day,\n            num_days=days_per_timestep * num_timesteps,\n        )\n\n        if end_month_day is not None:\n            print(\n                f\"Average overlapping days between planting to harvest and \"\n                f\"export dates: {np.mean([x[3] for x in bounding_boxes_to_download])}\"\n            )\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            harvest_date = bounding_info[-2]\n\n            dates = self.get_start_and_end_dates(harvest_date, days_per_timestep, num_timesteps)\n\n            if dates is not None:\n\n                self._export_for_polygon(\n                    polygon=bounding_info[1].to_ee_polygon(),\n                    polygon_identifier=bounding_info[0],\n                    start_date=dates[0],\n                    end_date=dates[1],\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n",
  "history_output" : "",
  "history_begin_time" : 1647347149277,
  "history_end_time" : 1647347149850,
  "history_notes" : null,
  "history_process" : "i4s7l1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "umj4vg6l19s",
  "history_input" : "from datetime import date, timedelta\nimport pandas as pd\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_sentinel_utils import bounding_box_to_earth_engine_bounding_box\nfrom src_utils import STR2BB\n\nfrom typing import Optional\n\n\nclass RegionalExporter(BaseSentinelExporter):\n    r\"\"\"\n    This is useful if you are trying to export\n    full regions for predictions\n    \"\"\"\n\n    dataset = \"earth_engine_region_busia_partial_slow_cloudfree\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # We don't need any labels for this exporter,\n        # so we can return an empty dataframe\n        return pd.DataFrame()\n\n    def export_for_region(\n        self,\n        region_name: str,\n        end_date: date,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        checkpoint: bool = True,\n        monitor: bool = True,\n        metres_per_polygon: Optional[int] = 10000,\n        fast: bool = True,\n    ):\n        r\"\"\"\n        Run the regional exporter. For each label, the exporter will export\n        data from (end_date - timedelta(days=days_per_timestep * num_timesteps)) to end_date\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param region_name: The name of the region to export. This must be defined in\n            src.utils.STR2BB\n        :param end_date: The end date of the data export\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param num_timesteps: The number of timesteps to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param metres_per_polygon: Whether to split the export of a large region into smaller\n            boxes of (max) area metres_per_polygon * metres_per_polygon. It is better to instead\n            split the area once it has been exported\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        start_date = end_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        region = bounding_box_to_earth_engine_bounding_box(STR2BB[region_name])\n\n        if metres_per_polygon is not None:\n\n            regions = region.to_polygons(metres_per_patch=metres_per_polygon)\n\n            for idx, region in enumerate(regions):\n                self._export_for_polygon(\n                    polygon=region,\n                    polygon_identifier=f\"{idx}-{region_name}\",\n                    start_date=start_date,\n                    end_date=end_date,\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n        else:\n            self._export_for_polygon(\n                polygon=region.to_ee_polygon(),\n                polygon_identifier=region_name,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1647347148651,
  "history_end_time" : 1647347150471,
  "history_notes" : null,
  "history_process" : "9c0ch9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gj456of1v51",
  "history_input" : "from pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cartopy.crs as ccrs\nfrom datetime import datetime\nimport xarray as xr\n\n\nfrom src_engineer_base import BaseEngineer\n\n\ndef sentinel_as_tci(sentinel_ds: xr.DataArray, scale: bool = True) -> xr.DataArray:\n    r\"\"\"\n    Get a True Colour Image from Sentinel data exported from Earth Engine\n    :param sentinel_ds: The sentinel data, exported from Earth Engine\n    :param scale: Whether or not to add the factor 10,000 scale\n    :return: A dataframe with true colour bands\n    \"\"\"\n\n    band2idx = {band: idx for idx, band in enumerate(sentinel_ds.attrs[\"band_descriptions\"])}\n\n    tci_bands = [\"B4\", \"B3\", \"B2\"]\n    tci_indices = [band2idx[band] for band in tci_bands]\n    if scale:\n        return sentinel_ds.isel(band=tci_indices) / 10000 * 2.5\n    else:\n        return sentinel_ds.isel(band=tci_indices) * 2.5\n\n\ndef plot_results(model_preds: xr.Dataset, tci_path: Path, savepath: Path, prefix: str = \"\") -> None:\n\n    multi_output = len(model_preds.data_vars) > 1\n\n    tci = sentinel_as_tci(\n        BaseEngineer.load_tif(tci_path, start_date=datetime(2020, 1, 1), days_per_timestep=30),\n        scale=False,\n    ).isel(time=-1)\n\n    tci = tci.sortby(\"x\").sortby(\"y\")\n    model_preds = model_preds.sortby(\"lat\").sortby(\"lon\")\n\n    plt.clf()\n    fig, ax = plt.subplots(1, 3, figsize=(20, 7.5), subplot_kw={\"projection\": ccrs.PlateCarree()})\n\n    fig.suptitle(\n        f\"Model results for tile with bottom left corner:\"\n        f\"\\nat latitude {float(model_preds.lat.min())}\"\n        f\"\\n and longitude {float(model_preds.lon.min())}\",\n        fontsize=15,\n    )\n    # ax 1 - original\n    img_extent_1 = (tci.x.min(), tci.x.max(), tci.y.min(), tci.y.max())\n    img = np.clip(np.moveaxis(tci.values, 0, -1), 0, 1)\n\n    ax[0].set_title(\"True colour image\")\n    ax[0].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict = {\n        \"origin\": \"upper\",\n        \"extent\": img_extent_1,\n        \"transform\": ccrs.PlateCarree(),\n    }\n\n    if multi_output:\n        mask = np.argmax(model_preds.to_array().values, axis=0)\n\n        # currently, we have 10 classes (at most). It seems unlikely we will go\n        # above 20\n        args_dict[\"cmap\"] = plt.cm.get_cmap(\"tab20\", len(model_preds.data_vars))\n    else:\n        mask = model_preds.prediction_0\n        args_dict.update({\"vmin\": 0, \"vmax\": 1})\n\n    # ax 2 - mask\n    ax[1].set_title(\"Mask\")\n    im = ax[1].imshow(mask, **args_dict)\n\n    # finally, all together\n    ax[2].set_title(\"Mask on top of the true colour image\")\n    ax[2].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict[\"alpha\"] = 0.3\n    if not multi_output:\n        mask = mask > 0.5\n    ax[2].imshow(mask, **args_dict)\n\n    colorbar_args = {\n        \"ax\": ax.ravel().tolist(),\n    }\n\n    if multi_output:\n        # This function formatter will replace integers with target names\n        formatter = plt.FuncFormatter(lambda val, loc: list(model_preds.data_vars)[val])\n        colorbar_args.update({\"ticks\": range(len(model_preds.data_vars)), \"format\": formatter})\n\n    # We must be sure to specify the ticks matching our target names\n    fig.colorbar(im, **colorbar_args)\n\n    plt.savefig(savepath / f\"results_{prefix}{tci_path.name}.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n",
  "history_output" : "",
  "history_begin_time" : 1647347134136,
  "history_end_time" : 1647347134566,
  "history_notes" : null,
  "history_process" : "qdzo28",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cera753xj52",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_processors_geowiki import *\nfrom src_processors_kenya_non_crop import *\nfrom src_processors_pv_kenya import *\n\ndef process_geowiki():\n    processor = GeoWikiProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_plantvillage():\n    processor = KenyaPVProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_kenya_noncrop():\n    processor = KenyaNonCropProcessor(Path(\"../data\"))\n    processor.process()\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...process.py\")\n    process_geowiki()\n    process_plantvillage()\n    #process_kenya_noncrop()\n",
  "history_output" : "Starting...process.py\n",
  "history_begin_time" : 1647347135958,
  "history_end_time" : 1647347138885,
  "history_notes" : null,
  "history_process" : "iticjd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o6coc2p19jf",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_engineer_geowiki import GeoWikiEngineer\nfrom src_engineer_pv_kenya import PVKenyaEngineer\nfrom src_engineer_kenya_non_crop import KenyaNonCropEngineer\n\n\ndef engineer_geowiki():\n    engineer = GeoWikiEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.2)\n\n\ndef engineer_kenya():\n    engineer = PVKenyaEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.1, test_set_size=0.1)\n\n\ndef engineer_kenya_noncrop():\n    engineer = KenyaNonCropEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.1, test_set_size=0.1)\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...engineer.py\")  \n    engineer_geowiki()\n    engineer_kenya()\n    #engineer_kenya_noncrop()",
  "history_output" : "",
  "history_begin_time" : 1647347139240,
  "history_end_time" : 1647347139805,
  "history_notes" : null,
  "history_process" : "3cars9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "sad0n2vf0eu",
  "history_input" : "import sys\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_models_model import Model\nfrom src_models_train_funcs import train_model\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...model.py\")\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--max_epochs\", type=int, default=1000)\n    parser.add_argument(\"--patience\", type=int, default=10)\n\n    model_args = Model.add_model_specific_args(parser).parse_args()\n    model = Model(model_args)\n\n    train_model(model, model_args)\n",
  "history_output" : "",
  "history_begin_time" : 1647347140240,
  "history_end_time" : 1647347140685,
  "history_notes" : null,
  "history_process" : "r5a5p5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "kgglgxya5xw",
  "history_input" : "from pathlib import Path\nimport sys\nimport os\n\nsys.path.append(\"..\")\n\nfrom src_models_model import Model\nfrom src_analysis import plot_results\n\n\ndef kenya_crop_type_mapper():\n    data_dir = \"../data\"\n\n    test_folder = Path(\"../data/raw/earth_engine_plant_village_kenya/\")\n    test_files = test_folder.glob(\"*.tif\")\n    print(test_files)\n\n    list_of_models = list(Path('../data/lightning_logs/').glob('version*/checkpoints/*.ckpt'))\n    latest_model_path = str(max(list_of_models, key=os.path.getctime))\n    print(f\"Using model {latest_model_path}\")\n\n    model = Model.load_from_checkpoint(latest_model_path)\n\n    for test_path in test_files:\n\n        save_dir = Path(data_dir) / \"Autoencoder\"\n        save_dir.mkdir(exist_ok=True)\n\n        print(f\"Running for {test_path}\")\n\n        savepath = save_dir / f\"preds_{test_path.name}\"\n        if savepath.exists():\n            print(\"File already generated. Skipping\")\n            continue\n\n        out_forecasted = model.predict(test_path, with_forecaster=True)\n        plot_results(out_forecasted, test_path, savepath=save_dir, prefix=\"forecasted_\")\n\n        out_normal = model.predict(test_path, with_forecaster=False)\n        plot_results(out_normal, test_path, savepath=save_dir, prefix=\"full_input_\")\n\n        out_forecasted.to_netcdf(save_dir / f\"preds_forecasted_{test_path.name}.nc\")\n        out_normal.to_netcdf(save_dir / f\"preds_normal_{test_path.name}.nc\")\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...predict.py\")\n    kenya_crop_type_mapper()\n",
  "history_output" : "Starting...predict.py\n<generator object Path.glob at 0x7fbc5da26f20>\nTraceback (most recent call last):\n  File \"scripts_predict.py\", line 48, in <module>\n    kenya_crop_type_mapper()\n  File \"scripts_predict.py\", line 19, in kenya_crop_type_mapper\n    latest_model_path = str(max(list_of_models, key=os.path.getctime))\nValueError: max() arg is an empty sequence\n",
  "history_begin_time" : 1647347140898,
  "history_end_time" : 1647347144356,
  "history_notes" : null,
  "history_process" : "delykw",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5fj57p1gtjk",
  "history_input" : "import pytorch_lightning as pl\nfrom pathlib import Path\nfrom argparse import ArgumentParser\nimport os\n\nimport sys\n\nsys.path.append(\"..\")\nfrom src_models_model import Model\n\n\ndef get_checkpoint(data_folder: Path) -> str:\n\n    log_folder = data_folder / \"lightning_logs/\" \n    list_of_checkpoints = list(log_folder.glob('version*/checkpoints/*.ckpt'))\n    print(log_folder.absolute())\n    return str(max(list_of_checkpoints, key=os.path.getctime))\n\n\ndef test_model():\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--version\", type=int, default=0)\n\n    args = parser.parse_args()\n\n    model_path = get_checkpoint(Path(\"../data\"))\n\n    print(f\"Using model {model_path}\")\n\t\n    model = Model.load_from_checkpoint(model_path)\n\n    trainer = pl.Trainer()\n    trainer.test(model)\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...test.py\")\n    test_model()\n",
  "history_output" : "Starting...test.py\n/Users/uhhmed/gw-workspace/Z6CfJoPMvhm2lCseSXQxrmgU21/../data/lightning_logs\nTraceback (most recent call last):\n  File \"scripts_test.py\", line 39, in <module>\n    test_model()\n  File \"scripts_test.py\", line 27, in test_model\n    model_path = get_checkpoint(Path(\"../data\"))\n  File \"scripts_test.py\", line 17, in get_checkpoint\n    return str(max(list_of_checkpoints, key=os.path.getctime))\nValueError: max() arg is an empty sequence\n",
  "history_begin_time" : 1647347145357,
  "history_end_time" : 1647347148688,
  "history_notes" : null,
  "history_process" : "q1j13t",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "080er1n21sr",
  "history_input" : "from pathlib import Path\nfrom src_utils import set_seed\n\n\nclass BaseProcessor:\n    r\"\"\"Base for all processor classes. It creates the appropriate\n    directory in the data dir (``data_dir/processed/{dataset}``).\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n\n    def __init__(self, data_folder: Path) -> None:\n\n        set_seed()\n        self.data_folder = data_folder\n        self.raw_folder = self.data_folder / \"raw\" / self.dataset\n        assert self.raw_folder.exists(), f\"{self.raw_folder} does not exist!\"\n\n        self.output_folder = self.data_folder / \"processed\" / self.dataset\n        self.output_folder.mkdir(exist_ok=True, parents=True)\n",
  "history_output" : "",
  "history_begin_time" : 1647347146772,
  "history_end_time" : 1647347147270,
  "history_notes" : null,
  "history_process" : "6nnond",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dd0g4dfuyzc",
  "history_input" : "import pandas as pd\n\nfrom src_processors_base import BaseProcessor\n\n\nclass GeoWikiProcessor(BaseProcessor):\n\n    dataset = \"geowiki_landcover_2017\"\n\n    def load_raw_data(self, participants: str) -> pd.DataFrame:\n\n        participants_to_file_labels = {\n            \"all\": \"all\",\n            \"students\": \"con\",\n            \"experts\": \"exp\",\n        }\n\n        file_label = participants_to_file_labels.get(participants, participants)\n        assert (\n            file_label in participants_to_file_labels.values()\n        ), f\"Unknown participant {file_label}\"\n\n        return pd.read_csv(\n            self.raw_folder / f\"loc_{file_label}{'_2' if file_label == 'all' else ''}.txt\",\n            sep=\"\\t\",\n        )\n\n    def process(self, participants: str = \"all\") -> None:\n\n        location_data = self.load_raw_data(participants)\n\n        # first, we find the mean sumcrop calculated per location\n        mean_per_location = (\n            location_data[[\"location_id\", \"sumcrop\", \"loc_cent_X\", \"loc_cent_Y\"]]\n            .groupby(\"location_id\")\n            .mean()\n        )\n\n        # then, we rename the columns\n        mean_per_location = mean_per_location.rename(\n            {\"loc_cent_X\": \"lon\", \"loc_cent_Y\": \"lat\", \"sumcrop\": \"mean_sumcrop\"},\n            axis=\"columns\",\n            errors=\"raise\",\n        )\n        # then, we turn it into an xarray with x and y as indices\n        output_xr = (\n            mean_per_location.reset_index().set_index([\"lon\", \"lat\"])[\"mean_sumcrop\"].to_xarray()\n        )\n\n        # and save\n        output_xr.to_netcdf(self.output_folder / \"data.nc\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347139895,
  "history_end_time" : 1647347140680,
  "history_notes" : null,
  "history_process" : "m6v1cg",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "xela27qzi8g",
  "history_input" : "import geopandas\nimport pandas as pd\nimport numpy as np\n\nfrom src_processors_base import BaseProcessor\n\nfrom typing import List\n\n\nclass KenyaPVProcessor(BaseProcessor):\n    dataset = \"plant_village_kenya\"\n\n    def process(self) -> None:\n\n        subfolders = [f\"ref_african_crops_kenya_01_labels_0{i}\" for i in [0, 1, 2]]\n\n        dfs: List[geopandas.GeoDataFrame] = []\n        for subfolder in subfolders:\n            df = geopandas.read_file(\n                self.raw_folder / \"ref_african_crops_kenya_01_labels\" / subfolder / \"labels.geojson\"\n            )\n            df = df.rename(\n                columns={\n                    \"Latitude\": \"lat\",\n                    \"Longitude\": \"lon\",\n                    \"Planting Date\": \"planting_date\",\n                    \"Estimated Harvest Date\": \"harvest_date\",\n                    \"Crop1\": \"label\",\n                    \"Survey Date\": \"collection_date\",\n                }\n            )\n            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"]).dt.to_pydatetime()\n            df[\"harvest_date\"] = pd.to_datetime(df[\"harvest_date\"]).dt.to_pydatetime()\n            df[\"collection_date\"] = pd.to_datetime(df[\"collection_date\"]).dt.to_pydatetime()\n            df[\"is_crop\"] = np.where((df[\"label\"] == \"Fallowland\"), 0, 1)\n            df = df.to_crs(\"EPSG:4326\")\n            dfs.append(df)\n\n        df = pd.concat(dfs)\n        df = df.reset_index(drop=True)\n        df[\"index\"] = df.index\n        df.to_file(self.output_folder / \"data.geojson\", driver=\"GeoJSON\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347144708,
  "history_end_time" : 1647347145758,
  "history_notes" : null,
  "history_process" : "m9myzm",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b7a2jociscz",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347146366,
  "history_end_time" : 1647347146648,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "amzhkpum1hp",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import datetime\nimport pandas as pd\nfrom pathlib import Path\nimport xarray as xr\n\nfrom typing import Optional\n\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_geowiki import GeoWikiSentinelExporter\nfrom src_config import PROBABILITY_THRESHOLD\nfrom src_engineer_base import BaseEngineer, DataInstance\n\n\nclass GeoWikiEngineer(BaseEngineer):\n\n    sentinel_dataset = GeoWikiSentinelExporter.dataset\n    dataset = GeoWikiExporter.dataset\n\n    @staticmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        geowiki = data_folder / \"processed\" / GeoWikiExporter.dataset / \"data.nc\"\n        assert geowiki.exists(), \"GeoWiki processor must be run to load labels\"\n        return xr.open_dataset(geowiki).to_dataframe().dropna().reset_index()\n\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        r\"\"\"\n        Return a tuple of np.ndarrays of shape [n_timesteps, n_features] for\n        1) the anchor (labelled)\n        \"\"\"\n\n        da = self.load_tif(path_to_file, days_per_timestep=days_per_timestep, start_date=start_date)\n\n        # first, we find the label encompassed within the da\n\n        min_lon, min_lat = float(da.x.min()), float(da.y.min())\n        max_lon, max_lat = float(da.x.max()), float(da.y.max())\n        overlap = self.labels[\n            (\n                (self.labels.lon <= max_lon)\n                & (self.labels.lon >= min_lon)\n                & (self.labels.lat <= max_lat)\n                & (self.labels.lat >= min_lat)\n            )\n        ]\n        if len(overlap) == 0:\n            return None\n\n        label_lat = overlap.iloc[0].lat\n        label_lon = overlap.iloc[0].lon\n\n        # we turn the percentage into a fraction\n        crop_probability = overlap.iloc[0].mean_sumcrop / 100\n\n        closest_lon, _ = self.find_nearest(da.x, label_lon)\n        closest_lat, _ = self.find_nearest(da.y, label_lat)\n\n        labelled_np = da.sel(x=closest_lon).sel(y=closest_lat).values\n\n        if add_ndvi:\n            labelled_np = self.calculate_ndvi(labelled_np)\n        if add_ndwi:\n            labelled_np = self.calculate_ndwi(labelled_np)\n\n        labelled_array = self.maxed_nan_to_num(labelled_np, nan=nan_fill, max_ratio=max_nan_ratio)\n\n        if (not is_test) and calculate_normalizing_dict:\n            # we won't use the neighbouring array for now, since tile2vec is\n            # not really working\n            self.update_normalizing_values(self.normalizing_dict_interim, labelled_array)\n\n        if labelled_array is not None:\n            return DataInstance(\n                label_lat=label_lat,\n                label_lon=label_lon,\n                instance_lat=closest_lat,\n                instance_lon=closest_lon,\n                labelled_array=labelled_array,\n                is_crop=crop_probability >= PROBABILITY_THRESHOLD,\n                dataset=self.dataset,\n            )\n        else:\n            return None\n",
  "history_output" : "",
  "history_begin_time" : 1647347140812,
  "history_end_time" : 1647347143345,
  "history_notes" : null,
  "history_process" : "rus783",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lue1gbluazk",
  "history_input" : "from dataclasses import dataclass\nimport pandas as pd\nfrom pathlib import Path\nimport geopandas\nfrom datetime import datetime\n\nfrom typing import Optional\n\n\nfrom src_processors_pv_kenya import KenyaPVProcessor\nfrom src_exporters_sentinel_pv_kenya import KenyaPVSentinelExporter\nfrom src_engineer_base import BaseEngineer, DataInstance\n\n\nclass PVKenyaEngineer(BaseEngineer):\n\n    sentinel_dataset = KenyaPVSentinelExporter.dataset\n    dataset = KenyaPVProcessor.dataset\n\n    @staticmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        pv_kenya = data_folder / \"processed\" / KenyaPVProcessor.dataset / \"data.geojson\"\n        assert pv_kenya.exists(), \"Kenya Plant Village processor must be run to load labels\"\n        return geopandas.read_file(pv_kenya)\n\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        r\"\"\"\n        Return a tuple of np.ndarrays of shape [n_timesteps, n_features] for\n        1) the anchor (labelled)\n        \"\"\"\n\n        da = self.load_tif(path_to_file, days_per_timestep=days_per_timestep, start_date=start_date)\n\n        # first, we find the label encompassed within the da\n\n        min_lon, min_lat = float(da.x.min()), float(da.y.min())\n        max_lon, max_lat = float(da.x.max()), float(da.y.max())\n        overlap = self.labels[\n            (\n                (self.labels.lon <= max_lon)\n                & (self.labels.lon >= min_lon)\n                & (self.labels.lat <= max_lat)\n                & (self.labels.lat >= min_lat)\n            )\n        ]\n        if len(overlap) == 0:\n            return None\n        else:\n            label_lat = overlap.iloc[0].lat\n            label_lon = overlap.iloc[0].lon\n\n            is_crop = bool(overlap.iloc[0].is_crop)\n\n            closest_lon, _ = self.find_nearest(da.x, label_lon)\n            closest_lat, _ = self.find_nearest(da.y, label_lat)\n\n            labelled_np = da.sel(x=closest_lon).sel(y=closest_lat).values\n\n            if add_ndvi:\n                labelled_np = self.calculate_ndvi(labelled_np)\n            if add_ndwi:\n                labelled_np = self.calculate_ndwi(labelled_np)\n\n            labelled_array = self.maxed_nan_to_num(\n                labelled_np, nan=nan_fill, max_ratio=max_nan_ratio\n            )\n\n            if (not is_test) and calculate_normalizing_dict:\n                self.update_normalizing_values(self.normalizing_dict_interim, labelled_array)\n\n            if labelled_array is not None:\n                return DataInstance(\n                    label_lat=label_lat,\n                    label_lon=label_lon,\n                    instance_lat=closest_lat,\n                    instance_lon=closest_lon,\n                    labelled_array=labelled_array,\n                    is_crop=is_crop,\n                    dataset=self.dataset,\n                )\n            else:\n                return None\n",
  "history_output" : "",
  "history_begin_time" : 1647347145369,
  "history_end_time" : 1647347145763,
  "history_notes" : null,
  "history_process" : "s024ve",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6404aiv03yo",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347147655,
  "history_end_time" : 1647347149843,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "qj5srqr1ln9",
  "history_input" : "import geopandas\nfrom pathlib import Path\nimport pandas as pd\nfrom pyproj import Transformer\n\nfrom src_processors_base import BaseProcessor\n\nfrom typing import List\n\n\nclass KenyaNonCropProcessor(BaseProcessor):\n\n    dataset = \"kenya_non_crop\"\n\n    @staticmethod\n    def process_set(filepath: Path, latlon: bool, reversed: bool) -> geopandas.GeoDataFrame:\n        df = geopandas.read_file(filepath)\n\n        x, y = df.geometry.centroid.x.values, df.geometry.centroid.y.values\n\n        if reversed:\n            x, y = y, x\n\n        if not latlon:\n\n            transformer = Transformer.from_crs(crs_from=32636, crs_to=4326)\n\n            lat, lon = transformer.transform(xx=x, yy=y)\n            df[\"lat\"] = lat\n            df[\"lon\"] = lon\n        else:\n            df[\"lat\"] = x\n            df[\"lon\"] = y\n\n        df[\"index\"] = df.index\n\n        return df\n\n    def process(self) -> None:\n\n        filepaths = [\n            (self.raw_folder / \"noncrop_labels_v2\", False, False),\n            (self.raw_folder / \"noncrop_labels_set2\", False, False),\n            (self.raw_folder / \"2019_gepro_noncrop\", True, True),\n            (self.raw_folder / \"noncrop_water_kenya_gt\", True, True),\n            (self.raw_folder / \"noncrop_kenya_gt\", True, True),\n        ]\n\n        dfs: List[geopandas.GeoDataFrame] = []\n        for filepath, is_latlon, is_reversed in filepaths:\n            dfs.append(self.process_set(filepath, is_latlon, is_reversed))\n\n        df = pd.concat(dfs)\n        df.to_file(self.output_folder / \"data.geojson\", driver=\"GeoJSON\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347150856,
  "history_end_time" : 1647347151861,
  "history_notes" : null,
  "history_process" : "9x9elz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "whho4n273fg",
  "history_input" : "from dataclasses import dataclass\nimport pandas as pd\nfrom pathlib import Path\nimport geopandas\nfrom datetime import datetime\nimport numpy as np\n\nfrom typing import Optional\n\nfrom src_processors_kenya_non_crop import KenyaNonCropProcessor\nfrom src_exporters_sentinel_kenya_non_crop import KenyaNonCropSentinelExporter\nfrom src_engineer_base import BaseEngineer, DataInstance\n\n\nclass KenyaNonCropEngineer(BaseEngineer):\n\n    sentinel_dataset = KenyaNonCropSentinelExporter.dataset\n    dataset = KenyaNonCropProcessor.dataset\n\n    @staticmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        pv_kenya = data_folder / \"processed\" / KenyaNonCropProcessor.dataset / \"data.geojson\"\n        assert pv_kenya.exists(), \"Kenya Non Crop processor must be run to load labels\"\n        return geopandas.read_file(pv_kenya)\n\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        r\"\"\"\n        Return a tuple of np.ndarrays of shape [n_timesteps, n_features] for\n        1) the anchor (labelled)\n        \"\"\"\n\n        da = self.load_tif(path_to_file, days_per_timestep=days_per_timestep, start_date=start_date)\n\n        # first, we find the label encompassed within the da\n\n        min_lon, min_lat = float(da.x.min()), float(da.y.min())\n        max_lon, max_lat = float(da.x.max()), float(da.y.max())\n        overlap = self.labels[\n            (\n                (self.labels.lon <= max_lon)\n                & (self.labels.lon >= min_lon)\n                & (self.labels.lat <= max_lat)\n                & (self.labels.lat >= min_lat)\n            )\n        ]\n        if len(overlap) == 0:\n            return None\n        label_lat = overlap.iloc[0].lat\n        label_lon = overlap.iloc[0].lon\n\n        closest_lon, _ = self.find_nearest(da.x, label_lon)\n        closest_lat, _ = self.find_nearest(da.y, label_lat)\n\n        labelled_np = da.sel(x=closest_lon).sel(y=closest_lat).values\n\n        if add_ndvi:\n            labelled_np = self.calculate_ndvi(labelled_np)\n        if add_ndwi:\n            labelled_np = self.calculate_ndwi(labelled_np)\n\n        labelled_array = self.maxed_nan_to_num(labelled_np, nan=nan_fill, max_ratio=max_nan_ratio)\n\n        if (not is_test) and calculate_normalizing_dict:\n            self.update_normalizing_values(self.normalizing_dict_interim, labelled_array)\n\n        if labelled_array is not None:\n            return DataInstance(\n                label_lat=label_lat,\n                label_lon=label_lon,\n                instance_lat=closest_lat,\n                instance_lon=closest_lon,\n                labelled_array=labelled_array,\n                is_crop=False,\n                dataset=self.dataset,\n            )\n        else:\n            return None\n",
  "history_output" : "",
  "history_begin_time" : 1647347147769,
  "history_end_time" : 1647347150612,
  "history_notes" : null,
  "history_process" : "a3ucnn",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "62r4o7kil4s",
  "history_input" : "from pathlib import Path\nimport numpy as np\nimport pickle\nimport geopandas\nimport random\nimport math\n\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_processors_pv_kenya import KenyaPVProcessor\nfrom src_processors_kenya_non_crop import KenyaNonCropProcessor\nfrom src_processors_geowiki import GeoWikiProcessor\n\nfrom typing import cast, Tuple, Optional, List, Dict, Sequence, Union\n\n\nclass CropDataset(Dataset):\n\n    bands_to_remove = [\"B1\", \"B10\"]\n\n    def __init__(\n        self,\n        data_folder: Path,\n        subset: str,\n        remove_b1_b10: bool,\n        include_geowiki: bool,\n        cache: bool,\n        upsample: bool,\n        noise_factor: bool,\n        normalizing_dict: Optional[Dict] = None,\n    ) -> None:\n\n        self.include_geowiki = include_geowiki\n        self.upsample = upsample\n\n        self.data_folder = data_folder\n        self.features_dir = data_folder / \"features\"\n\n        assert subset in [\"training\", \"validation\", \"testing\"]\n        self.subset_name = subset\n\n        self.remove_b1_b10 = remove_b1_b10\n\n        self.x: Optional[torch.Tensor] = None\n        self.y: Optional[torch.Tensor] = None\n        self.weights: Optional[torch.Tensor] = None\n\n        # this is kept at False in case caching = True. It should be\n        # changed to the input noise argument at the end of the\n        # init function\n        self.noise_factor = 0\n\n        files_and_nds: List[Tuple] = []\n        for dataset in [\n            KenyaPVProcessor.dataset,\n            KenyaNonCropProcessor.dataset,\n            GeoWikiExporter.dataset,\n        ]:\n            files_and_nds.append(\n                self.load_files_and_normalizing_dicts(\n                    self.data_folder / \"features\" / dataset, self.subset_name,\n                )\n            )\n\n        if normalizing_dict is not None:\n            self.normalizing_dict: Optional[Dict] = normalizing_dict\n        else:\n            # if no normalizing dict was passed to the consturctor,\n            # then we want to make our own\n            self.normalizing_dict = self.adjust_normalizing_dict(\n                [(len(x[0]), x[1]) for x in files_and_nds]\n            )\n\n        pickle_files: List[Path] = []\n        for files, _ in files_and_nds:\n            pickle_files.extend(files)\n        self.pickle_files = pickle_files\n\n        self.cache = False\n\n        self.class_instances: List = []\n        if upsample:\n            instances_per_class = self.instances_per_class\n            max_instances_in_class = max(instances_per_class)\n\n            new_pickle_files: List[Path] = []\n\n            for idx, num_instances in enumerate(instances_per_class):\n                if num_instances > 0:\n                    new_pickle_files.extend(self.upsample_class(idx, max_instances_in_class))\n            self.pickle_files.extend(new_pickle_files)\n\n        if cache:\n            self.x, self.y, self.weights = self.to_array()\n            self.cache = cache\n        # we only save the noise attribute after the arrays have been cached, to\n        # ensure the saved arrays are the noiseless ones\n        self.noise_factor = noise_factor\n\n    @staticmethod\n    def load_files_and_normalizing_dicts(\n        features_dir: Path, subset_name: str, file_suffix: str = \"pkl\"\n    ) -> Tuple[List[Path], Optional[Dict[str, np.ndarray]]]:\n        pickle_files = list((features_dir / subset_name).glob(f\"*.{file_suffix}\"))\n\n        # try loading the normalizing dict. By default, if it exists we will use it\n        if (features_dir / \"normalizing_dict.pkl\").exists():\n            with (features_dir / \"normalizing_dict.pkl\").open(\"rb\") as f:\n                normalizing_dict = pickle.load(f)\n        else:\n            normalizing_dict = None\n\n        return pickle_files, normalizing_dict\n\n    def _normalize(self, array: np.ndarray) -> np.ndarray:\n        if self.normalizing_dict is None:\n            return array\n        else:\n            return (array - self.normalizing_dict[\"mean\"]) / self.normalizing_dict[\"std\"]\n\n    def __len__(self) -> int:\n        return len(self.pickle_files)\n\n    def to_array(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        if self.x is not None:\n            assert self.y is not None\n            assert self.weights is not None\n            return self.x, self.y, self.weights\n        else:\n            x_list: List[torch.Tensor] = []\n            y_list: List[torch.Tensor] = []\n            weight_list: List[torch.Tensor] = []\n            print(\"Loading data into memory\")\n            for i in tqdm(range(len(self))):\n                x, y, weight = self[i]\n                x_list.append(x)\n                y_list.append(y)\n                weight_list.append(weight)\n\n            return torch.stack(x_list), torch.stack(y_list), torch.stack(weight_list)\n\n    @property\n    def num_input_features(self) -> int:\n\n        # assumes the first value in the tuple is x\n        assert len(self.pickle_files) > 0, \"No files to load!\"\n\n        output = self[0]\n        if isinstance(output, tuple):\n            return output[0].shape[1]\n        else:\n            return output.shape[1]\n\n    @property\n    def num_timesteps(self) -> int:\n        # assumes the first value in the tuple is x\n        assert len(self.pickle_files) > 0, \"No files to load!\"\n        output_tuple = self[0]\n        return output_tuple[0].shape[0]\n\n    @staticmethod\n    def adjust_normalizing_dict(\n        dicts: Sequence[Tuple[int, Optional[Dict[str, np.ndarray]]]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        for _, single_dict in dicts:\n            if single_dict is None:\n                return None\n\n        dicts = cast(Sequence[Tuple[int, Dict[str, np.ndarray]]], dicts)\n\n        new_total = sum([x[0] for x in dicts])\n\n        new_mean = sum([single_dict[\"mean\"] * length for length, single_dict in dicts]) / new_total\n\n        new_variance = (\n            sum(\n                [\n                    (single_dict[\"std\"] ** 2 + (single_dict[\"mean\"] - new_mean) ** 2) * length\n                    for length, single_dict in dicts\n                ]\n            )\n            / new_total\n        )\n\n        return {\"mean\": new_mean, \"std\": np.sqrt(new_variance)}\n\n    def remove_bands(self, x: np.ndarray) -> np.ndarray:\n        \"\"\"This nested function is so that\n        _remove_bands can be called from an unitialized\n        dataset, speeding things up at inference while still\n        keeping the convenience of not having to check if remove\n        bands is true all the time.\n        \"\"\"\n\n        if self.remove_bands:\n            return self._remove_bands(x)\n        else:\n            return x\n\n    @classmethod\n    def _remove_bands(cls, x: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Expects the input to be of shape [timesteps, bands]\n        \"\"\"\n        indices_to_remove: List[int] = []\n        for band in cls.bands_to_remove:\n            indices_to_remove.append(BANDS.index(band))\n\n        bands_index = 1 if len(x.shape) == 2 else 2\n        indices_to_keep = [i for i in range(x.shape[bands_index]) if i not in indices_to_remove]\n        if len(x.shape) == 2:\n            # timesteps, bands\n            return x[:, indices_to_keep]\n        else:\n            # batches, timesteps, bands\n            return x[:, :, indices_to_keep]\n\n    def upsample_class(self, class_idx: int, max_instances: int) -> List[Path]:\n        \"\"\"Given a class to upsample and the maximum number of classes,\n        update self.pickle_files to reflect the new number of classes\n        \"\"\"\n        class_files: List[Path] = []\n        for idx, filepath in enumerate(self.pickle_files):\n            _, class_int, is_global = self[idx]\n            if is_global == 0:\n                if class_int == class_idx:\n                    class_files.append(filepath)\n\n        multiplier = max_instances / len(class_files)\n\n        # we will return files which need to be *added* to pickle files\n        # multiplier will definitely be >= 1\n        fraction_multiplier, int_multiplier = math.modf(multiplier - 1)\n\n        new_files = random.sample(class_files, int(fraction_multiplier * len(class_files)))\n        new_files += class_files * int(int_multiplier)\n        return new_files\n\n    @property\n    def num_output_classes(self) -> Union[int, Tuple[int, int]]:\n\n        if self.include_geowiki:\n            # multi headed\n            return 1, 1\n        else:\n            return 1\n\n    def filter_min_occurences(\n        self, files: List[Path], min_occurences: int, ignore_intercropped: bool\n    ) -> Tuple[List[Path], List[str]]:\n\n        org_data = geopandas.read_file(\n            self.data_folder / \"processed\" / KenyaPVProcessor.dataset / \"data.geojson\"\n        )\n\n        counts = org_data.crop_type.value_counts().to_dict()\n\n        crops_to_ignore = [crop for crop, count in counts.items() if count < min_occurences]\n\n        if ignore_intercropped:\n            crops_to_ignore.extend([crop for crop, count in counts.items() if \"intercrop\" in crop])\n            crops_to_ignore = list(set(crops_to_ignore))\n\n        output_files: List[Path] = []\n        for target_file in files:\n            with target_file.open(\"rb\") as f:\n                target_datainstance = pickle.load(f)\n            if target_datainstance.crop_label in crops_to_ignore:\n                continue\n            else:\n                output_files.append(target_file)\n        return output_files, crops_to_ignore\n\n    @property\n    def instances_per_class(self) -> List[int]:\n\n        num_output_classes = self.num_output_classes\n        num_local_output_classes = (\n            num_output_classes[1] if isinstance(num_output_classes, tuple) else num_output_classes\n        )\n        if len(self.class_instances) == 0:\n            # we set a minimum number of output classes since if its 1,\n            # its really 2 (binary)\n            instances_per_class = [0] * max(num_local_output_classes, 2)\n            for i in range(len(self)):\n                _, class_int, is_global = self[i]\n                if is_global == 0:\n                    instances_per_class[int(class_int)] += 1\n            self.class_instances = instances_per_class\n        return self.class_instances\n\n    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n\n        if (self.cache) & (self.x is not None):\n            # if we upsample, the caching might not have happened yet\n            return (\n                cast(torch.Tensor, self.x)[index],\n                cast(torch.Tensor, self.y)[index],\n                cast(torch.Tensor, self.weights)[index],\n            )\n\n        target_file = self.pickle_files[index]\n\n        # first, we load up the target file\n        with target_file.open(\"rb\") as f:\n            target_datainstance = pickle.load(f)\n\n        is_global: float = 0.0\n\n        crop_int = int(target_datainstance.is_crop)\n        is_global = 1 if target_datainstance.dataset == GeoWikiProcessor.dataset else 0\n\n        x = self.remove_bands(x=self._normalize(target_datainstance.labelled_array))\n\n        return (\n            torch.from_numpy(x).float(),\n            torch.tensor(crop_int).float(),\n            torch.tensor(is_global).float(),\n        )\n",
  "history_output" : "",
  "history_begin_time" : 1647347141693,
  "history_end_time" : 1647347142692,
  "history_notes" : null,
  "history_process" : "t2liev",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ql2gd6kbbkn",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347144440,
  "history_end_time" : 1647347146644,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "c7ab3quryw7",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647347141810,
  "history_end_time" : 1647347142427,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tbg562spkpn",
  "history_input" : "from argparse import ArgumentParser, Namespace\nfrom pathlib import Path\nimport numpy as np\nimport xarray as xr\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nimport pytorch_lightning as pl\n\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    mean_absolute_error,\n)\n\nfrom src_models_data import CropDataset\nfrom src_models_utils import tif_to_np, preds_to_xr\nfrom src_utils import set_seed\nfrom src_models_forecaster import Forecaster\nfrom src_models_classifier import Classifier\nfrom src_config import PROBABILITY_THRESHOLD\n\nfrom typing import cast, Callable, Tuple, Dict, Any, Type, Optional, List, Union\n\n\nclass Model(pl.LightningModule):\n    r\"\"\"\n    An model for annual and in-season crop mapping. This model consists of a\n    forecaster.Forecaster and a classifier.Classifier - it will require the arguments\n    required by those models too.\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.data_folder: The path to the data. Default (assumes the model\n        is being run from the scripts directory) = \"../data\"\n    :param hparams.learning_rate: The learning rate. Default = 0.001\n    :param hparams.batch_size: The batch size. Default = 64\n    :param hparams.input_months: The number of input months to pass to the model. If\n        hparams.forecast is True, the remaining months will be forecasted. Otherwise, only the\n        partial timeseries will be passed to the classifier. Default = 5\n    :param hparams.alpha: The weight to use when adding the global and local losses. This\n        parameter is only used if hparams.multi_headed is True. Default = 10\n    :param hparams.noise_factor: The standard deviation of the random noise to add to the\n        raw inputs to the classifier. Default = 0.1\n    :param hparams.remove_b1_b10: Whether or not to remove the B1 and B10 bands. Default = True\n    :param hparams.forecast: Whether or not to forecast the partial time series. Default = True\n    :param hparams.cache: Whether to load all the data into memory during training. Default = True\n    :param hparams.include_geowiki: Whether to include the global GeoWiki dataset during\n        training. Default = True\n    :param hparams.upsample: Whether to oversample the under-represented class so that each class\n        is equally represented in the training and validation dataset. Default = True\n    \"\"\"\n\n    def __init__(self, hparams: Namespace) -> None:\n        super().__init__()\n        set_seed()\n        self.hparams = hparams\n\n        self.data_folder = Path(hparams.data_folder)\n\n        dataset = self.get_dataset(subset=\"training\", cache=False)\n        self.num_outputs = dataset.num_output_classes\n        self.num_timesteps = dataset.num_timesteps\n        self.input_size = dataset.num_input_features\n\n        # we save the normalizing dict because we calculate weighted\n        # normalization values based on the datasets we combine.\n        # The number of instances per dataset (and therefore the weights) can\n        # vary between the train / test / val sets - this ensures the normalizing\n        # dict stays constant between them\n        self.normalizing_dict = dataset.normalizing_dict\n\n        if self.hparams.forecast:\n            num_output_timesteps = self.num_timesteps - self.hparams.input_months\n            print(\n                f\"Predicting {num_output_timesteps} timesteps in the forecaster\")\n            self.forecaster = Forecaster(\n                num_bands=self.input_size, output_timesteps=num_output_timesteps, hparams=hparams,\n            )\n\n            self.forecaster_loss = F.smooth_l1_loss\n\n        self.classifier = Classifier(\n            input_size=self.input_size, hparams=hparams)\n        self.global_loss_function: Callable = F.binary_cross_entropy\n        self.local_loss_function: Callable = F.binary_cross_entropy\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        # To keep the ABC happy\n        return self.classifier(x)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n    def get_dataset(\n        self, subset: str, normalizing_dict: Optional[Dict] = None, cache: Optional[bool] = None,\n    ) -> CropDataset:\n        return CropDataset(\n            data_folder=self.data_folder,\n            subset=subset,\n            remove_b1_b10=self.hparams.remove_b1_b10,\n            normalizing_dict=normalizing_dict,\n            include_geowiki=self.hparams.include_geowiki if subset != \"testing\" else False,\n            cache=self.hparams.cache if cache is None else cache,\n            upsample=self.hparams.upsample if subset != \"testing\" else False,\n            noise_factor=self.hparams.noise_factor if subset != \"testing\" else 0,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.get_dataset(subset=\"training\"), shuffle=True, batch_size=self.hparams.batch_size,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.get_dataset(subset=\"validation\",\n                             normalizing_dict=self.normalizing_dict,),\n            batch_size=self.hparams.batch_size,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.get_dataset(subset=\"testing\",\n                             normalizing_dict=self.normalizing_dict,),\n            batch_size=self.hparams.batch_size,\n        )\n\n    def predict(\n        self,\n        path_to_file: Path,\n        with_forecaster: bool,\n        batch_size: int = 64,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        nan_fill: float = 0,\n        days_per_timestep: int = 30,\n        local_head: bool = True,\n        use_gpu: bool = True,\n    ) -> xr.Dataset:\n\n        # check if a GPU is available, and if it is\n        # move the model onto the GPU\n        device: Optional[torch.device] = None\n        if use_gpu:\n            use_cuda = torch.cuda.is_available()\n            if not use_cuda:\n                print(\"No GPU - not using one\")\n            device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n            self.to(device)\n\n        self.eval()\n\n        input_data = tif_to_np(\n            path_to_file,\n            add_ndvi=add_ndvi,\n            add_ndwi=add_ndwi,\n            nan=nan_fill,\n            normalizing_dict=self.normalizing_dict,\n            days_per_timestep=days_per_timestep,\n        )\n\n        if with_forecaster:\n            input_data.x = input_data.x[:, : self.hparams.input_months, :]\n\n        predictions: List[np.ndarray] = []\n        cur_i = 0\n\n        pbar = tqdm(total=input_data.x.shape[0] - 1)\n        while cur_i < (input_data.x.shape[0] - 1):\n\n            batch_x_np = input_data.x[cur_i: cur_i + batch_size]\n            if self.hparams.remove_b1_b10:\n                batch_x_np = CropDataset._remove_bands(batch_x_np)\n            batch_x = torch.from_numpy(batch_x_np).float()\n\n            if use_gpu and (device is not None):\n                batch_x = batch_x.to(device)\n\n            with torch.no_grad():\n                if with_forecaster:\n                    batch_x_next = self.forecaster(batch_x)\n                    batch_x = torch.cat((batch_x, batch_x_next), dim=1)\n\n                batch_preds = self.classifier(batch_x)\n\n                if self.hparams.multi_headed:\n                    global_preds, local_preds = batch_preds\n\n                    if local_head:\n                        batch_preds = local_preds\n                    else:\n                        batch_preds = global_preds\n\n                # back to the CPU, if necessary\n                batch_preds = batch_preds.cpu()\n\n            predictions.append(cast(torch.Tensor, batch_preds).numpy())\n            cur_i += batch_size\n            pbar.update(batch_size)\n\n        all_preds = np.concatenate(predictions, axis=0)\n        if len(all_preds.shape) == 1:\n            all_preds = np.expand_dims(all_preds, axis=-1)\n\n        return preds_to_xr(all_preds, lats=input_data.lat, lons=input_data.lon,)\n\n    def _output_metrics(\n        self, preds: np.ndarray, labels: np.ndarray, prefix: str = \"\"\n    ) -> Dict[str, float]:\n\n        if len(preds) == 0:\n            # sometimes this happens in the warmup\n            return {}\n\n        output_dict: Dict[str, float] = {}\n        if not (labels == labels[0]).all():\n            # This can happen when lightning does its warm up on a subset of the\n            # validation data\n            output_dict[f\"{prefix}roc_auc_score\"] = roc_auc_score(\n                labels, preds)\n\n        preds = (preds > PROBABILITY_THRESHOLD).astype(int)\n\n        output_dict[f\"{prefix}precision_score\"] = precision_score(\n            labels, preds)\n        output_dict[f\"{prefix}recall_score\"] = recall_score(labels, preds)\n        output_dict[f\"{prefix}f1_score\"] = f1_score(labels, preds)\n        output_dict[f\"{prefix}accuracy\"] = accuracy_score(labels, preds)\n\n        return output_dict\n\n    def add_noise(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n        if (self.hparams.noise_factor == 0) or (not training):\n            return x\n\n        # expect input to be of shape [timesteps, bands]\n        # and to be normalized with mean 0, std=1\n        # if its not, it means no norm_dict was passed, so lets\n        # just assume std=1\n        noise = torch.normal(0, 1, size=x.shape).float() * \\\n            self.hparams.noise_factor\n\n        # the added noise is the same per band, so that the temporal relationships\n        # are preserved\n        # noise_per_timesteps = noise.repeat(x.shape[0], 1)\n        return x + noise\n\n    def _split_preds_and_get_loss(\n        self, batch, add_preds: bool, loss_label: str, log_loss: bool, training: bool\n    ) -> Dict:\n\n        x, label, is_global = batch\n\n        input_to_encode = x[:, : self.hparams.input_months, :]\n\n        if self.hparams.forecast:\n            # we will predict every timestep except the first one\n            output_to_predict = x[:, 1:, :]\n            encoder_output = self.forecaster(input_to_encode)\n            encoder_loss = self.forecaster_loss(\n                encoder_output, output_to_predict)\n            loss: Union[float, torch.Tensor] = encoder_loss\n\n            final_encoded_input = torch.cat(\n                (\n                    (\n                        self.add_noise(input_to_encode, training),\n                        # -1 because the encoder output has no value for the 0th\n                        # timestep\n                        encoder_output[:, self.hparams.input_months - 1:, :],\n                    )\n                ),\n                dim=1,\n            )\n\n            output_dict = {}\n            if add_preds:\n                output_dict.update(\n                    {\"encoder_prediction\": encoder_output,\n                        \"encoder_target\": output_to_predict, }\n                )\n            if log_loss:\n                output_dict[\"log\"] = {}\n\n            # we now repeat label and is_global\n            x = torch.cat((self.add_noise(x, training),\n                          final_encoded_input), dim=0)\n            label = torch.cat((label, label), dim=0)\n            is_global = torch.cat((is_global, is_global), dim=0)\n        else:\n            loss = 0\n            output_dict = {}\n            if log_loss:\n                output_dict[\"log\"] = {}\n            x = self.add_noise(input_to_encode, training=training)\n\n        if self.hparams.multi_headed:\n            org_global_preds, local_preds = self.classifier(x)\n            global_preds = org_global_preds[is_global != 0]\n            global_labels = label[is_global != 0]\n\n            local_preds = local_preds[is_global == 0]\n            local_labels = label[is_global == 0]\n\n            if local_preds.shape[0] > 0:\n                local_loss = self.local_loss_function(\n                    local_preds.squeeze(-1), local_labels,)\n                loss += local_loss\n\n            if global_preds.shape[0] > 0:\n                global_loss = self.global_loss_function(\n                    global_preds.squeeze(-1), global_labels,)\n\n                num_local_labels = local_preds.shape[0]\n                if num_local_labels == 0:\n                    alpha = 1\n                else:\n                    ratio = global_preds.shape[0] / num_local_labels\n                    alpha = ratio / self.hparams.alpha\n                loss += alpha * global_loss\n\n            output_dict[loss_label] = loss\n            if log_loss:\n                output_dict[\"log\"][loss_label] = loss\n            if add_preds:\n                output_dict.update(\n                    {\n                        \"global_pred\": global_preds,\n                        \"global_label\": global_labels,\n                        \"kenya_pred\": local_preds,\n                        \"kenya_label\": local_labels,\n                    }\n                )\n            return output_dict\n        else:\n            preds = cast(torch.Tensor, self.classifier(x))\n\n            loss += self.global_loss_function(\n                input=preds.squeeze(-1), target=label,)\n\n            output_dict = {loss_label: loss}\n            if log_loss:\n                output_dict[\"log\"][loss_label] = loss\n            if add_preds:\n                output_dict.update({\"pred\": preds, \"label\": label})\n            return output_dict\n\n    def training_step(self, batch, batch_idx):\n        return self._split_preds_and_get_loss(\n            batch, add_preds=False, loss_label=\"loss\", log_loss=True, training=True\n        )\n\n    def validation_step(self, batch, batch_idx):\n        return self._split_preds_and_get_loss(\n            batch, add_preds=True, loss_label=\"val_loss\", log_loss=True, training=False\n        )\n\n    def test_step(self, batch, batch_idx):\n        return self._split_preds_and_get_loss(\n            batch, add_preds=True, loss_label=\"test_loss\", log_loss=True, training=False\n        )\n\n    @staticmethod\n    def _split_tensor(outputs, label) -> Tuple[np.ndarray, np.ndarray]:\n        encoded_all, unencoded_all = [], []\n        for x in outputs:\n            # the first half is unencoded, the second is encoded\n            total = x[label]\n            unencoded_all.append(total[: total.shape[0] // 2])\n            encoded_all.append(total[total.shape[0] // 2:])\n        return (\n            torch.cat(unencoded_all).detach().cpu().numpy(),\n            torch.cat(encoded_all).detach().cpu().numpy(),\n        )\n\n    def _interpretable_metrics(self, outputs, input_prefix: str, output_prefix: str) -> Dict:\n\n        output_dict = {}\n\n        if self.hparams.forecast:\n            u_labels, e_labels = self._split_tensor(\n                outputs, f\"{input_prefix}label\")\n\n            u_preds, e_preds = self._split_tensor(\n                outputs, f\"{input_prefix}pred\")\n        else:\n            u_preds = torch.cat([x[f\"{input_prefix}pred\"]\n                                for x in outputs]).detach().cpu().numpy()\n            u_labels = (\n                torch.cat([x[f\"{input_prefix}label\"]\n                          for x in outputs]).detach().cpu().numpy()\n            )\n\n        output_dict.update(\n            self._output_metrics(\n                u_preds, u_labels, f\"unencoded_{output_prefix}{input_prefix}\")\n        )\n\n        if self.hparams.forecast:\n            output_dict.update(\n                self._output_metrics(\n                    e_preds, e_labels, f\"encoded_{output_prefix}{input_prefix}\")\n            )\n\n        return output_dict\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        logs = {\"val_loss\": avg_loss}\n        if self.hparams.forecast:\n            encoder_pred = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_prediction\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n            encoder_target = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_target\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n\n            logs[\"val_encoder_mae\"] = mean_absolute_error(\n                encoder_target, encoder_pred)\n\n        if self.hparams.multi_headed:\n            logs.update(self._interpretable_metrics(\n                outputs, \"global_\", \"val_\"))\n            logs.update(self._interpretable_metrics(outputs, \"kenya_\", \"val_\"))\n        else:\n            logs.update(self._interpretable_metrics(outputs, \"\", \"val_\"))\n        return {\"val_loss\": avg_loss, \"log\": logs}\n\n    def test_epoch_end(self, outputs):\n\n        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean().item()\n        output_dict = {\"val_loss\": avg_loss}\n\n        if self.hparams.forecast:\n            encoder_pred = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_prediction\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n            encoder_target = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_target\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n\n            output_dict[\"test_encoder_mae\"] = mean_absolute_error(\n                encoder_target, encoder_pred)\n\n        if self.hparams.multi_headed:\n            output_dict.update(self._interpretable_metrics(\n                outputs, \"global_\", \"test_\"))\n            output_dict.update(self._interpretable_metrics(\n                outputs, \"kenya_\", \"test_\"))\n        else:\n            output_dict.update(\n                self._interpretable_metrics(outputs, \"\", \"test_\"))\n\n        return {\"progress_bar\": output_dict}\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--data_folder\": (str, str(Path(\"../data\"))),\n            \"--learning_rate\": (float, 0.001),\n            \"--batch_size\": (int, 64),\n            \"--input_months\": (int, 5),\n            \"--alpha\": (float, 10),\n            \"--noise_factor\": (float, 0.1),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--remove_b1_b10\",\n                            dest=\"remove_b1_b10\", action=\"store_true\")\n        parser.add_argument(\n            \"--keep_b1_b10\", dest=\"remove_b1_b10\", action=\"store_false\")\n        parser.set_defaults(remove_b1_b10=True)\n\n        parser.add_argument(\"--forecast\", dest=\"forecast\", action=\"store_true\")\n        parser.add_argument(\"--do_not_forecast\",\n                            dest=\"forecast\", action=\"store_false\")\n        parser.set_defaults(forecast=True)\n\n        parser.add_argument(\"--cache\", dest=\"cache\", action=\"store_true\")\n        parser.add_argument(\"--do_not_cache\", dest=\"cache\",\n                            action=\"store_false\")\n        parser.set_defaults(cache=True)\n\n        parser.add_argument(\"--include_geowiki\",\n                            dest=\"include_geowiki\", action=\"store_true\")\n        parser.add_argument(\"--exclude_geowiki\",\n                            dest=\"include_geowiki\", action=\"store_false\")\n        parser.set_defaults(include_geowiki=True)\n\n        parser.add_argument(\"--upsample\", dest=\"upsample\", action=\"store_true\")\n        parser.add_argument(\"--do_not_upsample\",\n                            dest=\"upsample\", action=\"store_false\")\n        parser.set_defaults(upsample=True)\n\n        classifier_parser = Classifier.add_model_specific_args(parser)\n        return Forecaster.add_model_specific_args(classifier_parser)\n",
  "history_output" : "",
  "history_begin_time" : 1647347143698,
  "history_end_time" : 1647347144362,
  "history_notes" : null,
  "history_process" : "9ardvx",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3cqz48mj8ao",
  "history_input" : "from argparse import Namespace\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n\ndef train_model(model: pl.LightningModule, hparams: Namespace) -> pl.LightningModule:\n    early_stop_callback = EarlyStopping(\n        monitor=\"val_loss\", min_delta=0.00, patience=hparams.patience, verbose=True, mode=\"min\",\n    )\n    trainer = pl.Trainer(\n        default_save_path=hparams.data_folder,\n        max_epochs=hparams.max_epochs,\n        early_stop_callback=early_stop_callback,\n    )\n    trainer.fit(model)\n\n    return model\n",
  "history_output" : "",
  "history_begin_time" : 1647347137141,
  "history_end_time" : 1647347139799,
  "history_notes" : null,
  "history_process" : "o0vujj",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "pte9gvuh56w",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nimport numpy as np\nimport xarray as xr\nimport pandas as pd\n\nfrom src_engineer_base import BaseEngineer\n\nfrom typing import cast, Dict, Optional, Tuple\n\n\n@dataclass\nclass TestInstance:\n    x: np.ndarray\n    lat: np.ndarray\n    lon: np.ndarray\n\n\ndef tif_to_np(\n    path_to_dataset: Path,\n    add_ndvi: bool,\n    add_ndwi: bool,\n    nan: float,\n    normalizing_dict: Optional[Dict[str, np.ndarray]],\n    days_per_timestep: int,\n) -> TestInstance:\n\n    _, start_date, _ = cast(\n        Tuple[str, datetime, datetime],\n        BaseEngineer.process_filename(path_to_dataset.name, include_extended_filenames=True),\n    )\n\n    x = BaseEngineer.load_tif(\n        path_to_dataset, days_per_timestep=days_per_timestep, start_date=start_date\n    )\n\n    lon, lat = np.meshgrid(x.x.values, x.y.values)\n    flat_lat, flat_lon = (\n        np.squeeze(lat.reshape(-1, 1), -1),\n        np.squeeze(lon.reshape(-1, 1), -1),\n    )\n\n    x_np = x.values\n    x_np = x_np.reshape(x_np.shape[0], x_np.shape[1], x_np.shape[2] * x_np.shape[3])\n    x_np = np.moveaxis(x_np, -1, 0)\n\n    if add_ndvi:\n        x_np = BaseEngineer.calculate_ndvi(x_np, num_dims=3)\n    if add_ndwi:\n        x_np = BaseEngineer.calculate_ndwi(x_np, num_dims=3)\n\n    x_np = BaseEngineer.maxed_nan_to_num(x_np, nan=nan)\n\n    if normalizing_dict is not None:\n        x_np = (x_np - normalizing_dict[\"mean\"]) / normalizing_dict[\"std\"]\n\n    return TestInstance(x=x_np, lat=flat_lat, lon=flat_lon)\n\n\ndef preds_to_xr(predictions: np.ndarray, lats: np.ndarray, lons: np.ndarray) -> xr.Dataset:\n\n    data_dict: Dict[str, np.ndarray] = {\"lat\": lats, \"lon\": lons}\n\n    for prediction_idx in range(predictions.shape[1]):\n        prediction_label = f\"prediction_{prediction_idx}\"\n        data_dict[prediction_label] = predictions[:, prediction_idx]\n\n    return pd.DataFrame(data=data_dict).set_index([\"lat\", \"lon\"]).to_xarray()\n",
  "history_output" : "",
  "history_begin_time" : 1647347138751,
  "history_end_time" : 1647347138889,
  "history_notes" : null,
  "history_process" : "bhdtil",
  "host_id" : "100001",
  "indicator" : "Done"
}]
