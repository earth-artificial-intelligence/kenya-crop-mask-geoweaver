[{
  "history_id" : "i4dhtfmchdm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666167825386,
  "history_end_time" : 1666167825386,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7rr2rw8c0xl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666167772373,
  "history_end_time" : 1666167772373,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zoyt6jfco06",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666166465612,
  "history_end_time" : 1666166465612,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hyxueuzgvix",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666166410293,
  "history_end_time" : 1666166419946,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ykptapcz0cg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666164233100,
  "history_end_time" : 1666164233100,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jj3hsmcxnes",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666163607319,
  "history_end_time" : 1666163607319,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oihj937821b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666137012963,
  "history_end_time" : 1666137012963,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z75dnt9e9i5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666136593516,
  "history_end_time" : 1666136593516,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aems3bec9fx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666136435878,
  "history_end_time" : 1666136435878,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tseru1zjhwv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666134110801,
  "history_end_time" : 1666134110801,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lpje8hr4eib",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666134036954,
  "history_end_time" : 1666134036954,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "enq7j28hara",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666134009000,
  "history_end_time" : 1666134009000,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "phicqda8pqw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666131993231,
  "history_end_time" : 1666131993231,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oid6nr5gb2d",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1666118344938,
  "history_end_time" : 1666118346121,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tjb0dhs2lgs",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1655909926661,
  "history_end_time" : 1655909927499,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "wns8ft414qs",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1655908836314,
  "history_end_time" : 1655908837362,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0ivj34n8l4y",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1655907452210,
  "history_end_time" : 1655907452391,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "a8n8bmd9i5z",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_lstm.py\", line 3, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1655865867182,
  "history_end_time" : 1655865867331,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "pjr4mh7ccn3",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1655865102681,
  "history_end_time" : 1655865103051,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3gji2do20dk",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647347440812,
  "history_end_time" : 1647347441484,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ter54kf2ea1",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647347372845,
  "history_end_time" : 1647347373255,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bdduj7jdso0",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647347287380,
  "history_end_time" : 1647347287895,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "c7ab3quryw7",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647347141810,
  "history_end_time" : 1647347142427,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3eteqikm7cq",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647346841981,
  "history_end_time" : 1647346842660,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yj98n51cjz1",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1647346683778,
  "history_end_time" : 1647346684358,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "govwin8xwnm",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1646138301690,
  "history_end_time" : 1646138302292,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dlcpfir8kwi",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1646138202668,
  "history_end_time" : 1646138203378,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ag3saq59k87",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1646138109887,
  "history_end_time" : 1646138110617,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "odm8ak1ekl3",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1646137796838,
  "history_end_time" : 1646137797528,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0k4r3kq2j3n",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1646137708881,
  "history_end_time" : 1646137709202,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "glh1ydyryvr",
  "history_input" : "import math\n\nimport torch\nfrom torch import nn\n\nfrom typing import Tuple, Optional\n\n\nclass UnrolledLSTM(nn.Module):\n    def __init__(\n        self, input_size: int, hidden_size: int, dropout: float, batch_first: bool\n    ) -> None:\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.hidden_size = hidden_size\n\n        self.rnn = UnrolledLSTMCell(\n            input_size=input_size, hidden_size=hidden_size, batch_first=batch_first\n        )\n        self.dropout = VariationalDropout(dropout)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        sequence_length = x.shape[1] if self.batch_first else x.shape[0]\n        batch_size = x.shape[0] if self.batch_first else x.shape[1]\n\n        if state is None:\n            # initialize to zeros\n            hidden, cell = (\n                torch.zeros(1, batch_size, self.hidden_size),\n                torch.zeros(1, batch_size, self.hidden_size),\n            )\n\n            if x.is_cuda:\n                hidden, cell = hidden.cuda(), cell.cuda()\n        else:\n            hidden, cell = state\n\n        outputs = []\n        for i in range(sequence_length):\n            input_x = x[:, i, :].unsqueeze(1)\n            _, (hidden, cell) = self.rnn(input_x, (hidden, cell))\n            outputs.append(hidden)\n\n            if self.training and (i == 0):\n                self.dropout.update_mask(hidden.shape, hidden.is_cuda)\n\n            hidden = self.dropout(hidden)\n\n        return torch.stack(outputs, dim=0), (hidden, cell)\n\n\nclass UnrolledLSTMCell(nn.Module):\n    \"\"\"An unrolled LSTM, so that dropout can be applied between\n    timesteps instead of between layers\n    \"\"\"\n\n    def __init__(self, input_size: int, hidden_size: int, batch_first: bool) -> None:\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.forget_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.update_candidates = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Tanh(),\n            ]\n        )\n\n        self.output_gate = nn.Sequential(\n            *[\n                nn.Linear(\n                    in_features=input_size + hidden_size, out_features=hidden_size, bias=True,\n                ),\n                nn.Sigmoid(),\n            ]\n        )\n\n        self.cell_state_activation = nn.Tanh()\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        sqrt_k = math.sqrt(1 / self.hidden_size)\n        for parameters in self.parameters():\n            for pam in parameters:\n                nn.init.uniform_(pam.data, -sqrt_k, sqrt_k)\n\n    def forward(  # type: ignore\n        self, x: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]\n    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        hidden, cell = state\n\n        if self.batch_first:\n            hidden, cell = torch.transpose(hidden, 0, 1), torch.transpose(cell, 0, 1)\n\n        forget_state = self.forget_gate(torch.cat((x, hidden), dim=-1))\n        update_state = self.update_gate(torch.cat((x, hidden), dim=-1))\n        cell_candidates = self.update_candidates(torch.cat((x, hidden), dim=-1))\n\n        updated_cell = (forget_state * cell) + (update_state * cell_candidates)\n\n        output_state = self.output_gate(torch.cat((x, hidden), dim=-1))\n        updated_hidden = output_state * self.cell_state_activation(updated_cell)\n\n        if self.batch_first:\n            updated_hidden = torch.transpose(updated_hidden, 0, 1)\n            updated_cell = torch.transpose(updated_cell, 0, 1)\n\n        return updated_hidden, (updated_hidden, updated_cell)\n\n\nclass VariationalDropout(nn.Module):\n    \"\"\"\n    This ensures the same dropout is applied to each timestep,\n    as described in https://arxiv.org/pdf/1512.05287.pdf\n    \"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n\n        self.p = p\n        self.mask = None\n\n    def update_mask(self, x_shape: Tuple, is_cuda: bool) -> None:\n        mask = torch.bernoulli(torch.ones(x_shape) * (1 - self.p)) / (1 - self.p)\n        if is_cuda:\n            mask = mask.cuda()\n        self.mask = mask\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if not self.training:\n            return x\n\n        return self.mask * x\n",
  "history_output" : "",
  "history_begin_time" : 1646137597664,
  "history_end_time" : 1646137598079,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ok89fm3rwic",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403263,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
