[{
  "history_id" : "fc1rczf90j8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666167825370,
  "history_end_time" : 1666167825370,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kwjfwtq98a8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666167772350,
  "history_end_time" : 1666167772350,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a8haupsp2lx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666166465603,
  "history_end_time" : 1666166465603,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v82tz7l13io",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666166410274,
  "history_end_time" : 1666166419944,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "40cc1bqlvks",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666164233089,
  "history_end_time" : 1666164233089,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r5e758ee2jq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666163607302,
  "history_end_time" : 1666163607302,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2b4av3t6icz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666137012950,
  "history_end_time" : 1666137012950,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0uonpkfaiup",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666136593507,
  "history_end_time" : 1666136593507,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c0go78enozg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666136435801,
  "history_end_time" : 1666136435801,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8cr0kfcjcnr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666134110776,
  "history_end_time" : 1666134110776,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3876pn8qfwk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666134036928,
  "history_end_time" : 1666134036928,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nn13yhmxkzp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666134008983,
  "history_end_time" : 1666134008983,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dfgcvnx08vs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666131993206,
  "history_end_time" : 1666131993206,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4d6ivri29fv",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1666118283795,
  "history_end_time" : 1666118287636,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5iumbtpw15g",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1655909885938,
  "history_end_time" : 1655909888170,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fe45s3por9k",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1655908830005,
  "history_end_time" : 1655908830466,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lga95hlcovq",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1655907441867,
  "history_end_time" : 1655907444854,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dbdlb5wu4et",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_engineer_base.py\", line 4, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655865868203,
  "history_end_time" : 1655865868375,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ig21cd33fns",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1655865099727,
  "history_end_time" : 1655865100684,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "f7i4pz5twad",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347443504,
  "history_end_time" : 1647347445686,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3g9dfwncgcn",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347347082,
  "history_end_time" : 1647347349116,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "v1k8n697lkg",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347285381,
  "history_end_time" : 1647347287241,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b7a2jociscz",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647347146366,
  "history_end_time" : 1647347146648,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "djo0nu8bp8u",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647346820015,
  "history_end_time" : 1647346820421,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "s7l1jvomra1",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647346659660,
  "history_end_time" : 1647346661566,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7dqhg9l57pf",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647345853990,
  "history_end_time" : 1647345855936,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "32wt3qs3axx",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647345669985,
  "history_end_time" : 1647345671468,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zow66ojj17q",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1647340132051,
  "history_end_time" : 1647340134104,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fwi9h5r3cuc",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646147727916,
  "history_end_time" : 1646147730711,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cet9lyngq4p",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646138280348,
  "history_end_time" : 1646138282994,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ekqjmmfdoap",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646138180741,
  "history_end_time" : 1646138183040,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ji00io0a3dv",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646138106760,
  "history_end_time" : 1646138108837,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lef3k6opvbi",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646137792626,
  "history_end_time" : 1646137794888,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b3vofimoh3t",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646137694823,
  "history_end_time" : 1646137696985,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "frag5r98nqo",
  "history_input" : "from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nimport pickle\nfrom tqdm import tqdm\nimport warnings\nimport xarray as xr\n\nfrom typing import Dict, List, Tuple, Optional, Union\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\nfrom src_utils import set_seed\nfrom src_utils import BoundingBox\n\n\n@dataclass\nclass DataInstance:\n    label_lat: float\n    label_lon: float\n    instance_lat: float\n    instance_lon: float\n    labelled_array: np.ndarray\n    is_crop: bool\n    dataset: str\n\n    def isin(self, bounding_box: BoundingBox) -> bool:\n        return (\n            (self.instance_lon <= bounding_box.max_lon)\n            & (self.instance_lon >= bounding_box.min_lon)\n            & (self.instance_lat <= bounding_box.max_lat)\n            & (self.instance_lat >= bounding_box.min_lat)\n        )\n\n\nclass BaseEngineer(ABC):\n    r\"\"\"Combine earth engine sentinel data\n    and geowiki landcover 2017 data to make\n    numpy arrays which can be input into the\n    machine learning model\n    \"\"\"\n\n    sentinel_dataset: str\n    dataset: str\n\n    # should be True if the dataset contains data which will\n    # only be used for evaluation (e.g. the TogoEvaluation dataset)\n    eval_only: bool = False\n\n    def __init__(self, data_folder: Path) -> None:\n        set_seed()\n        self.data_folder = data_folder\n        self.geospatial_files = self.get_geospatial_files(data_folder)\n        self.labels = self.read_labels(data_folder)\n\n        self.savedir = self.data_folder / \"features\" / self.dataset\n        self.savedir.mkdir(exist_ok=True, parents=True)\n\n        self.normalizing_dict_interim: Dict[str, Union[np.ndarray, int]] = {\"n\": 0}\n\n    def get_geospatial_files(self, data_folder: Path) -> List[Path]:\n        sentinel_files = data_folder / \"raw\" / self.sentinel_dataset\n        return list(sentinel_files.glob(\"*.tif\"))\n\n    @staticmethod\n    @abstractmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        raise NotImplementedError\n\n    @staticmethod\n    def find_nearest(array, value: float) -> Tuple[float, int]:\n        array = np.asarray(array)\n        idx = (np.abs(array - value)).argmin()\n        return array[idx], idx\n\n    @staticmethod\n    def process_filename(\n        filename: str, include_extended_filenames: bool\n    ) -> Optional[Tuple[str, datetime, datetime]]:\n        r\"\"\"\n        Given an exported sentinel file, process it to get the start\n        and end dates of the data. This assumes the filename ends with '.tif'\n        \"\"\"\n        date_format = \"%Y-%m-%d\"\n\n        identifier, start_date_str, end_date_str = filename[:-4].split(\"_\")\n\n        start_date = datetime.strptime(start_date_str, date_format)\n\n        try:\n            end_date = datetime.strptime(end_date_str, date_format)\n            return identifier, start_date, end_date\n\n        except ValueError:\n            if include_extended_filenames:\n                end_list = end_date_str.split(\"-\")\n                end_year, end_month, end_day = (\n                    end_list[0],\n                    end_list[1],\n                    end_list[2],\n                )\n\n                # if we allow extended filenames, we want to\n                # differentiate them too\n                id_number = end_list[3]\n                identifier = f\"{identifier}-{id_number}\"\n\n                return (\n                    identifier,\n                    start_date,\n                    datetime(int(end_year), int(end_month), int(end_day)),\n                )\n            else:\n                print(f\"Unexpected filename {filename} - skipping\")\n                return None\n\n    @staticmethod\n    def load_tif(filepath: Path, start_date: datetime, days_per_timestep: int) -> xr.DataArray:\n        r\"\"\"\n        The sentinel files exported from google earth have all the timesteps\n        concatenated together. This function loads a tif files and splits the\n        timesteps\n        \"\"\"\n\n        # this mirrors the eo-learn approach\n        # also, we divide by 10,000, to remove the scaling factor\n        # https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2\n        da = xr.open_rasterio(filepath).rename(\"FEATURES\") / 10000\n\n        da_split_by_time: List[xr.DataArray] = []\n\n        bands_per_timestep = len(BANDS)\n        num_bands = len(da.band)\n\n        assert (\n            num_bands % bands_per_timestep == 0\n        ), f\"Total number of bands not divisible by the expected bands per timestep\"\n\n        cur_band = 0\n        while cur_band + bands_per_timestep <= num_bands:\n            time_specific_da = da.isel(band=slice(cur_band, cur_band + bands_per_timestep))\n            time_specific_da[\"band\"] = range(bands_per_timestep)\n            da_split_by_time.append(time_specific_da)\n            cur_band += bands_per_timestep\n\n        timesteps = [\n            start_date + timedelta(days=days_per_timestep) * i for i in range(len(da_split_by_time))\n        ]\n\n        combined = xr.concat(da_split_by_time, pd.Index(timesteps, name=\"time\"))\n        combined.attrs[\"band_descriptions\"] = BANDS\n\n        return combined\n\n    @staticmethod\n    def update_normalizing_values(\n        norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n        # given an input array of shape [timesteps, bands]\n        # update the normalizing dict\n        # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n        # https://www.johndcook.com/blog/standard_deviation/\n        num_bands = array.shape[1]\n\n        # initialize\n        if \"mean\" not in norm_dict:\n            norm_dict[\"mean\"] = np.zeros(num_bands)\n            norm_dict[\"M2\"] = np.zeros(num_bands)\n\n        for time_idx in range(array.shape[0]):\n            norm_dict[\"n\"] += 1\n\n            x = array[time_idx, :]\n\n            delta = x - norm_dict[\"mean\"]\n            norm_dict[\"mean\"] += delta / norm_dict[\"n\"]\n            norm_dict[\"M2\"] += delta * (x - norm_dict[\"mean\"])\n\n    def update_batch_normalizing_values(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]], array: np.ndarray\n    ) -> None:\n\n        assert len(array.shape) == 3, \"Expected array of shape [batch, timesteps, bands]\"\n\n        for idx in range(array.shape[0]):\n            subarray = array[idx, :, :]\n            self.update_normalizing_values(norm_dict, subarray)\n\n    def calculate_normalizing_dict(\n        self, norm_dict: Dict[str, Union[np.ndarray, int]]\n    ) -> Optional[Dict[str, np.ndarray]]:\n\n        if \"mean\" not in norm_dict:\n            print(\"No normalizing dict calculated! Make sure to call update_normalizing_values\")\n            return None\n\n        variance = norm_dict[\"M2\"] / (norm_dict[\"n\"] - 1)\n        std = np.sqrt(variance)\n        return {\"mean\": norm_dict[\"mean\"], \"std\": std}\n\n    @staticmethod\n    def maxed_nan_to_num(\n        array: np.ndarray, nan: float, max_ratio: Optional[float] = None\n    ) -> Optional[np.ndarray]:\n\n        if max_ratio is not None:\n            num_nan = np.count_nonzero(np.isnan(array))\n            if (num_nan / array.size) > max_ratio:\n                return None\n        return np.nan_to_num(array, nan=nan)\n\n    @abstractmethod\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        raise NotImplementedError\n\n    @staticmethod\n    def _calculate_difference_index(\n        input_array: np.ndarray, num_dims: int, band_1: str, band_2: str\n    ) -> np.ndarray:\n\n        if num_dims == 2:\n            band_1_np = input_array[:, BANDS.index(band_1)]\n            band_2_np = input_array[:, BANDS.index(band_2)]\n        elif num_dims == 3:\n            band_1_np = input_array[:, :, BANDS.index(band_1)]\n            band_2_np = input_array[:, :, BANDS.index(band_2)]\n        else:\n            raise ValueError(f\"Expected num_dims to be 2 or 3 - got {num_dims}\")\n\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in true_divide\")\n            # suppress the following warning\n            # RuntimeWarning: invalid value encountered in true_divide\n            # for cases where near_infrared + red == 0\n            # since this is handled in the where condition\n            ndvi = np.where(\n                (band_1_np + band_2_np) > 0, (band_1_np - band_2_np) / (band_1_np + band_2_np), 0,\n            )\n        return np.append(input_array, np.expand_dims(ndvi, -1), axis=-1)\n\n    @classmethod\n    def calculate_ndvi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b08 - b04) / (b08 + b04)\n        \"\"\"\n\n        return cls._calculate_difference_index(input_array, num_dims, \"B8\", \"B4\")\n\n    @classmethod\n    def calculate_ndwi(cls, input_array: np.ndarray, num_dims: int = 2) -> np.ndarray:\n        r\"\"\"\n        Given an input array of shape [timestep, bands] or [batches, timesteps, bands]\n        where bands == len(BANDS), returns an array of shape\n        [timestep, bands + 1] where the extra band is NDVI,\n        (b03 - b8A) / (b3 + b8a)\n        \"\"\"\n        return cls._calculate_difference_index(input_array, num_dims, \"B3\", \"B8A\")\n\n    def engineer(\n        self,\n        val_set_size: float = 0.1,\n        test_set_size: float = 0.1,\n        nan_fill: float = 0.0,\n        max_nan_ratio: float = 0.3,\n        checkpoint: bool = True,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        include_extended_filenames: bool = True,\n        calculate_normalizing_dict: bool = True,\n        days_per_timestep: int = 30,\n    ):\n        for file_path in tqdm(self.geospatial_files):\n\n            file_info = self.process_filename(\n                file_path.name, include_extended_filenames=include_extended_filenames\n            )\n\n            if file_info is None:\n                continue\n\n            identifier, start_date, end_date = file_info\n\n            file_name = f\"{identifier}_{str(start_date.date())}_{str(end_date.date())}\"\n\n            if checkpoint:\n                # we check if the file has already been written\n                if (\n                    (self.savedir / \"validation\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"training\" / f\"{file_name}.pkl\").exists()\n                    or (self.savedir / \"testing\" / f\"{file_name}.pkl\").exists()\n                ):\n                    continue\n\n            if self.eval_only:\n                data_subset = \"testing\"\n            else:\n                random_float = np.random.uniform()\n                # we split into (val, test, train)\n                if random_float <= (val_set_size + test_set_size):\n                    if random_float <= val_set_size:\n                        data_subset = \"validation\"\n                    else:\n                        data_subset = \"testing\"\n                else:\n                    data_subset = \"training\"\n\n            instance = self.process_single_file(\n                file_path,\n                nan_fill=nan_fill,\n                max_nan_ratio=max_nan_ratio,\n                add_ndvi=add_ndvi,\n                add_ndwi=add_ndwi,\n                calculate_normalizing_dict=calculate_normalizing_dict,\n                start_date=start_date,\n                days_per_timestep=days_per_timestep,\n                is_test=True if data_subset == \"testing\" else False,\n            )\n            if instance is not None:\n                subset_path = self.savedir / data_subset\n                subset_path.mkdir(exist_ok=True)\n                save_path = subset_path / f\"{file_name}.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(instance, f)\n\n        if calculate_normalizing_dict:\n            normalizing_dict = self.calculate_normalizing_dict(\n                norm_dict=self.normalizing_dict_interim\n            )\n\n            if normalizing_dict is not None:\n                save_path = self.savedir / \"normalizing_dict.pkl\"\n                with save_path.open(\"wb\") as f:\n                    pickle.dump(normalizing_dict, f)\n            else:\n                print(\"No normalizing dict calculated!\")\n",
  "history_output" : "",
  "history_begin_time" : 1646137580794,
  "history_end_time" : 1646137582535,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lxgjczg35ad",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403143,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
