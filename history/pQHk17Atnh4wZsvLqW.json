[{
  "history_id" : "65guns9pqhy",
  "history_input" : "import sys\nfrom pathlib import Path\nfrom datetime import date\nimport os\n\nsys.path.append(\"..\")\n\nfrom src_exporters_geowiki import *\nfrom src_exporters_sentinel_geowiki import *\nfrom src_exporters_sentinel_pv_kenya import *\nfrom src_exporters_sentinel_kenya_non_crop import *\nfrom src_exporters_sentinel_region import *\nfrom src_exporters_sentinel_utils import *\n\n#from scripts_process import *\n\n\ndef export_geowiki():\n    if len(os.listdir('../data/raw/geowiki_landcover_2017')) == 0:\n        exporter = GeoWikiExporter(Path(\"../data\"))\n        exporter.export()\n\n\ndef export_geowiki_sentinel_ee():\n    if len(os.listdir('../data/raw/earth_engine_geowiki')) == 0:\n        exporter = GeoWikiSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_plant_village_sentinel_ee():\n    if len(os.listdir('../data/raw/earth_engine_plant_village_kenya')) == 0:\n        exporter = KenyaPVSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_kenya_non_crop():\n    if len(os.listdir('../data/raw/earth_engine_kenya_non_crop')) == 0:\n        exporter = KenyaNonCropSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_region():\n    if len(os.listdir('../data/raw/earth_engine_region_busia_partial_slow_cloudfree')) == 0:\n        exporter = RegionalExporter(Path(\"../data\"))\n        exporter.export_for_region(\n            region_name=\"Busia\",\n            end_date=date(2020, 9, 13),\n            num_timesteps=5,\n            monitor=False,\n            checkpoint=True,\n            metres_per_polygon=None,\n            fast=False,\n        )\n\n\nif __name__ == \"__main__\":\n    print(\"starting export_geowiki()...\")\n    export_geowiki()\n    print(\"Done export_geowiki()!\")\n    print(\"starting process_geowiki()...\")\n    #process_geowiki()\n    print(\"Done process_geowiki()!\")\n    print(\"starting export_geowiki_sentinel_ee()...this could take a while\")\n    export_geowiki_sentinel_ee()\n    print(\"Done export_geowiki_sentinel_ee()!\")\n    print(\"starting process_plantvillage()...\")\n    #process_plantvillage()\n    print(\"Done process_plantvillage()!\")\n    print(\"starting export_plant_village_sentinel_ee()...\")\n    export_plant_village_sentinel_ee()\n    print(\"Done export_plant_village_sentinel_ee()!\")\n    print(\"starting process_kenya_noncrop()...\")\n    #process_kenya_noncrop()\n    print(\"Done process_kenya_noncrop()!\")\n    print(\"starting export_kenya_non_crop()...\")\n    #export_kenya_non_crop()\n    print(\"Done export_kenya_non_crop()!\")\n    print(\"starting export_region()...\")\n    #export_region()\n    print(\"Done export_region()!\")\n",
  "history_output" : "starting export_geowiki()...\nDone export_geowiki()!\nstarting process_geowiki()...\nDone process_geowiki()!\nstarting export_geowiki_sentinel_ee()...this could take a while\nDone export_geowiki_sentinel_ee()!\nstarting process_plantvillage()...\nDone process_plantvillage()!\nstarting export_plant_village_sentinel_ee()...\nDone export_plant_village_sentinel_ee()!\nstarting process_kenya_noncrop()...\nDone process_kenya_noncrop()!\nstarting export_kenya_non_crop()...\nDone export_kenya_non_crop()!\nstarting export_region()...\nDone export_region()!\n",
  "history_begin_time" : 1646144595167,
  "history_end_time" : 1646144598509,
  "history_notes" : null,
  "history_process" : "gpetwx",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nlgh4i4hsxy",
  "history_input" : "from pathlib import Path\n\nfrom typing import Any, Dict\n\n\nclass BaseExporter:\n    r\"\"\"Base for all exporter classes. It creates the appropriate\n    directory in the data dir (``data_dir/raw/{dataset}``).\n\n    All classes which extend this should implement an export function.\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n    default_args_dict: Dict[str, Any] = {}\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n\n        self.data_folder = data_folder\n\n        self.raw_folder = self.data_folder / \"raw\"\n        self.output_folder = self.raw_folder / self.dataset\n        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
  "history_output" : "",
  "history_begin_time" : 1646144599673,
  "history_end_time" : 1646144599762,
  "history_notes" : null,
  "history_process" : "4q2yxd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ros6t5zl985",
  "history_input" : "from pathlib import Path\nimport urllib.request\nimport zipfile\n\nfrom src_exporters_base import BaseExporter\n\n\nclass GeoWikiExporter(BaseExporter):\n    r\"\"\"\n    Download the GeoWiki labels\n    \"\"\"\n\n    dataset = \"geowiki_landcover_2017\"\n\n    download_urls = [\n        \"http://store.pangaea.de/Publications/See_2017/crop_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_exp.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all_2.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_exp.zip\",\n    ]\n\n    @staticmethod\n    def download_file(url: str, output_folder: Path, remove_zip: bool = True) -> None:\n\n        filename = url.split(\"/\")[-1]\n        output_path = output_folder / filename\n\n        if output_path.exists():\n            print(f\"{filename} already exists! Skipping\")\n            return None\n\n        print(f\"Downloading {url}\")\n        urllib.request.urlretrieve(url, output_path)\n\n        if filename.endswith(\"zip\"):\n\n            print(f\"Downloaded! Unzipping to {output_folder}\")\n            with zipfile.ZipFile(output_path, \"r\") as zip_file:\n                zip_file.extractall(output_folder)\n\n            if remove_zip:\n                print(\"Deleting zip file\")\n                (output_path).unlink()\n\n    def export(self, remove_zip: bool = False) -> None:\n        r\"\"\"\n        Download the GeoWiki labels\n        :param remove_zip: Whether to remove the zip file once it has been expanded\n        \"\"\"\n        for file_url in self.download_urls:\n            self.download_file(file_url, self.output_folder, remove_zip)\n",
  "history_output" : "",
  "history_begin_time" : 1646144600035,
  "history_end_time" : 1646144600161,
  "history_notes" : null,
  "history_process" : "jonz77",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "evxcotz7q56",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import date\nfrom math import cos, radians\nimport ee\n\nfrom typing import List, Tuple, Union\n\nfrom src_utils import BoundingBox\n\n\ndef date_overlap(start1: date, end1: date, start2: date, end2: date) -> int:\n    overlaps = start1 <= end2 and end1 >= start2\n    if not overlaps:\n        return 0\n    return (min(end1, end2) - max(start1, start2)).days\n\n\ndef metre_per_degree(mid_lat: float) -> Tuple[float, float]:\n    # https://gis.stackexchange.com/questions/75528/understanding-terms-in-length-of-degree-formula\n    # see the link above to explain the magic numbers\n    m_per_deg_lat = 111132.954 - 559.822 * cos(2.0 * mid_lat) + 1.175 * cos(radians(4.0 * mid_lat))\n    m_per_deg_lon = (3.14159265359 / 180) * 6367449 * cos(radians(mid_lat))\n\n    return m_per_deg_lat, m_per_deg_lon\n\n\n@dataclass\nclass EEBoundingBox(BoundingBox):\n    r\"\"\"\n    A bounding box with additional earth-engine specific\n    functionality\n    \"\"\"\n\n    def to_ee_polygon(self) -> ee.Geometry.Polygon:\n        return ee.Geometry.Polygon(\n            [\n                [\n                    [self.min_lon, self.min_lat],\n                    [self.min_lon, self.max_lat],\n                    [self.max_lon, self.max_lat],\n                    [self.max_lon, self.min_lat],\n                ]\n            ]\n        )\n\n    def to_metres(self) -> Tuple[float, float]:\n        r\"\"\"\n        :return: [lat metres, lon metres]\n        \"\"\"\n        # https://gis.stackexchange.com/questions/75528/understanding-terms-in-length-of-degree-formula\n        mid_lat = (self.min_lat + self.max_lat) / 2.0\n        m_per_deg_lat, m_per_deg_lon = metre_per_degree(mid_lat)\n\n        delta_lat = self.max_lat - self.min_lat\n        delta_lon = self.max_lon - self.min_lon\n\n        return delta_lat * m_per_deg_lat, delta_lon * m_per_deg_lon\n\n    def to_polygons(self, metres_per_patch: int = 3300) -> List[ee.Geometry.Polygon]:\n\n        lat_metres, lon_metres = self.to_metres()\n\n        num_cols = int(lon_metres / metres_per_patch)\n        num_rows = int(lat_metres / metres_per_patch)\n\n        print(f\"Splitting into {num_cols} columns and {num_rows} rows\")\n\n        lon_size = (self.max_lon - self.min_lon) / num_cols\n        lat_size = (self.max_lat - self.min_lat) / num_rows\n\n        output_polygons: List[ee.Geometry.Polygon] = []\n\n        cur_lon = self.min_lon\n        while cur_lon < self.max_lon:\n            cur_lat = self.min_lat\n            while cur_lat < self.max_lat:\n                output_polygons.append(\n                    ee.Geometry.Polygon(\n                        [\n                            [\n                                [cur_lon, cur_lat],\n                                [cur_lon, cur_lat + lat_size],\n                                [cur_lon + lon_size, cur_lat + lat_size],\n                                [cur_lon + lon_size, cur_lat],\n                            ]\n                        ]\n                    )\n                )\n                cur_lat += lat_size\n            cur_lon += lon_size\n\n        return output_polygons\n\n\ndef bounding_box_from_centre(\n    mid_lat: float, mid_lon: float, surrounding_metres: Union[int, Tuple[int, int]]\n) -> EEBoundingBox:\n\n    m_per_deg_lat, m_per_deg_lon = metre_per_degree(mid_lat)\n\n    if isinstance(surrounding_metres, int):\n        surrounding_metres = (surrounding_metres, surrounding_metres)\n\n    surrounding_lat, surrounding_lon = surrounding_metres\n\n    deg_lat = surrounding_lat / m_per_deg_lat\n    deg_lon = surrounding_lon / m_per_deg_lon\n\n    max_lat, min_lat = mid_lat + deg_lat, mid_lat - deg_lat\n    max_lon, min_lon = mid_lon + deg_lon, mid_lon - deg_lon\n\n    return EEBoundingBox(max_lon=max_lon, min_lon=min_lon, max_lat=max_lat, min_lat=min_lat)\n\n\ndef bounding_box_to_earth_engine_bounding_box(bounding_box: BoundingBox,) -> EEBoundingBox:\n    return EEBoundingBox(\n        max_lat=bounding_box.max_lat,\n        min_lat=bounding_box.min_lat,\n        max_lon=bounding_box.max_lon,\n        min_lon=bounding_box.min_lon,\n    )\n\n\ndef cancel_all_tasks() -> None:\n\n    ee.Initialize()\n\n    tasks = ee.batch.Task.list()\n    print(f\"Cancelling up to {len(tasks)} tasks\")\n    # Cancel running and ready tasks\n    for task in tasks:\n        task_id = task.status()[\"id\"]\n        task_state = task.status()[\"state\"]\n        if task_state == \"RUNNING\" or task_state == \"READY\":\n            task.cancel()\n            print(f\"Task {task_id} cancelled\")\n        else:\n            print(f\"Task {task_id} state is {task_state}\")\n",
  "history_output" : "",
  "history_begin_time" : 1646144600523,
  "history_end_time" : 1646144602106,
  "history_notes" : null,
  "history_process" : "dmf4zo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fyijk20wven",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudFreeKeepThresh,\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(calcCloudStats)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef calcCloudStats(img):\n    imgPoly = ee.Algorithms.GeometryConstructors.Polygon(\n        ee.Geometry(img.get(\"system:footprint\")).coordinates()\n    )\n\n    roi = ee.Geometry(img.get(\"ROI\"))\n\n    intersection = roi.intersection(imgPoly, ee.ErrorMargin(0.5))\n    cloudMask = img.select([\"cloudScore\"]).gt(cloudThresh).clip(roi).rename(\"cloudMask\")\n\n    cloudAreaImg = cloudMask.multiply(ee.Image.pixelArea())\n\n    stats = cloudAreaImg.reduceRegion(\n        **{\"reducer\": ee.Reducer.sum(), \"geometry\": roi, \"scale\": 10, \"maxPixels\": 1e12}\n    )\n\n    cloudPercent = ee.Number(stats.get(\"cloudMask\")).divide(imgPoly.area()).multiply(100)\n    coveragePercent = ee.Number(intersection.area()).divide(roi.area()).multiply(100)\n    cloudPercentROI = ee.Number(stats.get(\"cloudMask\")).divide(roi.area()).multiply(100)\n\n    img = img.set(\"CLOUDY_PERCENTAGE\", cloudPercent)\n    img = img.set(\"ROI_COVERAGE_PERCENT\", coveragePercent)\n    img = img.set(\"CLOUDY_PERCENTAGE_ROI\", cloudPercentROI)\n\n    return img\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n    score = (\n        score.reproject(\"EPSG:4326\", None, 20)\n        .focal_min(**{\"radius\": erodePixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .focal_max(**{\"radius\": dilationPixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .reproject(\"EPSG:4326\", None, 20)\n    )\n\n    return score\n\n\ndef mergeCollection(imgC):\n    # Select the best images, which are below the cloud free threshold, sort them in reverse order\n    # (worst on top) for mosaicing\n    best = imgC.filterMetadata(\"CLOUDY_PERCENTAGE\", \"less_than\", cloudFreeKeepThresh).sort(\n        \"CLOUDY_PERCENTAGE\", False\n    )\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n\n    # Add the quality mosaic to fill in any missing areas of the ROI which aren't covered by good\n    # images\n    newC = ee.ImageCollection.fromImages([filtered, best.mosaic()])\n\n    return ee.Image(newC.mosaic())\n",
  "history_output" : "",
  "history_begin_time" : 1646144594822,
  "history_end_time" : 1646144596589,
  "history_notes" : null,
  "history_process" : "nph7xo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "s8hjorqqthx",
  "history_input" : "# These are algorithm settings for the cloud filtering algorithm\nimage_collection = \"COPERNICUS/S2\"\n\n# Ranges from 0-1.Lower value will mask more pixels out.\n# Generally 0.1-0.3 works well with 0.2 being used most commonly\ncloudThresh = 0.2\n# Height of clouds to use to project cloud shadows\ncloudHeights = [200, 10000, 250]\n# Sum of IR bands to include as shadows within TDOM and the\n# shadow shift method (lower number masks out less)\nirSumThresh = 0.3\nndviThresh = -0.1\n# Pixels to reduce cloud mask and dark shadows by to reduce inclusion\n# of single-pixel comission errors\nerodePixels = 1.5\ndilationPixels = 3\n\n# images with less than this many cloud pixels will be used with normal\n# mosaicing (most recent on top)\ncloudFreeKeepThresh = 3\n\nBANDS = [\n    \"B1\",\n    \"B2\",\n    \"B3\",\n    \"B4\",\n    \"B5\",\n    \"B6\",\n    \"B7\",\n    \"B8\",\n    \"B8A\",\n    \"B9\",\n    \"B10\",\n    \"B11\",\n    \"B12\",\n]\n",
  "history_output" : "",
  "history_begin_time" : 1646144595795,
  "history_end_time" : 1646144595890,
  "history_notes" : null,
  "history_process" : "jsnayl",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nabsi0fz59l",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PIXEL_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5), optimization=\"boxcar\"\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5))\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n\n    def erode(img, distance):\n        d = (\n            img.Not()\n            .unmask(1)\n            .fastDistanceTransform(30)\n            .sqrt()\n            .multiply(ee.Image.pixelArea().sqrt())\n        )\n        return img.updateMask(d.gt(distance))\n\n    def dilate(img, distance):\n        d = img.fastDistanceTransform(30).sqrt().multiply(ee.Image.pixelArea().sqrt())\n        return d.lt(distance)\n\n    score = score.reproject(\"EPSG:4326\", None, 20)\n    score = erode(score, erodePixels)\n    score = dilate(score, dilationPixels)\n\n    return score.reproject(\"EPSG:4326\", None, 20)\n\n\ndef mergeCollection(imgC):\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n    return filtered\n",
  "history_output" : "",
  "history_begin_time" : 1646144595087,
  "history_end_time" : 1646144596640,
  "history_notes" : null,
  "history_process" : "yqt708",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8tazilt6uxf",
  "history_input" : "r\"\"\"\nFunctions shared by both the fast and slow\ncloudfree algorithm\n\"\"\"\nimport ee\nfrom datetime import date\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\n\nfrom typing import Union\n\n\ndef combine_bands(current, previous):\n    # Transforms an Image Collection with 1 band per Image into a single Image with items as bands\n    # Author: Jamie Vleeshouwer\n\n    # Rename the band\n    previous = ee.Image(previous)\n    current = current.select(BANDS)\n    # Append it to the result (Note: only return current item on first element/iteration)\n    return ee.Algorithms.If(\n        ee.Algorithms.IsEqual(previous, None), current, previous.addBands(ee.Image(current)),\n    )\n\n\ndef export(\n    image: ee.Image, region: ee.Geometry, filename: str, drive_folder: str, monitor: bool = False,\n) -> ee.batch.Export:\n\n    task = ee.batch.Export.image(\n        image.clip(region),\n        filename,\n        {\"scale\": 10, \"region\": region, \"maxPixels\": 1e13, \"driveFolder\": drive_folder},\n    )\n\n    try:\n        task.start()\n    except ee.ee_exception.EEException as e:\n        print(f\"Task not started! Got exception {e}\")\n        return task\n\n    if monitor:\n        monitor_task(task)\n\n    return task\n\n\ndef date_to_string(input_date: Union[date, str]) -> str:\n    if isinstance(input_date, str):\n        return input_date\n    else:\n        assert isinstance(input_date, date)\n        return input_date.strftime(\"%Y-%m-%d\")\n\n\ndef monitor_task(task: ee.batch.Export) -> None:\n\n    while task.status()[\"state\"] in [\"READY\", \"RUNNING\"]:\n        print(task.status())\n        # print(f\"Running: {task.status()['state']}\")\n\n\ndef rescale(img, exp, thresholds):\n    return (\n        img.expression(exp, {\"img\": img})\n        .subtract(thresholds[0])\n        .divide(thresholds[1] - thresholds[0])\n    )\n",
  "history_output" : "",
  "history_begin_time" : 1646144595252,
  "history_end_time" : 1646144596662,
  "history_notes" : null,
  "history_process" : "q5a232",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "seitslzfkvz",
  "history_input" : "from pathlib import Path\nfrom src_utils import set_seed\n\n\nclass BaseProcessor:\n    r\"\"\"Base for all processor classes. It creates the appropriate\n    directory in the data dir (``data_dir/processed/{dataset}``).\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n\n    def __init__(self, data_folder: Path) -> None:\n\n        set_seed()\n        self.data_folder = data_folder\n        self.raw_folder = self.data_folder / \"raw\" / self.dataset\n        assert self.raw_folder.exists(), f\"{self.raw_folder} does not exist!\"\n\n        self.output_folder = self.data_folder / \"processed\" / self.dataset\n        self.output_folder.mkdir(exist_ok=True, parents=True)\n",
  "history_output" : "",
  "history_begin_time" : 1646144604336,
  "history_end_time" : 1646144604477,
  "history_notes" : null,
  "history_process" : "6nnond",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5f5r2v5rnn7",
  "history_input" : "import pandas as pd\n\nfrom src_processors_base import BaseProcessor\n\n\nclass GeoWikiProcessor(BaseProcessor):\n\n    dataset = \"geowiki_landcover_2017\"\n\n    def load_raw_data(self, participants: str) -> pd.DataFrame:\n\n        participants_to_file_labels = {\n            \"all\": \"all\",\n            \"students\": \"con\",\n            \"experts\": \"exp\",\n        }\n\n        file_label = participants_to_file_labels.get(participants, participants)\n        assert (\n            file_label in participants_to_file_labels.values()\n        ), f\"Unknown participant {file_label}\"\n\n        return pd.read_csv(\n            self.raw_folder / f\"loc_{file_label}{'_2' if file_label == 'all' else ''}.txt\",\n            sep=\"\\t\",\n        )\n\n    def process(self, participants: str = \"all\") -> None:\n\n        location_data = self.load_raw_data(participants)\n\n        # first, we find the mean sumcrop calculated per location\n        mean_per_location = (\n            location_data[[\"location_id\", \"sumcrop\", \"loc_cent_X\", \"loc_cent_Y\"]]\n            .groupby(\"location_id\")\n            .mean()\n        )\n\n        # then, we rename the columns\n        mean_per_location = mean_per_location.rename(\n            {\"loc_cent_X\": \"lon\", \"loc_cent_Y\": \"lat\", \"sumcrop\": \"mean_sumcrop\"},\n            axis=\"columns\",\n            errors=\"raise\",\n        )\n        # then, we turn it into an xarray with x and y as indices\n        output_xr = (\n            mean_per_location.reset_index().set_index([\"lon\", \"lat\"])[\"mean_sumcrop\"].to_xarray()\n        )\n\n        # and save\n        output_xr.to_netcdf(self.output_folder / \"data.nc\")\n",
  "history_output" : "",
  "history_begin_time" : 1646144602169,
  "history_end_time" : 1646144603079,
  "history_notes" : null,
  "history_process" : "m6v1cg",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "odm22nm4sqj",
  "history_input" : "# The probability threshold to use to label GeoWiki\n# instances as crop / not_crop (since the GeoWiki labels are a mean crop probability, as\n# assigned by several labellers). In addition, this is the threshold used when calculating\n# metrics which require binary predictions, such as accuracy score\nPROBABILITY_THRESHOLD = 0.5\n",
  "history_output" : "",
  "history_begin_time" : 1646144597899,
  "history_end_time" : 1646144598026,
  "history_notes" : null,
  "history_process" : "nt17bz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ldq0sop7dmz",
  "history_input" : "import torch\nimport numpy as np\nimport random\n\nfrom dataclasses import dataclass\n\n\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n\n@dataclass\nclass BoundingBox:\n\n    min_lon: float\n    max_lon: float\n    min_lat: float\n    max_lat: float\n\n\nSTR2BB = {\n    \"Kenya\": BoundingBox(min_lon=33.501, max_lon=42.283, min_lat=-5.202, max_lat=6.002),\n    \"Busia\": BoundingBox(\n        min_lon=33.88389587402344,\n        min_lat=-0.04119872691853491,\n        max_lon=34.44007873535156,\n        max_lat=0.7779454563313616,\n    ),\n}\n",
  "history_output" : "",
  "history_begin_time" : 1646144598599,
  "history_end_time" : 1646144599350,
  "history_notes" : null,
  "history_process" : "o5t3jb",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fb0lfwuj8n2",
  "history_input" : "import pandas as pd\nimport xarray as xr\nfrom datetime import date\nfrom tqdm import tqdm\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass GeoWikiSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_geowiki\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        geowiki = self.data_folder / \"processed\" / GeoWikiExporter.dataset / \"data.nc\"\n        assert geowiki.exists(), \"GeoWiki processor must be run to load labels\"\n        return xr.open_dataset(geowiki).to_dataframe().dropna().reset_index()\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        start_date: date = date(2017, 3, 28),\n        end_date: date = date(2018, 3, 28),\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        Run the GeoWiki exporter. For each label, the exporter will export\n        int( (end_date - start_date).days / days_per_timestep) timesteps of data,\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param start_date: The start data of the data export\n        :param end_date: The end date of the data export\n        :param num_labelled_points: (Optional) The number of labelled points to export.\n        :param surrounding_metres: The number of metres surrounding each labelled point to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        assert start_date >= self.min_date, f\"Sentinel data does not exist before {self.min_date}\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        for idx, bounding_box in enumerate(bounding_boxes_to_download):\n            self._export_for_polygon(\n                polygon=bounding_box.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1646144601359,
  "history_end_time" : 1646144604460,
  "history_notes" : null,
  "history_process" : "mw544v",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "jnh45gu1jao",
  "history_input" : "from abc import ABC, abstractmethod\nfrom datetime import date, timedelta\nfrom pathlib import Path\nimport pandas as pd\nimport ee\n\nfrom src_exporters_sentinel_cloudfree_cloudfree import *\nfrom src_exporters_sentinel_cloudfree_fast import get_single_image as get_single_image_fast\nfrom src_exporters_base import BaseExporter\nfrom src_exporters_sentinel_cloudfree_utils import *\n\n\nfrom typing import List, Union\n\n\nclass BaseSentinelExporter(BaseExporter, ABC):\n\n    r\"\"\"\n    Download cloud free sentinel data for countries,\n    where countries are defined by the simplified large scale\n    international boundaries.\n    \"\"\"\n\n    dataset: str\n    min_date = date(2017, 3, 28)\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n        super().__init__(data_folder)\n        try:\n            ee.Initialize()\n        except Exception:\n            print(\"This code doesn't work unless you have authenticated your earthengine account\")\n\n        self.labels = self.load_labels()\n\n    @abstractmethod\n    def load_labels(self) -> pd.DataFrame:\n        raise NotImplementedError\n\n    def _export_for_polygon(\n        self,\n        polygon: ee.Geometry.Polygon,\n        polygon_identifier: Union[int, str],\n        start_date: date,\n        end_date: date,\n        days_per_timestep: int,\n        checkpoint: bool,\n        monitor: bool,\n        fast: bool,\n    ) -> None:\n\n        if fast:\n            export_func = get_single_image_fast\n        else:\n            export_func = get_single_image\n\n        cur_date = start_date\n        cur_end_date = cur_date + timedelta(days=days_per_timestep)\n\n        image_collection_list: List[ee.Image] = []\n\n        print(\n            f\"Exporting image for polygon {polygon_identifier} from \"\n            f\"aggregated images between {str(cur_date)} and {str(end_date)}\"\n        )\n        filename = f\"{polygon_identifier}_{str(cur_date)}_{str(end_date)}\"\n\n        if checkpoint and (self.output_folder / f\"{filename}.tif\").exists():\n            print(\"File already exists! Skipping\")\n            return None\n\n        while cur_end_date <= end_date:\n\n            image_collection_list.append(\n                export_func(region=polygon, start_date=cur_date, end_date=cur_end_date)\n            )\n            cur_date += timedelta(days=days_per_timestep)\n            cur_end_date += timedelta(days=days_per_timestep)\n\n        # now, we want to take our image collection and append the bands into a single image\n        imcoll = ee.ImageCollection(image_collection_list)\n        img = ee.Image(imcoll.iterate(combine_bands))\n\n        # and finally, export the image\n        export(\n            image=img,\n            region=polygon,\n            filename=filename,\n            drive_folder=self.dataset,\n            monitor=monitor,\n        )\n",
  "history_output" : "",
  "history_begin_time" : 1646144598651,
  "history_end_time" : 1646144599357,
  "history_notes" : null,
  "history_process" : "vxuj3q",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o3li010zosh",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nfrom datetime import timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_kenya_non_crop import KenyaNonCropProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass KenyaNonCropSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_kenya_non_crop\"\n\n    # data collection date\n    data_date = date(2020, 4, 16)\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        non_crop = self.data_folder / \"processed\" / KenyaNonCropProcessor.dataset / \"data.geojson\"\n        assert non_crop.exists(), \"Kenya non crop processor must be run to load labels\"\n        return geopandas.read_file(non_crop)[[\"lat\", \"lon\"]]\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                ),\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        start_date = self.data_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            self._export_for_polygon(\n                polygon=bounding_info.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=self.data_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1646144603121,
  "history_end_time" : 1646144603328,
  "history_notes" : null,
  "history_process" : "nlb6f5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8vkjeaexu0p",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nimport numpy as np\nfrom datetime import datetime, timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_pv_kenya import KenyaPVProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre, date_overlap\n\nfrom typing import Dict, Optional, List, Tuple\n\n\nclass KenyaPVSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_plant_village_kenya\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        plantvillage = self.data_folder / \"processed\" / KenyaPVProcessor.dataset / \"data.geojson\"\n        assert plantvillage.exists(), \"Plant Village processor must be run to load labels\"\n        return geopandas.read_file(plantvillage)[\n            [\"lat\", \"lon\", \"index\", \"planting_date\", \"harvest_date\"]\n        ]\n\n    @staticmethod\n    def overlapping_year(\n        end_month: int, num_days: int, harvest_date: date, planting_date: date\n    ) -> Tuple[Optional[int], Optional[int]]:\n        r\"\"\"\n        Return the end_year of the most overlapping years\n        \"\"\"\n        harvest_year = harvest_date.year\n\n        overlap_dict: Dict[int, int] = {}\n\n        for diff in range(-1, 2):\n            end_date = date(harvest_year + diff, end_month, 1)\n\n            if end_date > datetime.now().date():\n                continue\n            else:\n                overlap_dict[harvest_year + diff] = date_overlap(\n                    planting_date, harvest_date, end_date - timedelta(days=num_days), end_date,\n                )\n        if len(overlap_dict) > 0:\n            return max(overlap_dict.items(), key=lambda x: x[1])\n        else:\n            # sometimes the harvest date is in the future? in which case\n            # we will just skip the datapoint for now\n            return None, None\n\n    def labels_to_bounding_boxes(\n        self,\n        num_labelled_points: Optional[int],\n        surrounding_metres: int,\n        end_month_day: Optional[Tuple[int, int]],\n        num_days: int,\n    ) -> List[Tuple[int, EEBoundingBox, date, Optional[int]]]:\n\n        output: List[Tuple[int, EEBoundingBox, date, Optional[int]]] = []\n\n        if end_month_day is not None:\n            end_month: Optional[int]\n            end_day: Optional[int]\n            end_month, end_day = end_month_day\n        else:\n            end_month = end_day = None\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            try:\n                harvest_date = datetime.strptime(row[\"harvest_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n            except ValueError:\n                continue\n\n            # this is only used if end_month is not None\n            overlapping_days: Optional[int] = 0\n            if end_month is not None:\n                planting_date = datetime.strptime(row[\"planting_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n\n                end_year, overlapping_days = self.overlapping_year(\n                    end_month, num_days, harvest_date, planting_date\n                )\n\n                if end_year is None:\n                    continue\n\n                if end_day is None:\n                    # if no end_day is passed, we will take the first month\n                    end_day = 1\n                harvest_date = date(end_year, end_month, end_day)\n\n            output.append(\n                (\n                    row[\"index\"],\n                    bounding_box_from_centre(\n                        mid_lat=row[\"lat\"],\n                        mid_lon=row[\"lon\"],\n                        surrounding_metres=surrounding_metres,\n                    ),\n                    harvest_date,\n                    overlapping_days,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def get_start_and_end_dates(\n        self, harvest_date: date, days_per_timestep: int, num_timesteps: int\n    ) -> Optional[Tuple[date, date]]:\n\n        if harvest_date < self.min_date:\n            print(\"Harvest date < min date - skipping\")\n            return None\n        else:\n            start_date = max(\n                harvest_date - timedelta(days_per_timestep * num_timesteps), self.min_date,\n            )\n            end_date = start_date + timedelta(days_per_timestep * num_timesteps)\n\n            return start_date, end_date\n\n    def export_for_labels(\n        self,\n        end_month_day: Optional[Tuple[int, int]] = (4, 16),\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param end_month_day: The final month-day to use. If None is passed, the harvest date\n            will be used. Default = (4, 16)\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points,\n            surrounding_metres=surrounding_metres,\n            end_month_day=end_month_day,\n            num_days=days_per_timestep * num_timesteps,\n        )\n\n        if end_month_day is not None:\n            print(\n                f\"Average overlapping days between planting to harvest and \"\n                f\"export dates: {np.mean([x[3] for x in bounding_boxes_to_download])}\"\n            )\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            harvest_date = bounding_info[-2]\n\n            dates = self.get_start_and_end_dates(harvest_date, days_per_timestep, num_timesteps)\n\n            if dates is not None:\n\n                self._export_for_polygon(\n                    polygon=bounding_info[1].to_ee_polygon(),\n                    polygon_identifier=bounding_info[0],\n                    start_date=dates[0],\n                    end_date=dates[1],\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n",
  "history_output" : "",
  "history_begin_time" : 1646144606470,
  "history_end_time" : 1646144607399,
  "history_notes" : null,
  "history_process" : "i4s7l1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "p97qrddcpj1",
  "history_input" : "from datetime import date, timedelta\nimport pandas as pd\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_sentinel_utils import bounding_box_to_earth_engine_bounding_box\nfrom src_utils import STR2BB\n\nfrom typing import Optional\n\n\nclass RegionalExporter(BaseSentinelExporter):\n    r\"\"\"\n    This is useful if you are trying to export\n    full regions for predictions\n    \"\"\"\n\n    dataset = \"earth_engine_region_busia_partial_slow_cloudfree\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # We don't need any labels for this exporter,\n        # so we can return an empty dataframe\n        return pd.DataFrame()\n\n    def export_for_region(\n        self,\n        region_name: str,\n        end_date: date,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        checkpoint: bool = True,\n        monitor: bool = True,\n        metres_per_polygon: Optional[int] = 10000,\n        fast: bool = True,\n    ):\n        r\"\"\"\n        Run the regional exporter. For each label, the exporter will export\n        data from (end_date - timedelta(days=days_per_timestep * num_timesteps)) to end_date\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param region_name: The name of the region to export. This must be defined in\n            src.utils.STR2BB\n        :param end_date: The end date of the data export\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param num_timesteps: The number of timesteps to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param metres_per_polygon: Whether to split the export of a large region into smaller\n            boxes of (max) area metres_per_polygon * metres_per_polygon. It is better to instead\n            split the area once it has been exported\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        start_date = end_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        region = bounding_box_to_earth_engine_bounding_box(STR2BB[region_name])\n\n        if metres_per_polygon is not None:\n\n            regions = region.to_polygons(metres_per_patch=metres_per_polygon)\n\n            for idx, region in enumerate(regions):\n                self._export_for_polygon(\n                    polygon=region,\n                    polygon_identifier=f\"{idx}-{region_name}\",\n                    start_date=start_date,\n                    end_date=end_date,\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n        else:\n            self._export_for_polygon(\n                polygon=region.to_ee_polygon(),\n                polygon_identifier=region_name,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "",
  "history_begin_time" : 1646144605484,
  "history_end_time" : 1646144606245,
  "history_notes" : null,
  "history_process" : "9c0ch9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zjt6vdak1mk",
  "history_input" : "import geopandas\nfrom pathlib import Path\nimport pandas as pd\nfrom pyproj import Transformer\n\nfrom src_processors_base import BaseProcessor\n\nfrom typing import List\n\n\nclass KenyaNonCropProcessor(BaseProcessor):\n\n    dataset = \"kenya_non_crop\"\n\n    @staticmethod\n    def process_set(filepath: Path, latlon: bool, reversed: bool) -> geopandas.GeoDataFrame:\n        df = geopandas.read_file(filepath)\n\n        x, y = df.geometry.centroid.x.values, df.geometry.centroid.y.values\n\n        if reversed:\n            x, y = y, x\n\n        if not latlon:\n\n            transformer = Transformer.from_crs(crs_from=32636, crs_to=4326)\n\n            lat, lon = transformer.transform(xx=x, yy=y)\n            df[\"lat\"] = lat\n            df[\"lon\"] = lon\n        else:\n            df[\"lat\"] = x\n            df[\"lon\"] = y\n\n        df[\"index\"] = df.index\n\n        return df\n\n    def process(self) -> None:\n\n        filepaths = [\n            (self.raw_folder / \"noncrop_labels_v2\", False, False),\n            (self.raw_folder / \"noncrop_labels_set2\", False, False),\n            (self.raw_folder / \"2019_gepro_noncrop\", True, True),\n            (self.raw_folder / \"noncrop_water_kenya_gt\", True, True),\n            (self.raw_folder / \"noncrop_kenya_gt\", True, True),\n        ]\n\n        dfs: List[geopandas.GeoDataFrame] = []\n        for filepath, is_latlon, is_reversed in filepaths:\n            dfs.append(self.process_set(filepath, is_latlon, is_reversed))\n\n        df = pd.concat(dfs)\n        df.to_file(self.output_folder / \"data.geojson\", driver=\"GeoJSON\")\n",
  "history_output" : "",
  "history_begin_time" : 1646144605086,
  "history_end_time" : 1646144606237,
  "history_notes" : null,
  "history_process" : "9x9elz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "eiiqlpr665a",
  "history_input" : "import geopandas\nimport pandas as pd\nimport numpy as np\n\nfrom src_processors_base import BaseProcessor\n\nfrom typing import List\n\n\nclass KenyaPVProcessor(BaseProcessor):\n    dataset = \"plant_village_kenya\"\n\n    def process(self) -> None:\n\n        subfolders = [f\"ref_african_crops_kenya_01_labels_0{i}\" for i in [0, 1, 2]]\n\n        dfs: List[geopandas.GeoDataFrame] = []\n        for subfolder in subfolders:\n            df = geopandas.read_file(\n                self.raw_folder / \"ref_african_crops_kenya_01_labels\" / subfolder / \"labels.geojson\"\n            )\n            df = df.rename(\n                columns={\n                    \"Latitude\": \"lat\",\n                    \"Longitude\": \"lon\",\n                    \"Planting Date\": \"planting_date\",\n                    \"Estimated Harvest Date\": \"harvest_date\",\n                    \"Crop1\": \"label\",\n                    \"Survey Date\": \"collection_date\",\n                }\n            )\n            df[\"planting_date\"] = pd.to_datetime(df[\"planting_date\"]).dt.to_pydatetime()\n            df[\"harvest_date\"] = pd.to_datetime(df[\"harvest_date\"]).dt.to_pydatetime()\n            df[\"collection_date\"] = pd.to_datetime(df[\"collection_date\"]).dt.to_pydatetime()\n            df[\"is_crop\"] = np.where((df[\"label\"] == \"Fallowland\"), 0, 1)\n            df = df.to_crs(\"EPSG:4326\")\n            dfs.append(df)\n\n        df = pd.concat(dfs)\n        df = df.reset_index(drop=True)\n        df[\"index\"] = df.index\n        df.to_file(self.output_folder / \"data.geojson\", driver=\"GeoJSON\")\n",
  "history_output" : "",
  "history_begin_time" : 1646144601773,
  "history_end_time" : 1646144602114,
  "history_notes" : null,
  "history_process" : "m9myzm",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "46f4b0yoesc",
  "history_input" : "from pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cartopy.crs as ccrs\nfrom datetime import datetime\nimport xarray as xr\n\n\nfrom src_engineer_base import BaseEngineer\n\n\ndef sentinel_as_tci(sentinel_ds: xr.DataArray, scale: bool = True) -> xr.DataArray:\n    r\"\"\"\n    Get a True Colour Image from Sentinel data exported from Earth Engine\n    :param sentinel_ds: The sentinel data, exported from Earth Engine\n    :param scale: Whether or not to add the factor 10,000 scale\n    :return: A dataframe with true colour bands\n    \"\"\"\n\n    band2idx = {band: idx for idx, band in enumerate(sentinel_ds.attrs[\"band_descriptions\"])}\n\n    tci_bands = [\"B4\", \"B3\", \"B2\"]\n    tci_indices = [band2idx[band] for band in tci_bands]\n    if scale:\n        return sentinel_ds.isel(band=tci_indices) / 10000 * 2.5\n    else:\n        return sentinel_ds.isel(band=tci_indices) * 2.5\n\n\ndef plot_results(model_preds: xr.Dataset, tci_path: Path, savepath: Path, prefix: str = \"\") -> None:\n\n    multi_output = len(model_preds.data_vars) > 1\n\n    tci = sentinel_as_tci(\n        BaseEngineer.load_tif(tci_path, start_date=datetime(2020, 1, 1), days_per_timestep=30),\n        scale=False,\n    ).isel(time=-1)\n\n    tci = tci.sortby(\"x\").sortby(\"y\")\n    model_preds = model_preds.sortby(\"lat\").sortby(\"lon\")\n\n    plt.clf()\n    fig, ax = plt.subplots(1, 3, figsize=(20, 7.5), subplot_kw={\"projection\": ccrs.PlateCarree()})\n\n    fig.suptitle(\n        f\"Model results for tile with bottom left corner:\"\n        f\"\\nat latitude {float(model_preds.lat.min())}\"\n        f\"\\n and longitude {float(model_preds.lon.min())}\",\n        fontsize=15,\n    )\n    # ax 1 - original\n    img_extent_1 = (tci.x.min(), tci.x.max(), tci.y.min(), tci.y.max())\n    img = np.clip(np.moveaxis(tci.values, 0, -1), 0, 1)\n\n    ax[0].set_title(\"True colour image\")\n    ax[0].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict = {\n        \"origin\": \"upper\",\n        \"extent\": img_extent_1,\n        \"transform\": ccrs.PlateCarree(),\n    }\n\n    if multi_output:\n        mask = np.argmax(model_preds.to_array().values, axis=0)\n\n        # currently, we have 10 classes (at most). It seems unlikely we will go\n        # above 20\n        args_dict[\"cmap\"] = plt.cm.get_cmap(\"tab20\", len(model_preds.data_vars))\n    else:\n        mask = model_preds.prediction_0\n        args_dict.update({\"vmin\": 0, \"vmax\": 1})\n\n    # ax 2 - mask\n    ax[1].set_title(\"Mask\")\n    im = ax[1].imshow(mask, **args_dict)\n\n    # finally, all together\n    ax[2].set_title(\"Mask on top of the true colour image\")\n    ax[2].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict[\"alpha\"] = 0.3\n    if not multi_output:\n        mask = mask > 0.5\n    ax[2].imshow(mask, **args_dict)\n\n    colorbar_args = {\n        \"ax\": ax.ravel().tolist(),\n    }\n\n    if multi_output:\n        # This function formatter will replace integers with target names\n        formatter = plt.FuncFormatter(lambda val, loc: list(model_preds.data_vars)[val])\n        colorbar_args.update({\"ticks\": range(len(model_preds.data_vars)), \"format\": formatter})\n\n    # We must be sure to specify the ticks matching our target names\n    fig.colorbar(im, **colorbar_args)\n\n    plt.savefig(savepath / f\"results_{prefix}{tci_path.name}.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n",
  "history_output" : "",
  "history_begin_time" : 1646144605338,
  "history_end_time" : 1646144607389,
  "history_notes" : null,
  "history_process" : "qdzo28",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "aamfkzm822e",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_processors_geowiki import *\nfrom src_processors_kenya_non_crop import *\nfrom src_processors_pv_kenya import *\n\ndef process_geowiki():\n    processor = GeoWikiProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_plantvillage():\n    processor = KenyaPVProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_kenya_noncrop():\n    processor = KenyaNonCropProcessor(Path(\"../data\"))\n    processor.process()\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...process.py\")\n    process_geowiki()\n    process_plantvillage()\n    #process_kenya_noncrop()\n",
  "history_output" : "Starting...process.py\n",
  "history_begin_time" : 1646144600365,
  "history_end_time" : 1646144603325,
  "history_notes" : null,
  "history_process" : "iticjd",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ic0bu2z08n3",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_engineer_geowiki import GeoWikiEngineer\nfrom src_engineer_pv_kenya import PVKenyaEngineer\nfrom src_engineer_kenya_non_crop import KenyaNonCropEngineer\n\n\ndef engineer_geowiki():\n    engineer = GeoWikiEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.2)\n\n\ndef engineer_kenya():\n    engineer = PVKenyaEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.1, test_set_size=0.1)\n\n\ndef engineer_kenya_noncrop():\n    engineer = KenyaNonCropEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.1, test_set_size=0.1)\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...engineer.py\")  \n    engineer_geowiki()\n    engineer_kenya()\n    #engineer_kenya_noncrop()",
  "history_output" : "  0%|          | 0/4 [00:00<?, ?it/s]\n100%|██████████| 4/4 [00:00<00:00, 1371.36it/s]\n  0%|          | 0/10 [00:00<?, ?it/s]\n100%|██████████| 10/10 [00:00<00:00, 9880.57it/s]\nStarting...engineer.py\nNo normalizing dict calculated! Make sure to call update_normalizing_values\nNo normalizing dict calculated!\nNo normalizing dict calculated! Make sure to call update_normalizing_values\nNo normalizing dict calculated!\n",
  "history_begin_time" : 1646144604120,
  "history_end_time" : 1646144607748,
  "history_notes" : null,
  "history_process" : "3cars9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "xlyq5y130g8",
  "history_input" : "import sys\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_models_model import Model\nfrom src_models_train_funcs import train_model\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...model.py\")\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--max_epochs\", type=int, default=1000)\n    parser.add_argument(\"--patience\", type=int, default=10)\n\n    model_args = Model.add_model_specific_args(parser).parse_args()\n    model = Model(model_args)\n\n    train_model(model, model_args)\n",
  "history_output" : "  0%|          | 0/1 [00:00<?, ?it/s]\n100%|██████████| 1/1 [00:00<00:00, 1608.25it/s]\nValidation sanity check:   0%|          | 0/5 [00:00<?, ?it/s]/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n                                                              Starting...model.py\nPredicting 7 timesteps in the forecaster\nUsing 1 layers for the global classifier\nUsing 2 layers for the local classifier\nLoading data into memory\n0it [00:00, ?it/s]\n  0%|          | 0/11 [00:00<?, ?it/s]\u001B[A\n100%|██████████| 11/11 [00:00<00:00, 6091.54it/s]\n  0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n100%|██████████| 1/1 [00:00<00:00, 3175.10it/s]Loading data into memory\nLoading data into memory\n  0%|          | 0/2 [00:00<?, ?it/s]\nEpoch 1:   0%|          | 0/2 [00:00<?, ?it/s]\nEpoch 1: : 50it [00:00, 904.21it/s, loss=0.765, v_num=9]\nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 1: : 100it [00:00, 1396.08it/s, loss=0.765, v_num=9]\n                                                 \u001B[A\nEpoch 1:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.765, v_num=9]\nEpoch 2:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.765, v_num=9]\nEpoch 2: : 50it [00:00, 1256.76it/s, loss=0.781, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 2: : 100it [00:00, 1776.13it/s, loss=0.781, v_num=9]\n                                                 \u001B[A\nEpoch 2:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.781, v_num=9]\nEpoch 3:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.781, v_num=9]\nEpoch 3: : 50it [00:00, 1302.93it/s, loss=0.772, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 3: : 100it [00:00, 1836.45it/s, loss=0.772, v_num=9]\n                                                 \u001B[A\nEpoch 3:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.772, v_num=9]\nEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.772, v_num=9]\nEpoch 4: : 50it [00:00, 1359.39it/s, loss=0.765, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 4: : 100it [00:00, 1852.64it/s, loss=0.765, v_num=9]\n                                                 \u001B[A\nEpoch 4:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.765, v_num=9]\nEpoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.765, v_num=9]\nEpoch 5: : 50it [00:00, 1268.20it/s, loss=0.746, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 5: : 100it [00:00, 1789.81it/s, loss=0.746, v_num=9]\n                                                 \u001B[A\nEpoch 5:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.746, v_num=9]\nEpoch 6:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.746, v_num=9]\nEpoch 6: : 50it [00:00, 1336.51it/s, loss=0.734, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 6: : 100it [00:00, 1860.13it/s, loss=0.734, v_num=9]\n                                                 \u001B[A\nEpoch 6:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.734, v_num=9]\nEpoch 7:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.734, v_num=9]\nEpoch 7: : 50it [00:00, 1331.38it/s, loss=0.737, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 7: : 100it [00:00, 1902.33it/s, loss=0.737, v_num=9]\n                                                 \u001B[A\nEpoch 7:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.737, v_num=9]\nEpoch 8:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.737, v_num=9]\nEpoch 8: : 50it [00:00, 1352.67it/s, loss=0.723, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 8: : 100it [00:00, 1787.56it/s, loss=0.723, v_num=9]\n                                                 \u001B[A\nEpoch 8:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.723, v_num=9]\nEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.723, v_num=9]\nEpoch 9: : 50it [00:00, 1057.24it/s, loss=0.719, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 9: : 100it [00:00, 1548.08it/s, loss=0.719, v_num=9]\n                                                 \u001B[A\nEpoch 9:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.719, v_num=9]\nEpoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.719, v_num=9]\nEpoch 10: : 50it [00:00, 1219.19it/s, loss=0.708, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 10: : 100it [00:00, 1653.61it/s, loss=0.708, v_num=9]\n                                                 \u001B[A\nEpoch 10:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.708, v_num=9]\nEpoch 11:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.708, v_num=9]\nEpoch 11: : 50it [00:00, 764.39it/s, loss=0.697, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 11: : 100it [00:00, 1222.12it/s, loss=0.697, v_num=9]\n                                                 \u001B[A\nEpoch 11:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.697, v_num=9]\nEpoch 12:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.697, v_num=9]\nEpoch 12: : 50it [00:00, 1165.50it/s, loss=0.687, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 12: : 100it [00:00, 1636.09it/s, loss=0.687, v_num=9]\n                                                 \u001B[A\nEpoch 12:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.687, v_num=9]\nEpoch 13:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.687, v_num=9]\nEpoch 13: : 50it [00:00, 1128.50it/s, loss=0.681, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 13: : 100it [00:00, 1668.65it/s, loss=0.681, v_num=9]\n                                                 \u001B[A\nEpoch 13:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.681, v_num=9]\nEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.681, v_num=9]\nEpoch 14: : 50it [00:00, 1057.79it/s, loss=0.674, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 14: : 100it [00:00, 1474.84it/s, loss=0.674, v_num=9]\n                                                 \u001B[A\nEpoch 14:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.674, v_num=9]\nEpoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.674, v_num=9]\nEpoch 15: : 50it [00:00, 1087.20it/s, loss=0.670, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 15: : 100it [00:00, 1545.26it/s, loss=0.670, v_num=9]\n                                                 \u001B[A\nEpoch 15:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.670, v_num=9]\nEpoch 16:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.670, v_num=9]\nEpoch 16: : 50it [00:00, 1242.54it/s, loss=0.672, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 16: : 100it [00:00, 1743.33it/s, loss=0.672, v_num=9]\n                                                 \u001B[A\nEpoch 16:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.672, v_num=9]\nEpoch 17:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.672, v_num=9]\nEpoch 17: : 50it [00:00, 1229.65it/s, loss=0.667, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 17: : 100it [00:00, 1751.56it/s, loss=0.667, v_num=9]\n                                                 \u001B[A\nEpoch 17:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.667, v_num=9]\nEpoch 18:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.667, v_num=9]\nEpoch 18: : 50it [00:00, 1144.85it/s, loss=0.663, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 18: : 100it [00:00, 1682.23it/s, loss=0.663, v_num=9]\n                                                 \u001B[A\nEpoch 18:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.663, v_num=9]\nEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.663, v_num=9]\nEpoch 19: : 50it [00:00, 1277.93it/s, loss=0.654, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 19: : 100it [00:00, 1749.45it/s, loss=0.654, v_num=9]\n                                                 \u001B[A\nEpoch 19:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.654, v_num=9]\nEpoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.654, v_num=9]\nEpoch 20: : 50it [00:00, 1200.36it/s, loss=0.648, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 20: : 100it [00:00, 1684.87it/s, loss=0.648, v_num=9]\n                                                 \u001B[A\nEpoch 20:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.648, v_num=9]\nEpoch 21:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.648, v_num=9]\nEpoch 21: : 50it [00:00, 1153.75it/s, loss=0.643, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 21: : 100it [00:00, 1641.46it/s, loss=0.643, v_num=9]\n                                                 \u001B[A\nEpoch 21:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.643, v_num=9]\nEpoch 22:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.643, v_num=9]\nEpoch 22: : 50it [00:00, 1282.65it/s, loss=0.634, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 22: : 100it [00:00, 1794.58it/s, loss=0.634, v_num=9]\n                                                 \u001B[A\nEpoch 22:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.634, v_num=9]\nEpoch 23:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.634, v_num=9]\nEpoch 23: : 50it [00:00, 1227.09it/s, loss=0.631, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 23: : 100it [00:00, 1703.21it/s, loss=0.631, v_num=9]\n                                                 \u001B[A\nEpoch 23:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.631, v_num=9]\nEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.631, v_num=9]\nEpoch 24: : 50it [00:00, 1299.04it/s, loss=0.622, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 24: : 100it [00:00, 1781.90it/s, loss=0.622, v_num=9]\n                                                 \u001B[A\nEpoch 24:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.622, v_num=9]\nEpoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.622, v_num=9]\nEpoch 25: : 50it [00:00, 816.61it/s, loss=0.615, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 25: : 100it [00:00, 1318.76it/s, loss=0.615, v_num=9]\n                                                 \u001B[A\nEpoch 25:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.615, v_num=9]\nEpoch 26:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.615, v_num=9]\nEpoch 26: : 50it [00:00, 1213.09it/s, loss=0.607, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 26: : 100it [00:00, 1718.69it/s, loss=0.607, v_num=9]\n                                                 \u001B[A\nEpoch 26:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.607, v_num=9]\nEpoch 27:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.607, v_num=9]\nEpoch 27: : 50it [00:00, 1273.85it/s, loss=0.602, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 27: : 100it [00:00, 1817.88it/s, loss=0.602, v_num=9]\n                                                 \u001B[A\nEpoch 27:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.602, v_num=9]\nEpoch 28:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.602, v_num=9]\nEpoch 28: : 50it [00:00, 1251.43it/s, loss=0.597, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 28: : 100it [00:00, 1759.23it/s, loss=0.597, v_num=9]\n                                                 \u001B[A\nEpoch 28:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.597, v_num=9]\nEpoch 29:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.597, v_num=9]\nEpoch 29: : 50it [00:00, 1222.49it/s, loss=0.590, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 29: : 100it [00:00, 1701.29it/s, loss=0.590, v_num=9]\n                                                 \u001B[A\nEpoch 29:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.590, v_num=9]\nEpoch 30:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.590, v_num=9]\nEpoch 30: : 50it [00:00, 1269.20it/s, loss=0.585, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 30: : 100it [00:00, 1709.11it/s, loss=0.585, v_num=9]\n                                                 \u001B[A\nEpoch 30:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.585, v_num=9]\nEpoch 31:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.585, v_num=9]\nEpoch 31: : 50it [00:00, 1257.10it/s, loss=0.576, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 31: : 100it [00:00, 1744.81it/s, loss=0.576, v_num=9]\n                                                 \u001B[A\nEpoch 31:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.576, v_num=9]\nEpoch 32:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.576, v_num=9]\nEpoch 32: : 50it [00:00, 1258.55it/s, loss=0.570, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 32: : 100it [00:00, 1767.88it/s, loss=0.570, v_num=9]\n                                                 \u001B[A\nEpoch 32:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.570, v_num=9]\nEpoch 33:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.570, v_num=9]\nEpoch 33: : 50it [00:00, 1257.29it/s, loss=0.565, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 33: : 100it [00:00, 1710.60it/s, loss=0.565, v_num=9]\n                                                 \u001B[A\nEpoch 33:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.565, v_num=9]\nEpoch 34:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.565, v_num=9]\nEpoch 34: : 50it [00:00, 1226.45it/s, loss=0.561, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 34: : 100it [00:00, 1732.56it/s, loss=0.561, v_num=9]\n                                                 \u001B[A\nEpoch 34:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.561, v_num=9]\nEpoch 35:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.561, v_num=9]\nEpoch 35: : 50it [00:00, 1256.79it/s, loss=0.554, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 35: : 100it [00:00, 1752.95it/s, loss=0.554, v_num=9]\n                                                 \u001B[A\nEpoch 35:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.554, v_num=9]\nEpoch 36:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.554, v_num=9]\nEpoch 36: : 50it [00:00, 1214.95it/s, loss=0.550, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 36: : 100it [00:00, 1661.32it/s, loss=0.550, v_num=9]\n                                                 \u001B[A\nEpoch 36:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.550, v_num=9]\nEpoch 37:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.550, v_num=9]\nEpoch 37: : 50it [00:00, 1276.19it/s, loss=0.545, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 37: : 100it [00:00, 1774.31it/s, loss=0.545, v_num=9]\n                                                 \u001B[A\nEpoch 37:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.545, v_num=9]\nEpoch 38:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.545, v_num=9]\nEpoch 38: : 50it [00:00, 1144.43it/s, loss=0.539, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 38: : 100it [00:00, 1600.72it/s, loss=0.539, v_num=9]\n                                                 \u001B[A\nEpoch 38:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.539, v_num=9]\nEpoch 39:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.539, v_num=9]\nEpoch 39: : 50it [00:00, 1023.48it/s, loss=0.536, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 39: : 100it [00:00, 1439.12it/s, loss=0.536, v_num=9]\n                                                 \u001B[A\nEpoch 39:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.536, v_num=9]\nEpoch 40:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.536, v_num=9]\nEpoch 40: : 50it [00:00, 1127.70it/s, loss=0.534, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 40: : 100it [00:00, 1593.24it/s, loss=0.534, v_num=9]\n                                                 \u001B[A\nEpoch 40:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.534, v_num=9]\nEpoch 41:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.534, v_num=9]\nEpoch 41: : 50it [00:00, 1136.08it/s, loss=0.529, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 41: : 100it [00:00, 1584.74it/s, loss=0.529, v_num=9]\n                                                 \u001B[A\nEpoch 41:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.529, v_num=9]\nEpoch 42:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.529, v_num=9]\nEpoch 42: : 50it [00:00, 1180.97it/s, loss=0.524, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 42: : 100it [00:00, 1628.13it/s, loss=0.524, v_num=9]\n                                                 \u001B[A\nEpoch 42:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.524, v_num=9]\nEpoch 43:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.524, v_num=9]\nEpoch 43: : 50it [00:00, 1173.04it/s, loss=0.518, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 43: : 100it [00:00, 1635.77it/s, loss=0.518, v_num=9]\n                                                 \u001B[A\nEpoch 43:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.518, v_num=9]\nEpoch 44:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.518, v_num=9]\nEpoch 44: : 50it [00:00, 1059.01it/s, loss=0.516, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 44: : 100it [00:00, 1503.64it/s, loss=0.516, v_num=9]\n                                                 \u001B[A\nEpoch 44:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.516, v_num=9]\nEpoch 45:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.516, v_num=9]\nEpoch 45: : 50it [00:00, 1126.67it/s, loss=0.511, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 45: : 100it [00:00, 1626.20it/s, loss=0.511, v_num=9]\n                                                 \u001B[A\nEpoch 45:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.511, v_num=9]\nEpoch 46:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.511, v_num=9]\nEpoch 46: : 50it [00:00, 1178.30it/s, loss=0.506, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 46: : 100it [00:00, 1610.90it/s, loss=0.506, v_num=9]\n                                                 \u001B[A\nEpoch 46:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.506, v_num=9]\nEpoch 47:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.506, v_num=9]\nEpoch 47: : 50it [00:00, 1138.61it/s, loss=0.501, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 47: : 100it [00:00, 1617.92it/s, loss=0.501, v_num=9]\n                                                 \u001B[A\nEpoch 47:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.501, v_num=9]\nEpoch 48:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.501, v_num=9]\nEpoch 48: : 50it [00:00, 1130.25it/s, loss=0.496, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 48: : 100it [00:00, 1601.11it/s, loss=0.496, v_num=9]\n                                                 \u001B[A\nEpoch 48:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.496, v_num=9]\nEpoch 49:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.496, v_num=9]\nEpoch 49: : 50it [00:00, 1180.10it/s, loss=0.491, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 49: : 100it [00:00, 1656.51it/s, loss=0.491, v_num=9]\n                                                 \u001B[A\nEpoch 49:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.491, v_num=9]\nEpoch 50:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.491, v_num=9]\nEpoch 50: : 50it [00:00, 1201.94it/s, loss=0.486, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 50: : 100it [00:00, 1655.93it/s, loss=0.486, v_num=9]\n                                                 \u001B[A\nEpoch 50:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.486, v_num=9]\nEpoch 51:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.486, v_num=9]\nEpoch 51: : 50it [00:00, 1159.32it/s, loss=0.481, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 51: : 100it [00:00, 1607.05it/s, loss=0.481, v_num=9]\n                                                 \u001B[A\nEpoch 51:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.481, v_num=9]\nEpoch 52:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.481, v_num=9]\nEpoch 52: : 50it [00:00, 1134.01it/s, loss=0.476, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 52: : 100it [00:00, 1586.02it/s, loss=0.476, v_num=9]\n                                                 \u001B[A\nEpoch 52:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.476, v_num=9]\nEpoch 53:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.476, v_num=9]\nEpoch 53: : 50it [00:00, 1122.23it/s, loss=0.471, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 53: : 100it [00:00, 1516.12it/s, loss=0.471, v_num=9]\n                                                 \u001B[A\nEpoch 53:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.471, v_num=9]\nEpoch 54:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.471, v_num=9]\nEpoch 54: : 50it [00:00, 1103.90it/s, loss=0.466, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 54: : 100it [00:00, 1531.54it/s, loss=0.466, v_num=9]\n                                                 \u001B[A\nEpoch 54:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.466, v_num=9]\nEpoch 55:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.466, v_num=9]\nEpoch 55: : 50it [00:00, 1235.54it/s, loss=0.462, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 55: : 100it [00:00, 1608.28it/s, loss=0.462, v_num=9]\n                                                 \u001B[A\nEpoch 55:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.462, v_num=9]\nEpoch 56:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.462, v_num=9]\nEpoch 56: : 50it [00:00, 1055.19it/s, loss=0.457, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 56: : 100it [00:00, 1468.43it/s, loss=0.457, v_num=9]\n                                                 \u001B[A\nEpoch 56:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.457, v_num=9]\nEpoch 57:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.457, v_num=9]\nEpoch 57: : 50it [00:00, 1147.61it/s, loss=0.453, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 57: : 100it [00:00, 1641.34it/s, loss=0.453, v_num=9]\n                                                 \u001B[A\nEpoch 57:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.453, v_num=9]\nEpoch 58:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.453, v_num=9]\nEpoch 58: : 50it [00:00, 1211.39it/s, loss=0.448, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 58: : 100it [00:00, 1661.93it/s, loss=0.448, v_num=9]\n                                                 \u001B[A\nEpoch 58:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.448, v_num=9]\nEpoch 59:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.448, v_num=9]\nEpoch 59: : 50it [00:00, 925.62it/s, loss=0.443, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 59: : 100it [00:00, 1395.11it/s, loss=0.443, v_num=9]\n                                                 \u001B[A\nEpoch 59:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.443, v_num=9]\nEpoch 60:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.443, v_num=9]\nEpoch 60: : 50it [00:00, 1132.35it/s, loss=0.438, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 60: : 100it [00:00, 1649.21it/s, loss=0.438, v_num=9]\n                                                 \u001B[A\nEpoch 60:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.438, v_num=9]\nEpoch 61:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.438, v_num=9]\nEpoch 61: : 50it [00:00, 1188.98it/s, loss=0.434, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 61: : 100it [00:00, 1686.08it/s, loss=0.434, v_num=9]\n                                                 \u001B[A\nEpoch 61:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.434, v_num=9]\nEpoch 62:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.434, v_num=9]\nEpoch 62: : 50it [00:00, 1212.01it/s, loss=0.428, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 62: : 100it [00:00, 1711.51it/s, loss=0.428, v_num=9]\n                                                 \u001B[A\nEpoch 62:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.428, v_num=9]\nEpoch 63:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.428, v_num=9]\nEpoch 63: : 50it [00:00, 1041.86it/s, loss=0.424, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 63: : 100it [00:00, 1497.84it/s, loss=0.424, v_num=9]\n                                                 \u001B[A\nEpoch 63:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.424, v_num=9]\nEpoch 64:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.424, v_num=9]\nEpoch 64: : 50it [00:00, 1033.61it/s, loss=0.419, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 64: : 100it [00:00, 1402.29it/s, loss=0.419, v_num=9]\n                                                 \u001B[A\nEpoch 64:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.419, v_num=9]\nEpoch 65:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.419, v_num=9]\nEpoch 65: : 50it [00:00, 1095.60it/s, loss=0.415, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 65: : 100it [00:00, 1582.75it/s, loss=0.415, v_num=9]\n                                                 \u001B[A\nEpoch 65:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.415, v_num=9]\nEpoch 66:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.415, v_num=9]\nEpoch 66: : 50it [00:00, 1142.49it/s, loss=0.410, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 66: : 100it [00:00, 1639.37it/s, loss=0.410, v_num=9]\n                                                 \u001B[A\nEpoch 66:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.410, v_num=9]\nEpoch 67:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.410, v_num=9]\nEpoch 67: : 50it [00:00, 1122.16it/s, loss=0.406, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 67: : 100it [00:00, 1616.74it/s, loss=0.406, v_num=9]\n                                                 \u001B[A\nEpoch 67:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.406, v_num=9]\nEpoch 68:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.406, v_num=9]\nEpoch 68: : 50it [00:00, 1228.17it/s, loss=0.402, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 68: : 100it [00:00, 1716.06it/s, loss=0.402, v_num=9]\n                                                 \u001B[A\nEpoch 68:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.402, v_num=9]\nEpoch 69:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.402, v_num=9]\nEpoch 69: : 50it [00:00, 1216.16it/s, loss=0.398, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 69: : 100it [00:00, 1674.82it/s, loss=0.398, v_num=9]\n                                                 \u001B[A\nEpoch 69:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.398, v_num=9]\nEpoch 70:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.398, v_num=9]\nEpoch 70: : 50it [00:00, 1190.87it/s, loss=0.394, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 70: : 100it [00:00, 1673.44it/s, loss=0.394, v_num=9]\n                                                 \u001B[A\nEpoch 70:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.394, v_num=9]\nEpoch 71:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.394, v_num=9]\nEpoch 71: : 50it [00:00, 1218.88it/s, loss=0.390, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 71: : 100it [00:00, 1737.92it/s, loss=0.390, v_num=9]\n                                                 \u001B[A\nEpoch 71:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.390, v_num=9]\nEpoch 72:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.390, v_num=9]\nEpoch 72: : 50it [00:00, 1087.85it/s, loss=0.387, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 72: : 100it [00:00, 1513.22it/s, loss=0.387, v_num=9]\n                                                 \u001B[A\nEpoch 72:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.387, v_num=9]\nEpoch 73:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.387, v_num=9]\nEpoch 73: : 50it [00:00, 1008.56it/s, loss=0.383, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 73: : 100it [00:00, 1417.17it/s, loss=0.383, v_num=9]\n                                                 \u001B[A\nEpoch 73:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.383, v_num=9]\nEpoch 74:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.383, v_num=9]\nEpoch 74: : 50it [00:00, 1201.49it/s, loss=0.379, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 74: : 100it [00:00, 1642.20it/s, loss=0.379, v_num=9]\n                                                 \u001B[A\nEpoch 74:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.379, v_num=9]\nEpoch 75:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.379, v_num=9]\nEpoch 75: : 50it [00:00, 1192.67it/s, loss=0.375, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 75: : 100it [00:00, 1614.57it/s, loss=0.375, v_num=9]\n                                                 \u001B[A\nEpoch 75:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.375, v_num=9]\nEpoch 76:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.375, v_num=9]\nEpoch 76: : 50it [00:00, 1219.42it/s, loss=0.372, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 76: : 100it [00:00, 1704.71it/s, loss=0.372, v_num=9]\n                                                 \u001B[A\nEpoch 76:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.372, v_num=9]\nEpoch 77:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.372, v_num=9]\nEpoch 77: : 50it [00:00, 1248.65it/s, loss=0.368, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 77: : 100it [00:00, 1657.55it/s, loss=0.368, v_num=9]\n                                                 \u001B[A\nEpoch 77:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.368, v_num=9]\nEpoch 78:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.368, v_num=9]\nEpoch 78: : 50it [00:00, 1157.99it/s, loss=0.364, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 78: : 100it [00:00, 1675.94it/s, loss=0.364, v_num=9]\n                                                 \u001B[A\nEpoch 78:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.364, v_num=9]\nEpoch 79:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.364, v_num=9]\nEpoch 79: : 50it [00:00, 1159.36it/s, loss=0.361, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 79: : 100it [00:00, 1595.61it/s, loss=0.361, v_num=9]\n                                                 \u001B[A\nEpoch 79:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.361, v_num=9]\nEpoch 80:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.361, v_num=9]\nEpoch 80: : 50it [00:00, 1110.86it/s, loss=0.357, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 80: : 100it [00:00, 1484.43it/s, loss=0.357, v_num=9]\n                                                 \u001B[A\nEpoch 80:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.357, v_num=9]\nEpoch 81:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.357, v_num=9]\nEpoch 81: : 50it [00:00, 1065.32it/s, loss=0.354, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 81: : 100it [00:00, 1487.16it/s, loss=0.354, v_num=9]\n                                                 \u001B[A\nEpoch 81:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.354, v_num=9]\nEpoch 82:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.354, v_num=9]\nEpoch 82: : 50it [00:00, 1064.92it/s, loss=0.351, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 82: : 100it [00:00, 1478.98it/s, loss=0.351, v_num=9]\n                                                 \u001B[A\nEpoch 82:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.351, v_num=9]\nEpoch 83:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.351, v_num=9]\nEpoch 83: : 50it [00:00, 1266.46it/s, loss=0.348, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 83: : 100it [00:00, 1740.04it/s, loss=0.348, v_num=9]\n                                                 \u001B[A\nEpoch 83:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.348, v_num=9]\nEpoch 84:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.348, v_num=9]\nEpoch 84: : 50it [00:00, 1249.28it/s, loss=0.345, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 84: : 100it [00:00, 1747.21it/s, loss=0.345, v_num=9]\n                                                 \u001B[A\nEpoch 84:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.345, v_num=9]\nEpoch 85:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.345, v_num=9]\nEpoch 85: : 50it [00:00, 1128.24it/s, loss=0.342, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 85: : 100it [00:00, 1627.31it/s, loss=0.342, v_num=9]\n                                                 \u001B[A\nEpoch 85:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.342, v_num=9]\nEpoch 86:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.342, v_num=9]\nEpoch 86: : 50it [00:00, 1106.31it/s, loss=0.339, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 86: : 100it [00:00, 1621.28it/s, loss=0.339, v_num=9]\n                                                 \u001B[A\nEpoch 86:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.339, v_num=9]\nEpoch 87:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.339, v_num=9]\nEpoch 87: : 50it [00:00, 1224.32it/s, loss=0.336, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 87: : 100it [00:00, 1637.47it/s, loss=0.336, v_num=9]\n                                                 \u001B[A\nEpoch 87:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.336, v_num=9]\nEpoch 88:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.336, v_num=9]\nEpoch 88: : 50it [00:00, 1093.00it/s, loss=0.333, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 88: : 100it [00:00, 1545.34it/s, loss=0.333, v_num=9]\n                                                 \u001B[A\nEpoch 88:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.333, v_num=9]\nEpoch 89:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.333, v_num=9]\nEpoch 89: : 50it [00:00, 1161.41it/s, loss=0.330, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 89: : 100it [00:00, 1686.85it/s, loss=0.330, v_num=9]\n                                                 \u001B[A\nEpoch 89:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.330, v_num=9]\nEpoch 90:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.330, v_num=9]\nEpoch 90: : 50it [00:00, 1199.05it/s, loss=0.327, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 90: : 100it [00:00, 1752.33it/s, loss=0.327, v_num=9]\n                                                 \u001B[A\nEpoch 90:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.327, v_num=9]\nEpoch 91:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.327, v_num=9]\nEpoch 91: : 50it [00:00, 1268.23it/s, loss=0.324, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 91: : 100it [00:00, 1813.73it/s, loss=0.324, v_num=9]\n                                                 \u001B[A\nEpoch 91:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.324, v_num=9]\nEpoch 92:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.324, v_num=9]\nEpoch 92: : 50it [00:00, 1260.85it/s, loss=0.322, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 92: : 100it [00:00, 1829.32it/s, loss=0.322, v_num=9]\n                                                 \u001B[A\nEpoch 92:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.322, v_num=9]\nEpoch 93:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.322, v_num=9]\nEpoch 93: : 50it [00:00, 1214.68it/s, loss=0.319, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 93: : 100it [00:00, 1731.12it/s, loss=0.319, v_num=9]\n                                                 \u001B[A\nEpoch 93:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.319, v_num=9]\nEpoch 94:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.319, v_num=9]\nEpoch 94: : 50it [00:00, 1159.94it/s, loss=0.316, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 94: : 100it [00:00, 1624.46it/s, loss=0.316, v_num=9]\n                                                 \u001B[A\nEpoch 94:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.316, v_num=9]\nEpoch 95:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.316, v_num=9]\nEpoch 95: : 50it [00:00, 648.42it/s, loss=0.313, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 95: : 100it [00:00, 1035.55it/s, loss=0.313, v_num=9]\n                                                 \u001B[A\nEpoch 95:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.313, v_num=9]\nEpoch 96:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.313, v_num=9]\nEpoch 96: : 50it [00:00, 1145.26it/s, loss=0.311, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 96: : 100it [00:00, 1607.72it/s, loss=0.311, v_num=9]\n                                                 \u001B[A\nEpoch 96:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.311, v_num=9]\nEpoch 97:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.311, v_num=9]\nEpoch 97: : 50it [00:00, 1254.14it/s, loss=0.308, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 97: : 100it [00:00, 1652.82it/s, loss=0.308, v_num=9]\n                                                 \u001B[A\nEpoch 97:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.308, v_num=9]\nEpoch 98:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.308, v_num=9]\nEpoch 98: : 50it [00:00, 1134.55it/s, loss=0.306, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 98: : 100it [00:00, 1626.12it/s, loss=0.306, v_num=9]\n                                                 \u001B[A\nEpoch 98:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.306, v_num=9]\nEpoch 99:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.306, v_num=9]\nEpoch 99: : 50it [00:00, 1185.23it/s, loss=0.303, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 99: : 100it [00:00, 1687.33it/s, loss=0.303, v_num=9]\n                                                 \u001B[A\nEpoch 99:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.303, v_num=9]\nEpoch 100:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.303, v_num=9]\nEpoch 100: : 50it [00:00, 1080.04it/s, loss=0.301, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 100: : 100it [00:00, 1580.31it/s, loss=0.301, v_num=9]\n                                                 \u001B[A\nEpoch 100:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.301, v_num=9]\nEpoch 101:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.301, v_num=9]\nEpoch 101: : 50it [00:00, 1002.63it/s, loss=0.294, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 101: : 100it [00:00, 1416.59it/s, loss=0.294, v_num=9]\n                                                 \u001B[A\nEpoch 101:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.294, v_num=9]\nEpoch 102:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.294, v_num=9]\nEpoch 102: : 50it [00:00, 1075.59it/s, loss=0.287, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 102: : 100it [00:00, 1491.54it/s, loss=0.287, v_num=9]\n                                                 \u001B[A\nEpoch 102:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.287, v_num=9]\nEpoch 103:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.287, v_num=9]\nEpoch 103: : 50it [00:00, 1123.57it/s, loss=0.280, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 103: : 100it [00:00, 1593.83it/s, loss=0.280, v_num=9]\n                                                 \u001B[A\nEpoch 103:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.280, v_num=9]\nEpoch 104:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.280, v_num=9]\nEpoch 104: : 50it [00:00, 1147.32it/s, loss=0.273, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 104: : 100it [00:00, 1652.73it/s, loss=0.273, v_num=9]\n                                                 \u001B[A\nEpoch 104:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.273, v_num=9]\nEpoch 105:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.273, v_num=9]\nEpoch 105: : 50it [00:00, 1202.65it/s, loss=0.267, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 105: : 100it [00:00, 1629.65it/s, loss=0.267, v_num=9]\n                                                 \u001B[A\nEpoch 105:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.267, v_num=9]\nEpoch 106:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.267, v_num=9]\nEpoch 106: : 50it [00:00, 1240.32it/s, loss=0.261, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 106: : 100it [00:00, 1749.66it/s, loss=0.261, v_num=9]\n                                                 \u001B[A\nEpoch 106:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.261, v_num=9]\nEpoch 107:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.261, v_num=9]\nEpoch 107: : 50it [00:00, 1244.77it/s, loss=0.254, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 107: : 100it [00:00, 1732.86it/s, loss=0.254, v_num=9]\n                                                 \u001B[A\nEpoch 107:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.254, v_num=9]\nEpoch 108:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.254, v_num=9]\nEpoch 108: : 50it [00:00, 1089.78it/s, loss=0.248, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 108: : 100it [00:00, 1480.19it/s, loss=0.248, v_num=9]\n                                                 \u001B[A\nEpoch 108:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.248, v_num=9]\nEpoch 109:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.248, v_num=9]\nEpoch 109: : 50it [00:00, 1149.03it/s, loss=0.241, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 109: : 100it [00:00, 1577.31it/s, loss=0.241, v_num=9]\n                                                 \u001B[A\nEpoch 109:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.241, v_num=9]\nEpoch 110:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.241, v_num=9]\nEpoch 110: : 50it [00:00, 1211.01it/s, loss=0.236, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 110: : 100it [00:00, 1679.18it/s, loss=0.236, v_num=9]\n                                                 \u001B[A\nEpoch 110:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.236, v_num=9]\nEpoch 111:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.236, v_num=9]\nEpoch 111: : 50it [00:00, 1166.43it/s, loss=0.230, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 111: : 100it [00:00, 1674.37it/s, loss=0.230, v_num=9]\n                                                 \u001B[A\nEpoch 111:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.230, v_num=9]\nEpoch 112:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.230, v_num=9]\nEpoch 112: : 50it [00:00, 1243.47it/s, loss=0.225, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 112: : 100it [00:00, 1757.53it/s, loss=0.225, v_num=9]\n                                                 \u001B[A\nEpoch 112:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.225, v_num=9]\nEpoch 113:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.225, v_num=9]\nEpoch 113: : 50it [00:00, 1182.37it/s, loss=0.219, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 113: : 100it [00:00, 1651.69it/s, loss=0.219, v_num=9]\n                                                 \u001B[A\nEpoch 113:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.219, v_num=9]\nEpoch 114:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.219, v_num=9]\nEpoch 114: : 50it [00:00, 1143.83it/s, loss=0.214, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 114: : 100it [00:00, 1606.09it/s, loss=0.214, v_num=9]\n                                                 \u001B[A\nEpoch 114:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.214, v_num=9]\nEpoch 115:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.214, v_num=9]\nEpoch 115: : 50it [00:00, 1140.95it/s, loss=0.208, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 115: : 100it [00:00, 1602.15it/s, loss=0.208, v_num=9]\n                                                 \u001B[A\nEpoch 115:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.208, v_num=9]\nEpoch 116:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.208, v_num=9]\nEpoch 116: : 50it [00:00, 1126.20it/s, loss=0.201, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 116: : 100it [00:00, 1513.13it/s, loss=0.201, v_num=9]\n                                                 \u001B[A\nEpoch 116:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.201, v_num=9]\nEpoch 117:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.201, v_num=9]\nEpoch 117: : 50it [00:00, 1162.08it/s, loss=0.196, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 117: : 100it [00:00, 1593.22it/s, loss=0.196, v_num=9]\n                                                 \u001B[A\nEpoch 117:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.196, v_num=9]\nEpoch 118:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.196, v_num=9]\nEpoch 118: : 50it [00:00, 1189.85it/s, loss=0.190, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 118: : 100it [00:00, 1666.53it/s, loss=0.190, v_num=9]\n                                                 \u001B[A\nEpoch 118:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.190, v_num=9]\nEpoch 119:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.190, v_num=9]\nEpoch 119: : 50it [00:00, 1158.37it/s, loss=0.186, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 119: : 100it [00:00, 1698.97it/s, loss=0.186, v_num=9]\n                                                 \u001B[A\nEpoch 119:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.186, v_num=9]\nEpoch 120:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.186, v_num=9]\nEpoch 120: : 50it [00:00, 1242.67it/s, loss=0.181, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 120: : 100it [00:00, 1716.33it/s, loss=0.181, v_num=9]\n                                                 \u001B[A\nEpoch 120:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.181, v_num=9]\nEpoch 121:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.181, v_num=9]\nEpoch 121: : 50it [00:00, 1286.86it/s, loss=0.176, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 121: : 100it [00:00, 1706.78it/s, loss=0.176, v_num=9]\n                                                 \u001B[A\nEpoch 121:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.176, v_num=9]\nEpoch 122:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.176, v_num=9]\nEpoch 122: : 50it [00:00, 1214.30it/s, loss=0.172, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 122: : 100it [00:00, 1726.96it/s, loss=0.172, v_num=9]\n                                                 \u001B[A\nEpoch 122:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.172, v_num=9]\nEpoch 123:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.172, v_num=9]\nEpoch 123: : 50it [00:00, 1150.90it/s, loss=0.166, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 123: : 100it [00:00, 1557.39it/s, loss=0.166, v_num=9]\n                                                 \u001B[A\nEpoch 123:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.166, v_num=9]\nEpoch 124:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.166, v_num=9]\nEpoch 124: : 50it [00:00, 1110.66it/s, loss=0.162, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 124: : 100it [00:00, 1539.12it/s, loss=0.162, v_num=9]\n                                                 \u001B[A\nEpoch 124:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.162, v_num=9]\nEpoch 125:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.162, v_num=9]\nEpoch 125: : 50it [00:00, 1156.23it/s, loss=0.158, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 125: : 100it [00:00, 1609.79it/s, loss=0.158, v_num=9]\n                                                 \u001B[A\nEpoch 125:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.158, v_num=9]\nEpoch 126:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.158, v_num=9]\nEpoch 126: : 50it [00:00, 1197.63it/s, loss=0.154, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 126: : 100it [00:00, 1740.92it/s, loss=0.154, v_num=9]\n                                                 \u001B[A\nEpoch 126:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.154, v_num=9]\nEpoch 127:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.154, v_num=9]\nEpoch 127: : 50it [00:00, 1283.89it/s, loss=0.150, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 127: : 100it [00:00, 1853.77it/s, loss=0.150, v_num=9]\n                                                 \u001B[A\nEpoch 127:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.150, v_num=9]\nEpoch 128:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.150, v_num=9]\nEpoch 128: : 50it [00:00, 1241.62it/s, loss=0.146, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 128: : 100it [00:00, 1628.03it/s, loss=0.146, v_num=9]\n                                                 \u001B[A\nEpoch 128:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.146, v_num=9]\nEpoch 129:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.146, v_num=9]\nEpoch 129: : 50it [00:00, 1218.50it/s, loss=0.142, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 129: : 100it [00:00, 1799.24it/s, loss=0.142, v_num=9]\n                                                 \u001B[A\nEpoch 129:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.142, v_num=9]\nEpoch 130:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.142, v_num=9]\nEpoch 130: : 50it [00:00, 1207.88it/s, loss=0.138, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 130: : 100it [00:00, 1694.57it/s, loss=0.138, v_num=9]\n                                                 \u001B[A\nEpoch 130:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.138, v_num=9]\nEpoch 131:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.138, v_num=9]\nEpoch 131: : 50it [00:00, 1087.81it/s, loss=0.135, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 131: : 100it [00:00, 1522.18it/s, loss=0.135, v_num=9]\n                                                 \u001B[A\nEpoch 131:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.135, v_num=9]\nEpoch 132:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.135, v_num=9]\nEpoch 132: : 50it [00:00, 1094.71it/s, loss=0.132, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 132: : 100it [00:00, 1534.85it/s, loss=0.132, v_num=9]\n                                                 \u001B[A\nEpoch 132:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.132, v_num=9]\nEpoch 133:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.132, v_num=9]\nEpoch 133: : 50it [00:00, 1177.27it/s, loss=0.128, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 133: : 100it [00:00, 1618.54it/s, loss=0.128, v_num=9]\n                                                 \u001B[A\nEpoch 133:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.128, v_num=9]\nEpoch 134:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.128, v_num=9]\nEpoch 134: : 50it [00:00, 1182.95it/s, loss=0.124, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 134: : 100it [00:00, 1692.79it/s, loss=0.124, v_num=9]\n                                                 \u001B[A\nEpoch 134:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.124, v_num=9]\nEpoch 135:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.124, v_num=9]\nEpoch 135: : 50it [00:00, 1288.63it/s, loss=0.121, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 135: : 100it [00:00, 1751.63it/s, loss=0.121, v_num=9]\n                                                 \u001B[A\nEpoch 135:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.121, v_num=9]\nEpoch 136:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.121, v_num=9]\nEpoch 136: : 50it [00:00, 1249.50it/s, loss=0.118, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 136: : 100it [00:00, 1753.96it/s, loss=0.118, v_num=9]\n                                                 \u001B[A\nEpoch 136:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.118, v_num=9]\nEpoch 137:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.118, v_num=9]\nEpoch 137: : 50it [00:00, 1207.91it/s, loss=0.114, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 137: : 100it [00:00, 1668.95it/s, loss=0.114, v_num=9]\n                                                 \u001B[A\nEpoch 137:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.114, v_num=9]\nEpoch 138:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.114, v_num=9]\nEpoch 138: : 50it [00:00, 1106.12it/s, loss=0.111, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 138: : 100it [00:00, 1519.71it/s, loss=0.111, v_num=9]\n                                                 \u001B[A\nEpoch 138:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.111, v_num=9]\nEpoch 139:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.111, v_num=9]\nEpoch 139: : 50it [00:00, 1117.69it/s, loss=0.107, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 139: : 100it [00:00, 1564.85it/s, loss=0.107, v_num=9]\n                                                 \u001B[A\nEpoch 139:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.107, v_num=9]\nEpoch 140:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.107, v_num=9]\nEpoch 140: : 50it [00:00, 1193.43it/s, loss=0.103, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 140: : 100it [00:00, 1669.28it/s, loss=0.103, v_num=9]\n                                                 \u001B[A\nEpoch 140:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.103, v_num=9]\nEpoch 141:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.103, v_num=9]\nEpoch 141: : 50it [00:00, 1257.89it/s, loss=0.100, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 141: : 100it [00:00, 1242.90it/s, loss=0.100, v_num=9]\n                                                 \u001B[A\nEpoch 141:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.100, v_num=9]\nEpoch 142:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.100, v_num=9]\nEpoch 142: : 50it [00:00, 1351.10it/s, loss=0.097, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 142: : 100it [00:00, 1813.90it/s, loss=0.097, v_num=9]\n                                                 \u001B[A\nEpoch 142:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.097, v_num=9]\nEpoch 143:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.097, v_num=9]\nEpoch 143: : 50it [00:00, 1226.22it/s, loss=0.095, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 143: : 100it [00:00, 1744.44it/s, loss=0.095, v_num=9]\n                                                 \u001B[A\nEpoch 143:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.095, v_num=9]\nEpoch 144:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.095, v_num=9]\nEpoch 144: : 50it [00:00, 1226.61it/s, loss=0.091, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 144: : 100it [00:00, 1656.64it/s, loss=0.091, v_num=9]\n                                                 \u001B[A\nEpoch 144:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.091, v_num=9]\nEpoch 145:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.091, v_num=9]\nEpoch 145: : 50it [00:00, 1158.40it/s, loss=0.088, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 145: : 100it [00:00, 1020.65it/s, loss=0.088, v_num=9]\n                                                 \u001B[A\nEpoch 145:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.088, v_num=9]\nEpoch 146:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.088, v_num=9]\nEpoch 146: : 50it [00:00, 1157.62it/s, loss=0.085, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 146: : 100it [00:00, 1630.65it/s, loss=0.085, v_num=9]\n                                                 \u001B[A\nEpoch 146:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.085, v_num=9]\nEpoch 147:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.085, v_num=9]\nEpoch 147: : 50it [00:00, 1218.42it/s, loss=0.083, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 147: : 100it [00:00, 1678.98it/s, loss=0.083, v_num=9]\n                                                 \u001B[A\nEpoch 147:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.083, v_num=9]\nEpoch 148:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.083, v_num=9]\nEpoch 148: : 50it [00:00, 1278.11it/s, loss=0.081, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 148: : 100it [00:00, 1794.52it/s, loss=0.081, v_num=9]\n                                                 \u001B[A\nEpoch 148:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.081, v_num=9]\nEpoch 149:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.081, v_num=9]\nEpoch 149: : 50it [00:00, 1194.11it/s, loss=0.078, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 149: : 100it [00:00, 1692.99it/s, loss=0.078, v_num=9]\n                                                 \u001B[A\nEpoch 149:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.078, v_num=9]\nEpoch 150:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.078, v_num=9]\nEpoch 150: : 50it [00:00, 1251.10it/s, loss=0.076, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 150: : 100it [00:00, 1680.96it/s, loss=0.076, v_num=9]\n                                                 \u001B[A\nEpoch 150:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.076, v_num=9]\nEpoch 151:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.076, v_num=9]\nEpoch 151: : 50it [00:00, 1222.73it/s, loss=0.074, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 151: : 100it [00:00, 1803.00it/s, loss=0.074, v_num=9]\n                                                 \u001B[A\nEpoch 151:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.074, v_num=9]\nEpoch 152:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.074, v_num=9]\nEpoch 152: : 50it [00:00, 1081.34it/s, loss=0.072, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 152: : 100it [00:00, 1445.01it/s, loss=0.072, v_num=9]\n                                                 \u001B[A\nEpoch 152:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.072, v_num=9]\nEpoch 153:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.072, v_num=9]\nEpoch 153: : 50it [00:00, 987.50it/s, loss=0.070, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 153: : 100it [00:00, 1382.21it/s, loss=0.070, v_num=9]\n                                                 \u001B[A\nEpoch 153:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.070, v_num=9]\nEpoch 154:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.070, v_num=9]\nEpoch 154: : 50it [00:00, 1111.92it/s, loss=0.068, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 154: : 100it [00:00, 1526.94it/s, loss=0.068, v_num=9]\n                                                 \u001B[A\nEpoch 154:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.068, v_num=9]\nEpoch 155:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.068, v_num=9]\nEpoch 155: : 50it [00:00, 1236.59it/s, loss=0.067, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 155: : 100it [00:00, 1728.43it/s, loss=0.067, v_num=9]\n                                                 \u001B[A\nEpoch 155:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.067, v_num=9]\nEpoch 156:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.067, v_num=9]\nEpoch 156: : 50it [00:00, 1215.19it/s, loss=0.065, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 156: : 100it [00:00, 1703.44it/s, loss=0.065, v_num=9]\n                                                 \u001B[A\nEpoch 156:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.065, v_num=9]\nEpoch 157:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.065, v_num=9]\nEpoch 157: : 50it [00:00, 1241.37it/s, loss=0.063, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 157: : 100it [00:00, 1684.35it/s, loss=0.063, v_num=9]\n                                                 \u001B[A\nEpoch 157:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.063, v_num=9]\nEpoch 158:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.063, v_num=9]\nEpoch 158: : 50it [00:00, 1104.29it/s, loss=0.061, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 158: : 100it [00:00, 1575.35it/s, loss=0.061, v_num=9]\n                                                 \u001B[A\nEpoch 158:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.061, v_num=9]\nEpoch 159:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.061, v_num=9]\nEpoch 159: : 50it [00:00, 768.33it/s, loss=0.060, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 159: : 100it [00:00, 1085.07it/s, loss=0.060, v_num=9]\n                                                 \u001B[A\nEpoch 159:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.060, v_num=9]\nEpoch 160:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.060, v_num=9]\nEpoch 160: : 50it [00:00, 1053.54it/s, loss=0.058, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 160: : 100it [00:00, 1517.17it/s, loss=0.058, v_num=9]\n                                                 \u001B[A\nEpoch 160:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.058, v_num=9]\nEpoch 161:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.058, v_num=9]\nEpoch 161: : 50it [00:00, 1008.35it/s, loss=0.057, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 161: : 100it [00:00, 1357.62it/s, loss=0.057, v_num=9]\n                                                 \u001B[A\nEpoch 161:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.057, v_num=9]\nEpoch 162:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.057, v_num=9]\nEpoch 162: : 50it [00:00, 1110.42it/s, loss=0.056, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 162: : 100it [00:00, 1582.70it/s, loss=0.056, v_num=9]\n                                                 \u001B[A\nEpoch 162:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.056, v_num=9]\nEpoch 163:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.056, v_num=9]\nEpoch 163: : 50it [00:00, 1188.16it/s, loss=0.055, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 163: : 100it [00:00, 1719.01it/s, loss=0.055, v_num=9]\n                                                 \u001B[A\nEpoch 163:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.055, v_num=9]\nEpoch 164:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.055, v_num=9]\nEpoch 164: : 50it [00:00, 1237.48it/s, loss=0.054, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 164: : 100it [00:00, 1785.71it/s, loss=0.054, v_num=9]\n                                                 \u001B[A\nEpoch 164:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.054, v_num=9]\nEpoch 165:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.054, v_num=9]\nEpoch 165: : 50it [00:00, 1196.71it/s, loss=0.053, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 165: : 100it [00:00, 1567.86it/s, loss=0.053, v_num=9]\n                                                 \u001B[A\nEpoch 165:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.053, v_num=9]\nEpoch 166:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.053, v_num=9]\nEpoch 166: : 50it [00:00, 1097.84it/s, loss=0.052, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 166: : 100it [00:00, 1587.96it/s, loss=0.052, v_num=9]\n                                                 \u001B[A\nEpoch 166:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.052, v_num=9]\nEpoch 167:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.052, v_num=9]\nEpoch 167: : 50it [00:00, 1201.94it/s, loss=0.052, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 167: : 100it [00:00, 1642.78it/s, loss=0.052, v_num=9]\n                                                 \u001B[A\nEpoch 167:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.052, v_num=9]\nEpoch 168:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.052, v_num=9]\nEpoch 168: : 50it [00:00, 1283.31it/s, loss=0.051, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 168: : 100it [00:00, 1812.09it/s, loss=0.051, v_num=9]\n                                                 \u001B[A\nEpoch 168:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.051, v_num=9]\nEpoch 169:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.051, v_num=9]\nEpoch 169: : 50it [00:00, 1313.13it/s, loss=0.049, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 169: : 100it [00:00, 1767.19it/s, loss=0.049, v_num=9]\n                                                 \u001B[A\nEpoch 169:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.049, v_num=9]\nEpoch 170:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.049, v_num=9]\nEpoch 170: : 50it [00:00, 823.41it/s, loss=0.049, v_num=9]           \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 170: : 100it [00:00, 1280.59it/s, loss=0.049, v_num=9]\n                                                 \u001B[A\nEpoch 170:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.049, v_num=9]\nEpoch 171:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.049, v_num=9]\nEpoch 171: : 50it [00:00, 1290.69it/s, loss=0.047, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 171: : 100it [00:00, 1759.88it/s, loss=0.047, v_num=9]\n                                                 \u001B[A\nEpoch 171:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.047, v_num=9]\nEpoch 172:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.047, v_num=9]\nEpoch 172: : 50it [00:00, 1199.76it/s, loss=0.046, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 172: : 100it [00:00, 1575.57it/s, loss=0.046, v_num=9]\n                                                 \u001B[A\nEpoch 172:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.046, v_num=9]\nEpoch 173:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.046, v_num=9]\nEpoch 173: : 50it [00:00, 1077.65it/s, loss=0.046, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 173: : 100it [00:00, 1551.20it/s, loss=0.046, v_num=9]\n                                                 \u001B[A\nEpoch 173:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.046, v_num=9]\nEpoch 174:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.046, v_num=9]\nEpoch 174: : 50it [00:00, 1085.94it/s, loss=0.045, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 174: : 100it [00:00, 1462.57it/s, loss=0.045, v_num=9]\n                                                 \u001B[A\nEpoch 174:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.045, v_num=9]\nEpoch 175:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.045, v_num=9]\nEpoch 175: : 50it [00:00, 1139.24it/s, loss=0.044, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 175: : 100it [00:00, 1675.54it/s, loss=0.044, v_num=9]\n                                                 \u001B[A\nEpoch 175:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.044, v_num=9]\nEpoch 176:   0%|          | 0/2 [00:00<?, ?it/s, loss=0.044, v_num=9]\nEpoch 176: : 50it [00:00, 1219.59it/s, loss=0.043, v_num=9]          \nValidating:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\nEpoch 176: : 100it [00:00, 1680.44it/s, loss=0.043, v_num=9]\n                                                 \u001B[A\nEpoch 176: : 100it [00:00, 1614.15it/s, loss=0.043, v_num=9]\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n",
  "history_begin_time" : 1646144608407,
  "history_end_time" : 1646144628820,
  "history_notes" : null,
  "history_process" : "r5a5p5",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "76z9d4it3iq",
  "history_input" : "from pathlib import Path\nimport sys\n\nsys.path.append(\"..\")\n\nfrom src_models_model import Model\nfrom src_analysis import plot_results\n\n\ndef kenya_crop_type_mapper():\n    data_dir = \"../data\"\n\n    test_folder = Path(\"../data/raw/earth_engine_plant_village_kenya/\")\n    test_files = test_folder.glob(\"*.tif\")\n    print(test_files)\n\n    model_path = \"../data/lightning_logs/version_8/checkpoints/epoch=165.ckpt\"\n    print(f\"Using model {model_path}\")\n\n    model = Model.load_from_checkpoint(model_path)\n\n    for test_path in test_files:\n\n        save_dir = Path(data_dir) / \"Autoencoder\"\n        save_dir.mkdir(exist_ok=True)\n\n        print(f\"Running for {test_path}\")\n\n        savepath = save_dir / f\"preds_{test_path.name}\"\n        if savepath.exists():\n            print(\"File already generated. Skipping\")\n            continue\n\n        out_forecasted = model.predict(test_path, with_forecaster=True)\n        plot_results(out_forecasted, test_path, savepath=save_dir, prefix=\"forecasted_\")\n\n        out_normal = model.predict(test_path, with_forecaster=False)\n        plot_results(out_normal, test_path, savepath=save_dir, prefix=\"full_input_\")\n\n        out_forecasted.to_netcdf(save_dir / f\"preds_forecasted_{test_path.name}.nc\")\n        out_normal.to_netcdf(save_dir / f\"preds_normal_{test_path.name}.nc\")\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...predict.py\")\n    kenya_crop_type_mapper()\n",
  "history_output" : "  0%|          | 0/288 [00:00<?, ?it/s]\n 89%|████████▉ | 256/288 [00:00<00:00, 2548.30it/s]\n320it [00:00, 1339.84it/s]                         \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2182.87it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1609.84it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2264.73it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1627.03it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2183.51it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1623.84it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2312.87it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1393.75it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2220.46it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1602.59it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2300.45it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1635.25it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2374.81it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1660.34it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2323.12it/s]             \n  0%|          | 0/305 [00:00<?, ?it/s]\n320it [00:00, 1322.31it/s]             \n  0%|          | 0/305 [00:00<?, ?it/s]\n320it [00:00, 2248.38it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 1622.86it/s]             \n  0%|          | 0/288 [00:00<?, ?it/s]\n320it [00:00, 2291.11it/s]             \nStarting...predict.py\n<generator object Path.glob at 0x7f92c4818200>\nUsing model ../data/lightning_logs/version_8/checkpoints/epoch=165.ckpt\nPredicting 7 timesteps in the forecaster\nUsing 1 layers for the global classifier\nUsing 2 layers for the local classifier\nRunning for ../data/raw/earth_engine_plant_village_kenya/1_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/9_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/6_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/3_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/4_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/5_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/0_2018-04-21_2019-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/2_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/7_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\nRunning for ../data/raw/earth_engine_plant_village_kenya/8_2019-04-22_2020-04-16.tif\nNo GPU - not using one\nNo GPU - not using one\n",
  "history_begin_time" : 1646144630814,
  "history_end_time" : 1646144661933,
  "history_notes" : null,
  "history_process" : "delykw",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "i8i0r78rl6w",
  "history_input" : "import pytorch_lightning as pl\nfrom pathlib import Path\nfrom argparse import ArgumentParser\n\nimport sys\n\nsys.path.append(\"..\")\nfrom src_models_model import Model\n\n\ndef get_checkpoint(data_folder: Path, version: int) -> str:\n\n    log_folder = data_folder / \"lightning_logs\" / f\"version_{version}\" / \"checkpoints\"\n    checkpoint = list(log_folder.glob(\"*.ckpt\"))\n    return str(checkpoint[0])\n\n\ndef test_model():\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--version\", type=int, default=0)\n\n    args = parser.parse_args()\n\n    model_path = get_checkpoint(Path(\"../data\"), args.version)\n\n    print(f\"Using model {model_path}\")\n\n    model = Model.load_from_checkpoint(model_path)\n\n    trainer = pl.Trainer()\n    trainer.test(model)\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...test.py\")\n    test_model()\n",
  "history_output" : "Starting...test.py\nTraceback (most recent call last):\n  File \"scripts_test.py\", line 37, in <module>\n    test_model()\n  File \"scripts_test.py\", line 25, in test_model\n    model_path = get_checkpoint(Path(\"../data\"), args.version)\n  File \"scripts_test.py\", line 15, in get_checkpoint\n    return str(checkpoint[0])\nIndexError: list index out of range\n",
  "history_begin_time" : 1646144663936,
  "history_end_time" : 1646144669292,
  "history_notes" : null,
  "history_process" : "q1j13t",
  "host_id" : "100001",
  "indicator" : "Done"
}]
