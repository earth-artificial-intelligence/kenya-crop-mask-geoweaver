[{
  "history_id" : "ek02n14kp14",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1666118339821,
  "history_end_time" : 1666118344060,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "vzq2hmk13ds",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655909920910,
  "history_end_time" : 1655909924641,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "syo1tkcyzkc",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655908841870,
  "history_end_time" : 1655908844781,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "9glq91w61u6",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655907448421,
  "history_end_time" : 1655907451177,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "sm0rvln84qn",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_classifier.py\", line 3, in <module>\n    import pytorch_lightning as pl\nModuleNotFoundError: No module named 'pytorch_lightning'\n",
  "history_begin_time" : 1655907403968,
  "history_end_time" : 1655907404159,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "laoz87xl513",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_classifier.py\", line 3, in <module>\n    import pytorch_lightning as pl\nModuleNotFoundError: No module named 'pytorch_lightning'\n",
  "history_begin_time" : 1655865873011,
  "history_end_time" : 1655865873193,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nz3gnn5yg3t",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655865103138,
  "history_end_time" : 1655865103592,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "13adfcp9rex",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347437982,
  "history_end_time" : 1647347440159,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7wktwtiqv4s",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347368401,
  "history_end_time" : 1647347370835,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "k4zeei9uz3e",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347280721,
  "history_end_time" : 1647347283417,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6404aiv03yo",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347147655,
  "history_end_time" : 1647347149843,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "366i0hp861g",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647346838625,
  "history_end_time" : 1647346840967,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "injasle3bpb",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647346680616,
  "history_end_time" : 1647346682770,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b6474svqfkb",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647345857948,
  "history_end_time" : 1647345859520,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "t1bektouemy",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647345668950,
  "history_end_time" : 1647345671454,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bcdjaznoydg",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647340171661,
  "history_end_time" : 1647340174320,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ki9ygofb0bx",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646147754030,
  "history_end_time" : 1646147756981,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "e013l6ome12",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646138297917,
  "history_end_time" : 1646138300658,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yz7tyqedgkm",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646138199257,
  "history_end_time" : 1646138201658,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ntz3zintc2j",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646138108884,
  "history_end_time" : 1646138111345,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dovuhhrml14",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646137795677,
  "history_end_time" : 1646137795829,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ge8xtpit18v",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646137705585,
  "history_end_time" : 1646137707871,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "r0seqg3qagk",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646137596636,
  "history_end_time" : 1646137599723,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Done"
},]
