[{
  "history_id" : "vpv6w5z2koa",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1666118339996,
  "history_end_time" : 1666118344037,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ne76cpwlzgl",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655909920819,
  "history_end_time" : 1655909924644,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "w3yvsylbhbd",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655908837967,
  "history_end_time" : 1655908838846,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "i7ctpqfum8w",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655907448879,
  "history_end_time" : 1655907451198,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "sxvy4i7zvvi",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_forecaster.py\", line 3, in <module>\n    import pytorch_lightning as pl\nModuleNotFoundError: No module named 'pytorch_lightning'\n",
  "history_begin_time" : 1655865868477,
  "history_end_time" : 1655865868657,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "rpsea53dpsg",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1655865104068,
  "history_end_time" : 1655865106432,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o11dcomtetf",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347438811,
  "history_end_time" : 1647347440857,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "enxphu0gfzv",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347368392,
  "history_end_time" : 1647347370835,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yca2xv0nqa4",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347281246,
  "history_end_time" : 1647347283829,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ql2gd6kbbkn",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647347144440,
  "history_end_time" : 1647347146644,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cjbptr3z1i9",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647346838898,
  "history_end_time" : 1647346841121,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7tza8tgm2hw",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1647346680373,
  "history_end_time" : 1647346682754,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2jiveyu1c6y",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646138297853,
  "history_end_time" : 1646138300642,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6w27swjijdt",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646138200205,
  "history_end_time" : 1646138202544,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lgt1279u6ln",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646138113089,
  "history_end_time" : 1646138115344,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "wfiqrfk32hw",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646137797836,
  "history_end_time" : 1646137800303,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "x5mr754zhd9",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "",
  "history_begin_time" : 1646137705019,
  "history_end_time" : 1646137707855,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "x4lzmyd1o9q",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Dict, Tuple, Type, Any, List, Optional\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Forecaster(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict a multispectral sequence.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n    :param output_timesteps: The number of timesteps to predict\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.forecasting_vector_size: The size of the hidden vector in the LSTM\n        Default = 128\n    :param hparams.forecasting_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    \"\"\"\n\n    def __init__(self, num_bands: int, output_timesteps: int, hparams: Namespace,) -> None:\n        super().__init__()\n        self.output_timesteps = output_timesteps\n        self.lstm = UnrolledLSTM(\n            input_size=num_bands,\n            hidden_size=hparams.forecasting_vector_size,\n            dropout=hparams.forecasting_dropout,\n            batch_first=True,\n        )\n\n        self.to_bands = nn.Linear(\n            in_features=hparams.forecasting_vector_size, out_features=num_bands\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n\n        hidden_tuple: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n\n        input_timesteps = x.shape[1]\n        assert input_timesteps >= 1\n\n        predicted_output: List[torch.Tensor] = []\n        for i in range(input_timesteps):\n            input = x[:, i : i + 1, :]\n            output, hidden_tuple = self.lstm(input, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n\n        # we have already predicted the first output timestep (the last\n        # output of the loop above)\n        for i in range(self.output_timesteps - 1):\n            output, hidden_tuple = self.lstm(output, hidden_tuple)\n            output = self.to_bands(torch.transpose(output[0, :, :, :], 0, 1))\n            predicted_output.append(output)\n        return torch.cat(predicted_output, dim=1)\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            \"--forecasting_vector_size\": (int, 256),\n            \"--forecasting_dropout\": (float, 0.2),\n        }\n\n        for key, vals in parser_args.items():\n            parser.add_argument(key, type=vals[0], default=vals[1])\n\n        return parser\n",
  "history_output" : "\nStream closed",
  "history_begin_time" : 1646137596612,
  "history_end_time" : 1646137596660,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "vdlcofwac4b",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403257,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
