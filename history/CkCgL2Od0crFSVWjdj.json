[{
  "history_id" : "cz010n7xyou",
  "history_input" : "import sys\nfrom pathlib import Path\nfrom datetime import date\nimport os\n\nsys.path.append(\"..\")\n\nfrom src_exporters_geowiki import *\nfrom src_exporters_sentinel_geowiki import *\nfrom src_exporters_sentinel_pv_kenya import *\nfrom src_exporters_sentinel_kenya_non_crop import *\nfrom src_exporters_sentinel_region import *\nfrom src_exporters_sentinel_utils import *\n\n\n\n\ndef export_geowiki():\n    if len(os.listdir('../data/raw/geowiki_landcover_2017')) == 0:\n        exporter = GeoWikiExporter(Path(\"../data\"))\n        exporter.export()\n\n\ndef export_geowiki_sentinel_ee():\n    if len(os.listdir('../data/raw/earth_engine_geowiki')) == 0:\n        exporter = GeoWikiSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_plant_village_sentinel_ee():\n    if len(os.listdir('../data/raw/earth_engine_plant_village_kenya')) == 0:\n        exporter = KenyaPVSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_kenya_non_crop():\n    if len(os.listdir('../data/raw/earth_engine_kenya_non_crop')) == 0:\n        exporter = KenyaNonCropSentinelExporter(Path(\"../data\"))\n        exporter.export_for_labels(\n            num_labelled_points=10, monitor=False, checkpoint=True)\n\n\ndef export_region():\n    if len(os.listdir('../data/raw/earth_engine_region_busia_partial_slow_cloudfree')) == 0:\n        exporter = RegionalExporter(Path(\"../data\"))\n        exporter.export_for_region(\n            region_name=\"Busia\",\n            end_date=date(2020, 9, 13),\n            num_timesteps=5,\n            monitor=False,\n            checkpoint=True,\n            metres_per_polygon=None,\n            fast=False,\n        )\n\n\nif __name__ == \"__main__\":\n    print(\"starting export_geowiki()...\")\n    export_geowiki()\n    print(\"Done export_geowiki()!\")\n    print(\"starting process_geowiki()...\")\n    #process_geowiki()\n    print(\"Done process_geowiki()!\")\n    print(\"starting export_geowiki_sentinel_ee()...this could take a while\")\n    export_geowiki_sentinel_ee()\n    print(\"Done export_geowiki_sentinel_ee()!\")\n    print(\"starting process_plantvillage()...\")\n    #process_plantvillage()\n    print(\"Done process_plantvillage()!\")\n    print(\"starting export_plant_village_sentinel_ee()...\")\n    export_plant_village_sentinel_ee()\n    print(\"Done export_plant_village_sentinel_ee()!\")\n    print(\"starting process_kenya_noncrop()...\")\n    #process_kenya_noncrop()\n    print(\"Done process_kenya_noncrop()!\")\n    print(\"starting export_kenya_non_crop()...\")\n    #export_kenya_non_crop()\n    print(\"Done export_kenya_non_crop()!\")\n    print(\"starting export_region()...\")\n    #export_region()\n    print(\"Done export_region()!\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"scripts_export.py\", line 9, in <module>\n    from src_exporters_sentinel_geowiki import *\n  File \"/Users/uhhmed/gw-workspace/cz010n7xyou/src_exporters_sentinel_geowiki.py\", line 1, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907391216,
  "history_end_time" : 1655907403024,
  "history_notes" : null,
  "history_process" : "gpetwx",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0t611wjopqi",
  "history_input" : "from pathlib import Path\n\nfrom typing import Any, Dict\n\n\nclass BaseExporter:\n    r\"\"\"Base for all exporter classes. It creates the appropriate\n    directory in the data dir (``data_dir/raw/{dataset}``).\n\n    All classes which extend this should implement an export function.\n\n    :param data_folder (pathlib.Path, optional)``: The location of the data folder.\n            Default: ``pathlib.Path(\"data\")``\n    \"\"\"\n\n    dataset: str\n    default_args_dict: Dict[str, Any] = {}\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n\n        self.data_folder = data_folder\n\n        self.raw_folder = self.data_folder / \"raw\"\n        self.output_folder = self.raw_folder / self.dataset\n        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
  "history_output" : "",
  "history_begin_time" : 1655907393957,
  "history_end_time" : 1655907403029,
  "history_notes" : null,
  "history_process" : "4q2yxd",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gsykjglqpvx",
  "history_input" : "from pathlib import Path\nimport urllib.request\nimport zipfile\n\nfrom src_exporters_base import BaseExporter\n\n\nclass GeoWikiExporter(BaseExporter):\n    r\"\"\"\n    Download the GeoWiki labels\n    \"\"\"\n\n    dataset = \"geowiki_landcover_2017\"\n\n    download_urls = [\n        \"http://store.pangaea.de/Publications/See_2017/crop_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/crop_exp.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_all_2.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_con.zip\",\n        \"http://store.pangaea.de/Publications/See_2017/loc_exp.zip\",\n    ]\n\n    @staticmethod\n    def download_file(url: str, output_folder: Path, remove_zip: bool = True) -> None:\n\n        filename = url.split(\"/\")[-1]\n        output_path = output_folder / filename\n\n        if output_path.exists():\n            print(f\"{filename} already exists! Skipping\")\n            return None\n\n        print(f\"Downloading {url}\")\n        urllib.request.urlretrieve(url, output_path)\n\n        if filename.endswith(\"zip\"):\n\n            print(f\"Downloaded! Unzipping to {output_folder}\")\n            with zipfile.ZipFile(output_path, \"r\") as zip_file:\n                zip_file.extractall(output_folder)\n\n            if remove_zip:\n                print(\"Deleting zip file\")\n                (output_path).unlink()\n\n    def export(self, remove_zip: bool = False) -> None:\n        r\"\"\"\n        Download the GeoWiki labels\n        :param remove_zip: Whether to remove the zip file once it has been expanded\n        \"\"\"\n        for file_url in self.download_urls:\n            self.download_file(file_url, self.output_folder, remove_zip)\n",
  "history_output" : "",
  "history_begin_time" : 1655907393957,
  "history_end_time" : 1655907403031,
  "history_notes" : null,
  "history_process" : "jonz77",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wqxddbw2crc",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403032,
  "history_notes" : null,
  "history_process" : "dmf4zo",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zr3iobciwje",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudFreeKeepThresh,\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(calcCloudStats)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef calcCloudStats(img):\n    imgPoly = ee.Algorithms.GeometryConstructors.Polygon(\n        ee.Geometry(img.get(\"system:footprint\")).coordinates()\n    )\n\n    roi = ee.Geometry(img.get(\"ROI\"))\n\n    intersection = roi.intersection(imgPoly, ee.ErrorMargin(0.5))\n    cloudMask = img.select([\"cloudScore\"]).gt(cloudThresh).clip(roi).rename(\"cloudMask\")\n\n    cloudAreaImg = cloudMask.multiply(ee.Image.pixelArea())\n\n    stats = cloudAreaImg.reduceRegion(\n        **{\"reducer\": ee.Reducer.sum(), \"geometry\": roi, \"scale\": 10, \"maxPixels\": 1e12}\n    )\n\n    cloudPercent = ee.Number(stats.get(\"cloudMask\")).divide(imgPoly.area()).multiply(100)\n    coveragePercent = ee.Number(intersection.area()).divide(roi.area()).multiply(100)\n    cloudPercentROI = ee.Number(stats.get(\"cloudMask\")).divide(roi.area()).multiply(100)\n\n    img = img.set(\"CLOUDY_PERCENTAGE\", cloudPercent)\n    img = img.set(\"ROI_COVERAGE_PERCENT\", coveragePercent)\n    img = img.set(\"CLOUDY_PERCENTAGE_ROI\", cloudPercentROI)\n\n    return img\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.mean(), \"kernel\": ee.Kernel.square(5)}\n    )\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n    score = (\n        score.reproject(\"EPSG:4326\", None, 20)\n        .focal_min(**{\"radius\": erodePixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .focal_max(**{\"radius\": dilationPixels, \"kernelType\": \"circle\", \"iterations\": 3})\n        .reproject(\"EPSG:4326\", None, 20)\n    )\n\n    return score\n\n\ndef mergeCollection(imgC):\n    # Select the best images, which are below the cloud free threshold, sort them in reverse order\n    # (worst on top) for mosaicing\n    best = imgC.filterMetadata(\"CLOUDY_PERCENTAGE\", \"less_than\", cloudFreeKeepThresh).sort(\n        \"CLOUDY_PERCENTAGE\", False\n    )\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n\n    # Add the quality mosaic to fill in any missing areas of the ROI which aren't covered by good\n    # images\n    newC = ee.ImageCollection.fromImages([filtered, best.mosaic()])\n\n    return ee.Image(newC.mosaic())\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_exporters_sentinel_cloudfree_cloudfree.py\", line 1, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1655907391747,
  "history_end_time" : 1655907403054,
  "history_notes" : null,
  "history_process" : "nph7xo",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3467mlf3fk7",
  "history_input" : "# These are algorithm settings for the cloud filtering algorithm\nimage_collection = \"COPERNICUS/S2\"\n\n# Ranges from 0-1.Lower value will mask more pixels out.\n# Generally 0.1-0.3 works well with 0.2 being used most commonly\ncloudThresh = 0.2\n# Height of clouds to use to project cloud shadows\ncloudHeights = [200, 10000, 250]\n# Sum of IR bands to include as shadows within TDOM and the\n# shadow shift method (lower number masks out less)\nirSumThresh = 0.3\nndviThresh = -0.1\n# Pixels to reduce cloud mask and dark shadows by to reduce inclusion\n# of single-pixel comission errors\nerodePixels = 1.5\ndilationPixels = 3\n\n# images with less than this many cloud pixels will be used with normal\n# mosaicing (most recent on top)\ncloudFreeKeepThresh = 3\n\nBANDS = [\n    \"B1\",\n    \"B2\",\n    \"B3\",\n    \"B4\",\n    \"B5\",\n    \"B6\",\n    \"B7\",\n    \"B8\",\n    \"B8A\",\n    \"B9\",\n    \"B10\",\n    \"B11\",\n    \"B12\",\n]\n",
  "history_output" : "",
  "history_begin_time" : 1655907391216,
  "history_end_time" : 1655907403060,
  "history_notes" : null,
  "history_process" : "jsnayl",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tag4oe1c5xj",
  "history_input" : "import ee\nfrom datetime import date\nimport math\n\nfrom src_exporters_sentinel_cloudfree_constants import (\n    cloudHeights,\n    cloudThresh,\n    ndviThresh,\n    irSumThresh,\n    erodePixels,\n    dilationPixels,\n    image_collection,\n)\nfrom src_exporters_sentinel_cloudfree_utils import date_to_string, rescale\n\n\ndef get_single_image(region: ee.Geometry, start_date: date, end_date: date) -> ee.Image:\n\n    dates = ee.DateRange(date_to_string(start_date), date_to_string(end_date),)\n\n    startDate = ee.DateRange(dates).start()\n    endDate = ee.DateRange(dates).end()\n    imgC = ee.ImageCollection(image_collection).filterDate(startDate, endDate).filterBounds(region)\n\n    imgC = (\n        imgC.map(lambda x: x.clip(region))\n        .map(lambda x: x.set(\"ROI\", region))\n        .map(computeS2CloudScore)\n        .map(projectShadows)\n        .map(computeQualityScore)\n        .sort(\"CLOUDY_PIXEL_PERCENTAGE\")\n    )\n\n    cloudFree = mergeCollection(imgC)\n\n    return cloudFree\n\n\ndef computeQualityScore(img):\n    score = img.select([\"cloudScore\"]).max(img.select([\"shadowScore\"]))\n\n    score = score.reproject(\"EPSG:4326\", None, 20).reduceNeighborhood(\n        reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5), optimization=\"boxcar\"\n    )\n\n    score = score.multiply(-1)\n\n    return img.addBands(score.rename(\"cloudShadowScore\"))\n\n\ndef computeS2CloudScore(img):\n    toa = img.select(\n        [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B10\", \"B11\", \"B12\",]\n    ).divide(10000)\n\n    toa = toa.addBands(img.select([\"QA60\"]))\n\n    # ['QA60', 'B1','B2',    'B3',    'B4',   'B5','B6','B7', 'B8','  B8A',\n    #  'B9',          'B10', 'B11','B12']\n    # ['QA60','cb', 'blue', 'green', 'red', 're1','re2','re3','nir', 'nir2',\n    #  'waterVapor', 'cirrus','swir1', 'swir2']);\n\n    # Compute several indicators of cloudyness and take the minimum of them.\n    score = ee.Image(1)\n\n    # Clouds are reasonably bright in the blue and cirrus bands.\n    score = score.min(rescale(toa, \"img.B2\", [0.1, 0.5]))\n    score = score.min(rescale(toa, \"img.B1\", [0.1, 0.3]))\n    score = score.min(rescale(toa, \"img.B1 + img.B10\", [0.15, 0.2]))\n\n    # Clouds are reasonably bright in all visible bands.\n    score = score.min(rescale(toa, \"img.B4 + img.B3 + img.B2\", [0.2, 0.8]))\n\n    # Clouds are moist\n    ndmi = img.normalizedDifference([\"B8\", \"B11\"])\n    score = score.min(rescale(ndmi, \"img\", [-0.1, 0.1]))\n\n    # However, clouds are not snow.\n    ndsi = img.normalizedDifference([\"B3\", \"B11\"])\n    score = score.min(rescale(ndsi, \"img\", [0.8, 0.6]))\n\n    # Clip the lower end of the score\n    score = score.max(ee.Image(0.001))\n\n    # score = score.multiply(dilated)\n    score = score.reduceNeighborhood(reducer=ee.Reducer.mean(), kernel=ee.Kernel.square(5))\n\n    return img.addBands(score.rename(\"cloudScore\"))\n\n\ndef projectShadows(image):\n    meanAzimuth = image.get(\"MEAN_SOLAR_AZIMUTH_ANGLE\")\n    meanZenith = image.get(\"MEAN_SOLAR_ZENITH_ANGLE\")\n\n    cloudMask = image.select([\"cloudScore\"]).gt(cloudThresh)\n\n    # Find dark pixels\n    darkPixelsImg = image.select([\"B8\", \"B11\", \"B12\"]).divide(10000).reduce(ee.Reducer.sum())\n\n    ndvi = image.normalizedDifference([\"B8\", \"B4\"])\n    waterMask = ndvi.lt(ndviThresh)\n\n    darkPixels = darkPixelsImg.lt(irSumThresh)\n\n    # Get the mask of pixels which might be shadows excluding water\n    darkPixelMask = darkPixels.And(waterMask.Not())\n    darkPixelMask = darkPixelMask.And(cloudMask.Not())\n\n    # Find where cloud shadows should be based on solar geometry\n    # Convert to radians\n    azR = ee.Number(meanAzimuth).add(180).multiply(math.pi).divide(180.0)\n    zenR = ee.Number(meanZenith).multiply(math.pi).divide(180.0)\n\n    # Find the shadows\n    def getShadows(cloudHeight):\n        cloudHeight = ee.Number(cloudHeight)\n\n        shadowCastedDistance = zenR.tan().multiply(cloudHeight)  # Distance shadow is cast\n        x = azR.sin().multiply(shadowCastedDistance).multiply(-1)  # /X distance of shadow\n        y = azR.cos().multiply(shadowCastedDistance).multiply(-1)  # Y distance of shadow\n        return image.select([\"cloudScore\"]).displace(\n            ee.Image.constant(x).addBands(ee.Image.constant(y))\n        )\n\n    shadows = ee.List(cloudHeights).map(getShadows)\n    shadowMasks = ee.ImageCollection.fromImages(shadows)\n    shadowMask = shadowMasks.mean()\n\n    # Create shadow mask\n    shadowMask = dilatedErossion(shadowMask.multiply(darkPixelMask))\n\n    shadowScore = shadowMask.reduceNeighborhood(\n        **{\"reducer\": ee.Reducer.max(), \"kernel\": ee.Kernel.square(1)}\n    )\n\n    image = image.addBands(shadowScore.rename([\"shadowScore\"]))\n\n    return image\n\n\ndef dilatedErossion(score):\n    # Perform opening on the cloud scores\n\n    def erode(img, distance):\n        d = (\n            img.Not()\n            .unmask(1)\n            .fastDistanceTransform(30)\n            .sqrt()\n            .multiply(ee.Image.pixelArea().sqrt())\n        )\n        return img.updateMask(d.gt(distance))\n\n    def dilate(img, distance):\n        d = img.fastDistanceTransform(30).sqrt().multiply(ee.Image.pixelArea().sqrt())\n        return d.lt(distance)\n\n    score = score.reproject(\"EPSG:4326\", None, 20)\n    score = erode(score, erodePixels)\n    score = dilate(score, dilationPixels)\n\n    return score.reproject(\"EPSG:4326\", None, 20)\n\n\ndef mergeCollection(imgC):\n    filtered = imgC.qualityMosaic(\"cloudShadowScore\")\n    return filtered\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_exporters_sentinel_cloudfree_fast.py\", line 1, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1655907391217,
  "history_end_time" : 1655907403063,
  "history_notes" : null,
  "history_process" : "yqt708",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sdppityj3ya",
  "history_input" : "r\"\"\"\nFunctions shared by both the fast and slow\ncloudfree algorithm\n\"\"\"\nimport ee\nfrom datetime import date\nfrom src_exporters_sentinel_cloudfree_constants import BANDS\n\nfrom typing import Union\n\n\ndef combine_bands(current, previous):\n    # Transforms an Image Collection with 1 band per Image into a single Image with items as bands\n    # Author: Jamie Vleeshouwer\n\n    # Rename the band\n    previous = ee.Image(previous)\n    current = current.select(BANDS)\n    # Append it to the result (Note: only return current item on first element/iteration)\n    return ee.Algorithms.If(\n        ee.Algorithms.IsEqual(previous, None), current, previous.addBands(ee.Image(current)),\n    )\n\n\ndef export(\n    image: ee.Image, region: ee.Geometry, filename: str, drive_folder: str, monitor: bool = False,\n) -> ee.batch.Export:\n\n    task = ee.batch.Export.image(\n        image.clip(region),\n        filename,\n        {\"scale\": 10, \"region\": region, \"maxPixels\": 1e13, \"driveFolder\": drive_folder},\n    )\n\n    try:\n        task.start()\n    except ee.ee_exception.EEException as e:\n        print(f\"Task not started! Got exception {e}\")\n        return task\n\n    if monitor:\n        monitor_task(task)\n\n    return task\n\n\ndef date_to_string(input_date: Union[date, str]) -> str:\n    if isinstance(input_date, str):\n        return input_date\n    else:\n        assert isinstance(input_date, date)\n        return input_date.strftime(\"%Y-%m-%d\")\n\n\ndef monitor_task(task: ee.batch.Export) -> None:\n\n    while task.status()[\"state\"] in [\"READY\", \"RUNNING\"]:\n        print(task.status())\n        # print(f\"Running: {task.status()['state']}\")\n\n\ndef rescale(img, exp, thresholds):\n    return (\n        img.expression(exp, {\"img\": img})\n        .subtract(thresholds[0])\n        .divide(thresholds[1] - thresholds[0])\n    )\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_exporters_sentinel_cloudfree_utils.py\", line 5, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1655907391217,
  "history_end_time" : 1655907403065,
  "history_notes" : null,
  "history_process" : "q5a232",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mwacy6lods0",
  "history_input" : "# The probability threshold to use to label GeoWiki\n# instances as crop / not_crop (since the GeoWiki labels are a mean crop probability, as\n# assigned by several labellers). In addition, this is the threshold used when calculating\n# metrics which require binary predictions, such as accuracy score\nPROBABILITY_THRESHOLD = 0.5\n",
  "history_output" : "",
  "history_begin_time" : 1655907393979,
  "history_end_time" : 1655907403067,
  "history_notes" : null,
  "history_process" : "nt17bz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e9nnag49lhc",
  "history_input" : "import torch\nimport numpy as np\nimport random\n\nfrom dataclasses import dataclass\n\n\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n\n@dataclass\nclass BoundingBox:\n\n    min_lon: float\n    max_lon: float\n    min_lat: float\n    max_lat: float\n\n\nSTR2BB = {\n    \"Kenya\": BoundingBox(min_lon=33.501, max_lon=42.283, min_lat=-5.202, max_lat=6.002),\n    \"Busia\": BoundingBox(\n        min_lon=33.88389587402344,\n        min_lat=-0.04119872691853491,\n        max_lon=34.44007873535156,\n        max_lat=0.7779454563313616,\n    ),\n}\n",
  "history_output" : "",
  "history_begin_time" : 1655907399616,
  "history_end_time" : 1655907403069,
  "history_notes" : null,
  "history_process" : "o5t3jb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "47vaaizr7qa",
  "history_input" : "import pandas as pd\nimport xarray as xr\nfrom datetime import date\nfrom tqdm import tqdm\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass GeoWikiSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_geowiki\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        geowiki = self.data_folder / \"processed\" / GeoWikiExporter.dataset / \"data.nc\"\n        assert geowiki.exists(), \"GeoWiki processor must be run to load labels\"\n        return xr.open_dataset(geowiki).to_dataframe().dropna().reset_index()\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        start_date: date = date(2017, 3, 28),\n        end_date: date = date(2018, 3, 28),\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        Run the GeoWiki exporter. For each label, the exporter will export\n        int( (end_date - start_date).days / days_per_timestep) timesteps of data,\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param start_date: The start data of the data export\n        :param end_date: The end date of the data export\n        :param num_labelled_points: (Optional) The number of labelled points to export.\n        :param surrounding_metres: The number of metres surrounding each labelled point to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        assert start_date >= self.min_date, f\"Sentinel data does not exist before {self.min_date}\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        for idx, bounding_box in enumerate(bounding_boxes_to_download):\n            self._export_for_polygon(\n                polygon=bounding_box.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_exporters_sentinel_geowiki.py\", line 1, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907396276,
  "history_end_time" : 1655907403071,
  "history_notes" : null,
  "history_process" : "mw544v",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "08gquktukdw",
  "history_input" : "from abc import ABC, abstractmethod\nfrom datetime import date, timedelta\nfrom pathlib import Path\nimport pandas as pd\nimport ee\n\nfrom src_exporters_sentinel_cloudfree_cloudfree import *\nfrom src_exporters_sentinel_cloudfree_fast import get_single_image as get_single_image_fast\nfrom src_exporters_base import BaseExporter\nfrom src_exporters_sentinel_cloudfree_utils import *\n\n\nfrom typing import List, Union\n\n\nclass BaseSentinelExporter(BaseExporter, ABC):\n\n    r\"\"\"\n    Download cloud free sentinel data for countries,\n    where countries are defined by the simplified large scale\n    international boundaries.\n    \"\"\"\n\n    dataset: str\n    min_date = date(2017, 3, 28)\n\n    def __init__(self, data_folder: Path = Path(\"data\")) -> None:\n        super().__init__(data_folder)\n        try:\n            ee.Initialize()\n        except Exception:\n            print(\"This code doesn't work unless you have authenticated your earthengine account\")\n\n        self.labels = self.load_labels()\n\n    @abstractmethod\n    def load_labels(self) -> pd.DataFrame:\n        raise NotImplementedError\n\n    def _export_for_polygon(\n        self,\n        polygon: ee.Geometry.Polygon,\n        polygon_identifier: Union[int, str],\n        start_date: date,\n        end_date: date,\n        days_per_timestep: int,\n        checkpoint: bool,\n        monitor: bool,\n        fast: bool,\n    ) -> None:\n\n        if fast:\n            export_func = get_single_image_fast\n        else:\n            export_func = get_single_image\n\n        cur_date = start_date\n        cur_end_date = cur_date + timedelta(days=days_per_timestep)\n\n        image_collection_list: List[ee.Image] = []\n\n        print(\n            f\"Exporting image for polygon {polygon_identifier} from \"\n            f\"aggregated images between {str(cur_date)} and {str(end_date)}\"\n        )\n        filename = f\"{polygon_identifier}_{str(cur_date)}_{str(end_date)}\"\n\n        if checkpoint and (self.output_folder / f\"{filename}.tif\").exists():\n            print(\"File already exists! Skipping\")\n            return None\n\n        while cur_end_date <= end_date:\n\n            image_collection_list.append(\n                export_func(region=polygon, start_date=cur_date, end_date=cur_end_date)\n            )\n            cur_date += timedelta(days=days_per_timestep)\n            cur_end_date += timedelta(days=days_per_timestep)\n\n        # now, we want to take our image collection and append the bands into a single image\n        imcoll = ee.ImageCollection(image_collection_list)\n        img = ee.Image(imcoll.iterate(combine_bands))\n\n        # and finally, export the image\n        export(\n            image=img,\n            region=polygon,\n            filename=filename,\n            drive_folder=self.dataset,\n            monitor=monitor,\n        )\n",
  "history_output" : "",
  "history_begin_time" : 1655907398528,
  "history_end_time" : 1655907403072,
  "history_notes" : null,
  "history_process" : "vxuj3q",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rv75hw6j5lj",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nfrom datetime import timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_kenya_non_crop import KenyaNonCropProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre\n\nfrom typing import Optional, List\n\n\nclass KenyaNonCropSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_kenya_non_crop\"\n\n    # data collection date\n    data_date = date(2020, 4, 16)\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        non_crop = self.data_folder / \"processed\" / KenyaNonCropProcessor.dataset / \"data.geojson\"\n        assert non_crop.exists(), \"Kenya non crop processor must be run to load labels\"\n        return geopandas.read_file(non_crop)[[\"lat\", \"lon\"]]\n\n    def labels_to_bounding_boxes(\n        self, num_labelled_points: Optional[int], surrounding_metres: int\n    ) -> List[EEBoundingBox]:\n\n        output: List[EEBoundingBox] = []\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            output.append(\n                bounding_box_from_centre(\n                    mid_lat=row[\"lat\"], mid_lon=row[\"lon\"], surrounding_metres=surrounding_metres,\n                ),\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def export_for_labels(\n        self,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points, surrounding_metres=surrounding_metres,\n        )\n\n        start_date = self.data_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            self._export_for_polygon(\n                polygon=bounding_info.to_ee_polygon(),\n                polygon_identifier=idx,\n                start_date=start_date,\n                end_date=self.data_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_exporters_sentinel_kenya_non_crop.py\", line 1, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907394001,
  "history_end_time" : 1655907403074,
  "history_notes" : null,
  "history_process" : "nlb6f5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wxy4vuucr8e",
  "history_input" : "import pandas as pd\nimport geopandas\nfrom tqdm import tqdm\nimport numpy as np\nfrom datetime import datetime, timedelta, date\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_processors_pv_kenya import KenyaPVProcessor\nfrom src_exporters_sentinel_utils import EEBoundingBox, bounding_box_from_centre, date_overlap\n\nfrom typing import Dict, Optional, List, Tuple\n\n\nclass KenyaPVSentinelExporter(BaseSentinelExporter):\n\n    dataset = \"earth_engine_plant_village_kenya\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # right now, this just loads geowiki data. In the future,\n        # it would be neat to merge all labels together\n        plantvillage = self.data_folder / \"processed\" / KenyaPVProcessor.dataset / \"data.geojson\"\n        assert plantvillage.exists(), \"Plant Village processor must be run to load labels\"\n        return geopandas.read_file(plantvillage)[\n            [\"lat\", \"lon\", \"index\", \"planting_date\", \"harvest_date\"]\n        ]\n\n    @staticmethod\n    def overlapping_year(\n        end_month: int, num_days: int, harvest_date: date, planting_date: date\n    ) -> Tuple[Optional[int], Optional[int]]:\n        r\"\"\"\n        Return the end_year of the most overlapping years\n        \"\"\"\n        harvest_year = harvest_date.year\n\n        overlap_dict: Dict[int, int] = {}\n\n        for diff in range(-1, 2):\n            end_date = date(harvest_year + diff, end_month, 1)\n\n            if end_date > datetime.now().date():\n                continue\n            else:\n                overlap_dict[harvest_year + diff] = date_overlap(\n                    planting_date, harvest_date, end_date - timedelta(days=num_days), end_date,\n                )\n        if len(overlap_dict) > 0:\n            return max(overlap_dict.items(), key=lambda x: x[1])\n        else:\n            # sometimes the harvest date is in the future? in which case\n            # we will just skip the datapoint for now\n            return None, None\n\n    def labels_to_bounding_boxes(\n        self,\n        num_labelled_points: Optional[int],\n        surrounding_metres: int,\n        end_month_day: Optional[Tuple[int, int]],\n        num_days: int,\n    ) -> List[Tuple[int, EEBoundingBox, date, Optional[int]]]:\n\n        output: List[Tuple[int, EEBoundingBox, date, Optional[int]]] = []\n\n        if end_month_day is not None:\n            end_month: Optional[int]\n            end_day: Optional[int]\n            end_month, end_day = end_month_day\n        else:\n            end_month = end_day = None\n\n        for idx, row in tqdm(self.labels.iterrows()):\n\n            try:\n                harvest_date = datetime.strptime(row[\"harvest_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n            except ValueError:\n                continue\n\n            # this is only used if end_month is not None\n            overlapping_days: Optional[int] = 0\n            if end_month is not None:\n                planting_date = datetime.strptime(row[\"planting_date\"], \"%Y-%m-%dT%H:%M:%S\").date()\n\n                end_year, overlapping_days = self.overlapping_year(\n                    end_month, num_days, harvest_date, planting_date\n                )\n\n                if end_year is None:\n                    continue\n\n                if end_day is None:\n                    # if no end_day is passed, we will take the first month\n                    end_day = 1\n                harvest_date = date(end_year, end_month, end_day)\n\n            output.append(\n                (\n                    row[\"index\"],\n                    bounding_box_from_centre(\n                        mid_lat=row[\"lat\"],\n                        mid_lon=row[\"lon\"],\n                        surrounding_metres=surrounding_metres,\n                    ),\n                    harvest_date,\n                    overlapping_days,\n                )\n            )\n\n            if num_labelled_points is not None:\n                if len(output) >= num_labelled_points:\n                    return output\n        return output\n\n    def get_start_and_end_dates(\n        self, harvest_date: date, days_per_timestep: int, num_timesteps: int\n    ) -> Optional[Tuple[date, date]]:\n\n        if harvest_date < self.min_date:\n            print(\"Harvest date < min date - skipping\")\n            return None\n        else:\n            start_date = max(\n                harvest_date - timedelta(days_per_timestep * num_timesteps), self.min_date,\n            )\n            end_date = start_date + timedelta(days_per_timestep * num_timesteps)\n\n            return start_date, end_date\n\n    def export_for_labels(\n        self,\n        end_month_day: Optional[Tuple[int, int]] = (4, 16),\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        num_labelled_points: Optional[int] = None,\n        surrounding_metres: int = 80,\n        checkpoint: bool = True,\n        monitor: bool = False,\n        fast: bool = True,\n    ) -> None:\n        r\"\"\"\n        :param end_month_day: The final month-day to use. If None is passed, the harvest date\n            will be used. Default = (4, 16)\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n            Default = 30\n        :param num_timesteps: The number of timesteps to export. Default = 12\n        :param num_labelled_points: If not None, then only this many points will be exported.\n            Default = None.\n        :param surrouning_metres: The patch will be [2 * surrounding_metres,\n            2 * surrounding_metres], centered around the labelled point. Default = 80\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it. Default = True\n        :param monitor: Whether to monitor each task until it has been run. Default = True\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n\n        bounding_boxes_to_download = self.labels_to_bounding_boxes(\n            num_labelled_points=num_labelled_points,\n            surrounding_metres=surrounding_metres,\n            end_month_day=end_month_day,\n            num_days=days_per_timestep * num_timesteps,\n        )\n\n        if end_month_day is not None:\n            print(\n                f\"Average overlapping days between planting to harvest and \"\n                f\"export dates: {np.mean([x[3] for x in bounding_boxes_to_download])}\"\n            )\n        for idx, bounding_info in enumerate(bounding_boxes_to_download):\n\n            harvest_date = bounding_info[-2]\n\n            dates = self.get_start_and_end_dates(harvest_date, days_per_timestep, num_timesteps)\n\n            if dates is not None:\n\n                self._export_for_polygon(\n                    polygon=bounding_info[1].to_ee_polygon(),\n                    polygon_identifier=bounding_info[0],\n                    start_date=dates[0],\n                    end_date=dates[1],\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n",
  "history_output" : "",
  "history_begin_time" : 1655907396305,
  "history_end_time" : 1655907403079,
  "history_notes" : null,
  "history_process" : "i4s7l1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "opi0ixicqzf",
  "history_input" : "from datetime import date, timedelta\nimport pandas as pd\n\nfrom src_exporters_sentinel_base import BaseSentinelExporter\nfrom src_exporters_sentinel_utils import bounding_box_to_earth_engine_bounding_box\nfrom src_utils import STR2BB\n\nfrom typing import Optional\n\n\nclass RegionalExporter(BaseSentinelExporter):\n    r\"\"\"\n    This is useful if you are trying to export\n    full regions for predictions\n    \"\"\"\n\n    dataset = \"earth_engine_region_busia_partial_slow_cloudfree\"\n\n    def load_labels(self) -> pd.DataFrame:\n        # We don't need any labels for this exporter,\n        # so we can return an empty dataframe\n        return pd.DataFrame()\n\n    def export_for_region(\n        self,\n        region_name: str,\n        end_date: date,\n        days_per_timestep: int = 30,\n        num_timesteps: int = 12,\n        checkpoint: bool = True,\n        monitor: bool = True,\n        metres_per_polygon: Optional[int] = 10000,\n        fast: bool = True,\n    ):\n        r\"\"\"\n        Run the regional exporter. For each label, the exporter will export\n        data from (end_date - timedelta(days=days_per_timestep * num_timesteps)) to end_date\n        where each timestep consists of a mosaic of all available images within the\n        days_per_timestep of that timestep.\n        :param region_name: The name of the region to export. This must be defined in\n            src.utils.STR2BB\n        :param end_date: The end date of the data export\n        :param days_per_timestep: The number of days of data to use for each mosaiced image.\n        :param num_timesteps: The number of timesteps to export\n        :param checkpoint: Whether or not to check in self.data_folder to see if the file has\n            already been exported. If it has, skip it\n        :param monitor: Whether to monitor each task until it has been run\n        :param metres_per_polygon: Whether to split the export of a large region into smaller\n            boxes of (max) area metres_per_polygon * metres_per_polygon. It is better to instead\n            split the area once it has been exported\n        :param fast: Whether to use the faster cloudfree exporter. This function is considerably\n            faster, but cloud artefacts can be more pronounced. Default = True\n        \"\"\"\n        start_date = end_date - num_timesteps * timedelta(days=days_per_timestep)\n\n        region = bounding_box_to_earth_engine_bounding_box(STR2BB[region_name])\n\n        if metres_per_polygon is not None:\n\n            regions = region.to_polygons(metres_per_patch=metres_per_polygon)\n\n            for idx, region in enumerate(regions):\n                self._export_for_polygon(\n                    polygon=region,\n                    polygon_identifier=f\"{idx}-{region_name}\",\n                    start_date=start_date,\n                    end_date=end_date,\n                    days_per_timestep=days_per_timestep,\n                    checkpoint=checkpoint,\n                    monitor=monitor,\n                    fast=fast,\n                )\n        else:\n            self._export_for_polygon(\n                polygon=region.to_ee_polygon(),\n                polygon_identifier=region_name,\n                start_date=start_date,\n                end_date=end_date,\n                days_per_timestep=days_per_timestep,\n                checkpoint=checkpoint,\n                monitor=monitor,\n                fast=fast,\n            )\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_exporters_sentinel_region.py\", line 2, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907397412,
  "history_end_time" : 1655907403084,
  "history_notes" : null,
  "history_process" : "9c0ch9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "o0pn8kplvqa",
  "history_input" : "from pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cartopy.crs as ccrs\nfrom datetime import datetime\nimport xarray as xr\n\n\nfrom src_engineer_base import BaseEngineer\n\n\ndef sentinel_as_tci(sentinel_ds: xr.DataArray, scale: bool = True) -> xr.DataArray:\n    r\"\"\"\n    Get a True Colour Image from Sentinel data exported from Earth Engine\n    :param sentinel_ds: The sentinel data, exported from Earth Engine\n    :param scale: Whether or not to add the factor 10,000 scale\n    :return: A dataframe with true colour bands\n    \"\"\"\n\n    band2idx = {band: idx for idx, band in enumerate(sentinel_ds.attrs[\"band_descriptions\"])}\n\n    tci_bands = [\"B4\", \"B3\", \"B2\"]\n    tci_indices = [band2idx[band] for band in tci_bands]\n    if scale:\n        return sentinel_ds.isel(band=tci_indices) / 10000 * 2.5\n    else:\n        return sentinel_ds.isel(band=tci_indices) * 2.5\n\n\ndef plot_results(model_preds: xr.Dataset, tci_path: Path, savepath: Path, prefix: str = \"\") -> None:\n\n    multi_output = len(model_preds.data_vars) > 1\n\n    tci = sentinel_as_tci(\n        BaseEngineer.load_tif(tci_path, start_date=datetime(2020, 1, 1), days_per_timestep=30),\n        scale=False,\n    ).isel(time=-1)\n\n    tci = tci.sortby(\"x\").sortby(\"y\")\n    model_preds = model_preds.sortby(\"lat\").sortby(\"lon\")\n\n    plt.clf()\n    fig, ax = plt.subplots(1, 3, figsize=(20, 7.5), subplot_kw={\"projection\": ccrs.PlateCarree()})\n\n    fig.suptitle(\n        f\"Model results for tile with bottom left corner:\"\n        f\"\\nat latitude {float(model_preds.lat.min())}\"\n        f\"\\n and longitude {float(model_preds.lon.min())}\",\n        fontsize=15,\n    )\n    # ax 1 - original\n    img_extent_1 = (tci.x.min(), tci.x.max(), tci.y.min(), tci.y.max())\n    img = np.clip(np.moveaxis(tci.values, 0, -1), 0, 1)\n\n    ax[0].set_title(\"True colour image\")\n    ax[0].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict = {\n        \"origin\": \"upper\",\n        \"extent\": img_extent_1,\n        \"transform\": ccrs.PlateCarree(),\n    }\n\n    if multi_output:\n        mask = np.argmax(model_preds.to_array().values, axis=0)\n\n        # currently, we have 10 classes (at most). It seems unlikely we will go\n        # above 20\n        args_dict[\"cmap\"] = plt.cm.get_cmap(\"tab20\", len(model_preds.data_vars))\n    else:\n        mask = model_preds.prediction_0\n        args_dict.update({\"vmin\": 0, \"vmax\": 1})\n\n    # ax 2 - mask\n    ax[1].set_title(\"Mask\")\n    im = ax[1].imshow(mask, **args_dict)\n\n    # finally, all together\n    ax[2].set_title(\"Mask on top of the true colour image\")\n    ax[2].imshow(img, origin=\"upper\", extent=img_extent_1, transform=ccrs.PlateCarree())\n\n    args_dict[\"alpha\"] = 0.3\n    if not multi_output:\n        mask = mask > 0.5\n    ax[2].imshow(mask, **args_dict)\n\n    colorbar_args = {\n        \"ax\": ax.ravel().tolist(),\n    }\n\n    if multi_output:\n        # This function formatter will replace integers with target names\n        formatter = plt.FuncFormatter(lambda val, loc: list(model_preds.data_vars)[val])\n        colorbar_args.update({\"ticks\": range(len(model_preds.data_vars)), \"format\": formatter})\n\n    # We must be sure to specify the ticks matching our target names\n    fig.colorbar(im, **colorbar_args)\n\n    plt.savefig(savepath / f\"results_{prefix}{tci_path.name}.png\", bbox_inches=\"tight\", dpi=300)\n    plt.close()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_analysis.py\", line 2, in <module>\n    import matplotlib.pyplot as plt\nModuleNotFoundError: No module named 'matplotlib'\n",
  "history_begin_time" : 1655907398522,
  "history_end_time" : 1655907403087,
  "history_notes" : null,
  "history_process" : "qdzo28",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ge8yx5ywlex",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_processors_geowiki import *\nfrom src_processors_kenya_non_crop import *\nfrom src_processors_pv_kenya import *\n\ndef process_geowiki():\n    processor = GeoWikiProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_plantvillage():\n    processor = KenyaPVProcessor(Path(\"../data\"))\n    processor.process()\n\n\ndef process_kenya_noncrop():\n    processor = KenyaNonCropProcessor(Path(\"../data\"))\n    processor.process()\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...process.py\")\n    process_geowiki()\n    process_plantvillage()\n    #process_kenya_noncrop()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"scripts_process.py\", line 6, in <module>\n    from src_processors_geowiki import *\n  File \"/Users/uhhmed/gw-workspace/ge8yx5ywlex/src_processors_geowiki.py\", line 1, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907399557,
  "history_end_time" : 1655907403090,
  "history_notes" : null,
  "history_process" : "iticjd",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "b0stvaj7ddv",
  "history_input" : "import sys\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_engineer_geowiki import GeoWikiEngineer\nfrom src_engineer_pv_kenya import PVKenyaEngineer\nfrom src_engineer_kenya_non_crop import KenyaNonCropEngineer\n\n\ndef engineer_geowiki():\n    engineer = GeoWikiEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.2)\n\n\ndef engineer_kenya():\n    engineer = PVKenyaEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.1, test_set_size=0.1)\n\n\ndef engineer_kenya_noncrop():\n    engineer = KenyaNonCropEngineer(Path(\"../data\"))\n    engineer.engineer(val_set_size=0.1, test_set_size=0.1)\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...engineer.py\")  \n    engineer_geowiki()\n    engineer_kenya()\n    #engineer_kenya_noncrop()",
  "history_output" : "Traceback (most recent call last):\n  File \"scripts_engineer.py\", line 6, in <module>\n    from src_engineer_geowiki import GeoWikiEngineer\n  File \"/Users/uhhmed/gw-workspace/b0stvaj7ddv/src_engineer_geowiki.py\", line 3, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907399816,
  "history_end_time" : 1655907403092,
  "history_notes" : null,
  "history_process" : "3cars9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kggmvun1e2o",
  "history_input" : "import sys\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nsys.path.append(\"..\")\n\nfrom src_models_model import Model\nfrom src_models_train_funcs import train_model\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...model.py\")\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--max_epochs\", type=int, default=1000)\n    parser.add_argument(\"--patience\", type=int, default=10)\n\n    model_args = Model.add_model_specific_args(parser).parse_args()\n    model = Model(model_args)\n\n    train_model(model, model_args)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"scripts_model.py\", line 7, in <module>\n    from src_models_model import Model\n  File \"/Users/uhhmed/gw-workspace/kggmvun1e2o/src_models_model.py\", line 3, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n",
  "history_begin_time" : 1655907400759,
  "history_end_time" : 1655907403093,
  "history_notes" : null,
  "history_process" : "r5a5p5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cwwrilhp2zc",
  "history_input" : "from pathlib import Path\nimport sys\nimport os\n\nsys.path.append(\"..\")\n\nfrom src_models_model import Model\nfrom src_analysis import plot_results\n\n\ndef kenya_crop_type_mapper():\n    data_dir = \"../data\"\n\n    test_folder = Path(\"../data/raw/earth_engine_plant_village_kenya/\")\n    test_files = test_folder.glob(\"*.tif\")\n    print(test_files)\n\n    list_of_models = list(Path('../data/lightning_logs/').glob('version*/checkpoints/*.ckpt'))\n    latest_model_path = str(max(list_of_models, key=os.path.getctime))\n    print(f\"Using model {latest_model_path}\")\n\n    model = Model.load_from_checkpoint(latest_model_path)\n\n    for test_path in test_files:\n\n        save_dir = Path(data_dir) / \"Autoencoder\"\n        save_dir.mkdir(exist_ok=True)\n\n        print(f\"Running for {test_path}\")\n\n        savepath = save_dir / f\"preds_{test_path.name}\"\n        if savepath.exists():\n            print(\"File already generated. Skipping\")\n            continue\n\n        out_forecasted = model.predict(test_path, with_forecaster=True)\n        plot_results(out_forecasted, test_path, savepath=save_dir, prefix=\"forecasted_\")\n\n        out_normal = model.predict(test_path, with_forecaster=False)\n        plot_results(out_normal, test_path, savepath=save_dir, prefix=\"full_input_\")\n\n        out_forecasted.to_netcdf(save_dir / f\"preds_forecasted_{test_path.name}.nc\")\n        out_normal.to_netcdf(save_dir / f\"preds_normal_{test_path.name}.nc\")\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...predict.py\")\n    kenya_crop_type_mapper()\n",
  "history_output" : "",
  "history_begin_time" : 1655907401758,
  "history_end_time" : 1655907403095,
  "history_notes" : null,
  "history_process" : "delykw",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "km98o4udkwo",
  "history_input" : "import pytorch_lightning as pl\nfrom pathlib import Path\nfrom argparse import ArgumentParser\nimport os\n\nimport sys\n\nsys.path.append(\"..\")\nfrom src_models_model import Model\n\n\ndef get_checkpoint(data_folder: Path) -> str:\n\n    log_folder = data_folder / \"lightning_logs/\" \n    list_of_checkpoints = list(log_folder.glob('version*/checkpoints/*.ckpt'))\n    print(log_folder.absolute())\n    return str(max(list_of_checkpoints, key=os.path.getctime))\n\n\ndef test_model():\n    parser = ArgumentParser()\n\n    parser.add_argument(\"--version\", type=int, default=0)\n\n    args = parser.parse_args()\n\n    model_path = get_checkpoint(Path(\"../data\"))\n\n    print(f\"Using model {model_path}\")\n\t\n    model = Model.load_from_checkpoint(model_path)\n\n    trainer = pl.Trainer()\n    trainer.test(model)\n\n\nif __name__ == \"__main__\":\n    print(\"Starting...test.py\")\n    test_model()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"scripts_test.py\", line 1, in <module>\n    import pytorch_lightning as pl\nModuleNotFoundError: No module named 'pytorch_lightning'\n",
  "history_begin_time" : 1655907402019,
  "history_end_time" : 1655907403110,
  "history_notes" : null,
  "history_process" : "q1j13t",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l1o6bjczmz8",
  "history_input" : null,
  "history_output" : "",
  "history_begin_time" : 1655907403016,
  "history_end_time" : 1655907403159,
  "history_notes" : null,
  "history_process" : "6nnond",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ckroog6r53m",
  "history_input" : "import pandas as pd\n\nfrom src_processors_base import BaseProcessor\n\n\nclass GeoWikiProcessor(BaseProcessor):\n\n    dataset = \"geowiki_landcover_2017\"\n\n    def load_raw_data(self, participants: str) -> pd.DataFrame:\n\n        participants_to_file_labels = {\n            \"all\": \"all\",\n            \"students\": \"con\",\n            \"experts\": \"exp\",\n        }\n\n        file_label = participants_to_file_labels.get(participants, participants)\n        assert (\n            file_label in participants_to_file_labels.values()\n        ), f\"Unknown participant {file_label}\"\n\n        return pd.read_csv(\n            self.raw_folder / f\"loc_{file_label}{'_2' if file_label == 'all' else ''}.txt\",\n            sep=\"\\t\",\n        )\n\n    def process(self, participants: str = \"all\") -> None:\n\n        location_data = self.load_raw_data(participants)\n\n        # first, we find the mean sumcrop calculated per location\n        mean_per_location = (\n            location_data[[\"location_id\", \"sumcrop\", \"loc_cent_X\", \"loc_cent_Y\"]]\n            .groupby(\"location_id\")\n            .mean()\n        )\n\n        # then, we rename the columns\n        mean_per_location = mean_per_location.rename(\n            {\"loc_cent_X\": \"lon\", \"loc_cent_Y\": \"lat\", \"sumcrop\": \"mean_sumcrop\"},\n            axis=\"columns\",\n            errors=\"raise\",\n        )\n        # then, we turn it into an xarray with x and y as indices\n        output_xr = (\n            mean_per_location.reset_index().set_index([\"lon\", \"lat\"])[\"mean_sumcrop\"].to_xarray()\n        )\n\n        # and save\n        output_xr.to_netcdf(self.output_folder / \"data.nc\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_processors_geowiki.py\", line 1, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907400751,
  "history_end_time" : 1655907403122,
  "history_notes" : null,
  "history_process" : "m6v1cg",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bz44orjnnq2",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403125,
  "history_notes" : null,
  "history_process" : "m9myzm",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lxgjczg35ad",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403143,
  "history_notes" : null,
  "history_process" : "jlyr32",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h3f6wzyu3t2",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import datetime\nimport pandas as pd\nfrom pathlib import Path\nimport xarray as xr\n\nfrom typing import Optional\n\nfrom src_exporters_geowiki import GeoWikiExporter\nfrom src_exporters_sentinel_geowiki import GeoWikiSentinelExporter\nfrom src_config import PROBABILITY_THRESHOLD\nfrom src_engineer_base import BaseEngineer, DataInstance\n\n\nclass GeoWikiEngineer(BaseEngineer):\n\n    sentinel_dataset = GeoWikiSentinelExporter.dataset\n    dataset = GeoWikiExporter.dataset\n\n    @staticmethod\n    def read_labels(data_folder: Path) -> pd.DataFrame:\n        geowiki = data_folder / \"processed\" / GeoWikiExporter.dataset / \"data.nc\"\n        assert geowiki.exists(), \"GeoWiki processor must be run to load labels\"\n        return xr.open_dataset(geowiki).to_dataframe().dropna().reset_index()\n\n    def process_single_file(\n        self,\n        path_to_file: Path,\n        nan_fill: float,\n        max_nan_ratio: float,\n        add_ndvi: bool,\n        add_ndwi: bool,\n        calculate_normalizing_dict: bool,\n        start_date: datetime,\n        days_per_timestep: int,\n        is_test: bool,\n    ) -> Optional[DataInstance]:\n        r\"\"\"\n        Return a tuple of np.ndarrays of shape [n_timesteps, n_features] for\n        1) the anchor (labelled)\n        \"\"\"\n\n        da = self.load_tif(path_to_file, days_per_timestep=days_per_timestep, start_date=start_date)\n\n        # first, we find the label encompassed within the da\n\n        min_lon, min_lat = float(da.x.min()), float(da.y.min())\n        max_lon, max_lat = float(da.x.max()), float(da.y.max())\n        overlap = self.labels[\n            (\n                (self.labels.lon <= max_lon)\n                & (self.labels.lon >= min_lon)\n                & (self.labels.lat <= max_lat)\n                & (self.labels.lat >= min_lat)\n            )\n        ]\n        if len(overlap) == 0:\n            return None\n\n        label_lat = overlap.iloc[0].lat\n        label_lon = overlap.iloc[0].lon\n\n        # we turn the percentage into a fraction\n        crop_probability = overlap.iloc[0].mean_sumcrop / 100\n\n        closest_lon, _ = self.find_nearest(da.x, label_lon)\n        closest_lat, _ = self.find_nearest(da.y, label_lat)\n\n        labelled_np = da.sel(x=closest_lon).sel(y=closest_lat).values\n\n        if add_ndvi:\n            labelled_np = self.calculate_ndvi(labelled_np)\n        if add_ndwi:\n            labelled_np = self.calculate_ndwi(labelled_np)\n\n        labelled_array = self.maxed_nan_to_num(labelled_np, nan=nan_fill, max_ratio=max_nan_ratio)\n\n        if (not is_test) and calculate_normalizing_dict:\n            # we won't use the neighbouring array for now, since tile2vec is\n            # not really working\n            self.update_normalizing_values(self.normalizing_dict_interim, labelled_array)\n\n        if labelled_array is not None:\n            return DataInstance(\n                label_lat=label_lat,\n                label_lon=label_lon,\n                instance_lat=closest_lat,\n                instance_lon=closest_lon,\n                labelled_array=labelled_array,\n                is_crop=crop_probability >= PROBABILITY_THRESHOLD,\n                dataset=self.dataset,\n            )\n        else:\n            return None\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_engineer_geowiki.py\", line 3, in <module>\n    import pandas as pd\nModuleNotFoundError: No module named 'pandas'\n",
  "history_begin_time" : 1655907401752,
  "history_end_time" : 1655907403173,
  "history_notes" : null,
  "history_process" : "rus783",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u77bnyqo5pn",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403209,
  "history_notes" : null,
  "history_process" : "s024ve",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sm0rvln84qn",
  "history_input" : "from argparse import ArgumentParser, Namespace\n\nimport pytorch_lightning as pl\nimport torch\nfrom torch import nn\n\nfrom typing import Any, Dict, List, Union, Tuple, Type\nfrom src_models_lstm import UnrolledLSTM\n\n\nclass Classifier(pl.LightningModule):\n    r\"\"\"\n    An LSTM based model to predict the presence of cropland in a pixel.\n\n    :param input_size: The number of input bands passed to the model. The\n        input vector is expected to be of shape [batch_size, timesteps, bands]\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.classifier_vector_size: The size of the hidden vector in the LSTM base\n        (and therefore of the first classification layer). Default = 128\n    :param hparams.classifier_base_layers: The number of LSTM base layers to use. Default = 1\n    :param hparams.classifier_dropout: Variational dropout ratio to apply between timesteps in\n        the LSTM base. Default = 0.2\n    :param hparams.num_global_layers: The number of classification layers to use on the global\n        (GeoWiki) dataset. Default = 1\n    :param hparams.num_local_layers: The number of classification layers to use on the local\n        (Kenya) dataset. Default = 2\n    :param hparams.multi_headed: Whether or not to add a local head, to classify instances within\n        Togo. If False, the same classification layer will be used to classify\n        all pixels. Default = True\n    \"\"\"\n\n    def __init__(self, input_size: int, hparams: Namespace,) -> None:\n        super().__init__()\n\n        self.hparams = hparams\n\n        self.base = nn.ModuleList(\n            [\n                UnrolledLSTM(\n                    input_size=input_size if i == 0 else hparams.classifier_vector_size,\n                    hidden_size=hparams.classifier_vector_size,\n                    dropout=hparams.classifier_dropout,\n                    batch_first=True,\n                )\n                for i in range(hparams.classifier_base_layers)\n            ]\n        )\n\n        self.batchnorm = nn.BatchNorm1d(num_features=self.hparams.classifier_vector_size)\n\n        global_classification_layers: List[nn.Module] = []\n        num_global_layers = hparams.num_global_layers\n        print(f\"Using {num_global_layers} layers for the global classifier\")\n        for i in range(num_global_layers):\n            global_classification_layers.append(\n                nn.Linear(\n                    in_features=hparams.classifier_vector_size,\n                    out_features=1\n                    if i == (num_global_layers - 1)\n                    else hparams.classifier_vector_size,\n                    bias=True if i == 0 else False,\n                )\n            )\n            if i < (num_global_layers - 1):\n                global_classification_layers.append(nn.ReLU())\n                global_classification_layers.append(\n                    nn.BatchNorm1d(num_features=hparams.classifier_vector_size)\n                )\n\n        self.global_classifier = nn.Sequential(*global_classification_layers)\n\n        if self.hparams.multi_headed:\n\n            num_local_layers = hparams.num_local_layers\n            print(f\"Using {num_local_layers} layers for the local classifier\")\n            local_classification_layers: List[nn.Module] = []\n            for i in range(num_local_layers):\n                local_classification_layers.append(\n                    nn.Linear(\n                        in_features=hparams.classifier_vector_size,\n                        out_features=1\n                        if i == (num_local_layers - 1)\n                        else hparams.classifier_vector_size,\n                        bias=True if i == 0 else False,\n                    )\n                )\n                if i < (num_local_layers - 1):\n                    local_classification_layers.append(nn.ReLU())\n                    local_classification_layers.append(\n                        nn.BatchNorm1d(num_features=hparams.classifier_vector_size,)\n                    )\n\n            self.local_classifier = nn.Sequential(*local_classification_layers)\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n\n        for _, lstm in enumerate(self.base):\n            x, (hn, _) = lstm(x)\n            x = x[:, 0, :, :]\n\n        base = self.batchnorm(hn[-1, :, :])\n        x_global = torch.sigmoid(self.global_classifier(base))\n\n        if self.hparams.multi_headed:\n            x_local = torch.sigmoid(self.local_classifier(base))\n            return x_global, x_local\n        else:\n            return x_global\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--classifier_vector_size\": (int, 128),\n            \"--classifier_base_layers\": (int, 1),\n            \"--classifier_dropout\": (float, 0.2),\n            \"--num_global_layers\": (int, 1),\n            \"--num_local_layers\": (int, 2),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--multi_headed\", dest=\"multi_headed\", action=\"store_true\")\n        parser.add_argument(\"--not_multi_headed\", dest=\"multi_headed\", action=\"store_false\")\n        parser.set_defaults(multi_headed=True)\n\n        return parser\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_classifier.py\", line 3, in <module>\n    import pytorch_lightning as pl\nModuleNotFoundError: No module named 'pytorch_lightning'\n",
  "history_begin_time" : 1655907403968,
  "history_end_time" : 1655907404159,
  "history_notes" : null,
  "history_process" : "m3tvgo",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "3h8sst91yp0",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : 1655907402979,
  "history_end_time" : 1655907403242,
  "history_notes" : null,
  "history_process" : "9x9elz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qrsdlna77op",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403244,
  "history_notes" : null,
  "history_process" : "a3ucnn",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vdcrz3cq62l",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : 1655907402969,
  "history_end_time" : 1655907403255,
  "history_notes" : null,
  "history_process" : "t2liev",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vdlcofwac4b",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403257,
  "history_notes" : null,
  "history_process" : "1whsg3",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ok89fm3rwic",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1655907403263,
  "history_notes" : null,
  "history_process" : "7jza5a",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q08rl146af7",
  "history_input" : "from argparse import ArgumentParser, Namespace\nfrom pathlib import Path\nimport numpy as np\nimport xarray as xr\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nimport pytorch_lightning as pl\n\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    mean_absolute_error,\n)\n\nfrom src_models_data import CropDataset\nfrom src_models_utils import tif_to_np, preds_to_xr\nfrom src_utils import set_seed\nfrom src_models_forecaster import Forecaster\nfrom src_models_classifier import Classifier\nfrom src_config import PROBABILITY_THRESHOLD\n\nfrom typing import cast, Callable, Tuple, Dict, Any, Type, Optional, List, Union\n\n\nclass Model(pl.LightningModule):\n    r\"\"\"\n    An model for annual and in-season crop mapping. This model consists of a\n    forecaster.Forecaster and a classifier.Classifier - it will require the arguments\n    required by those models too.\n\n    hparams\n    --------\n    The default values for these parameters are set in add_model_specific_args\n\n    :param hparams.data_folder: The path to the data. Default (assumes the model\n        is being run from the scripts directory) = \"../data\"\n    :param hparams.learning_rate: The learning rate. Default = 0.001\n    :param hparams.batch_size: The batch size. Default = 64\n    :param hparams.input_months: The number of input months to pass to the model. If\n        hparams.forecast is True, the remaining months will be forecasted. Otherwise, only the\n        partial timeseries will be passed to the classifier. Default = 5\n    :param hparams.alpha: The weight to use when adding the global and local losses. This\n        parameter is only used if hparams.multi_headed is True. Default = 10\n    :param hparams.noise_factor: The standard deviation of the random noise to add to the\n        raw inputs to the classifier. Default = 0.1\n    :param hparams.remove_b1_b10: Whether or not to remove the B1 and B10 bands. Default = True\n    :param hparams.forecast: Whether or not to forecast the partial time series. Default = True\n    :param hparams.cache: Whether to load all the data into memory during training. Default = True\n    :param hparams.include_geowiki: Whether to include the global GeoWiki dataset during\n        training. Default = True\n    :param hparams.upsample: Whether to oversample the under-represented class so that each class\n        is equally represented in the training and validation dataset. Default = True\n    \"\"\"\n\n    def __init__(self, hparams: Namespace) -> None:\n        super().__init__()\n        set_seed()\n        self.hparams = hparams\n\n        self.data_folder = Path(hparams.data_folder)\n\n        dataset = self.get_dataset(subset=\"training\", cache=False)\n        self.num_outputs = dataset.num_output_classes\n        self.num_timesteps = dataset.num_timesteps\n        self.input_size = dataset.num_input_features\n\n        # we save the normalizing dict because we calculate weighted\n        # normalization values based on the datasets we combine.\n        # The number of instances per dataset (and therefore the weights) can\n        # vary between the train / test / val sets - this ensures the normalizing\n        # dict stays constant between them\n        self.normalizing_dict = dataset.normalizing_dict\n\n        if self.hparams.forecast:\n            num_output_timesteps = self.num_timesteps - self.hparams.input_months\n            print(\n                f\"Predicting {num_output_timesteps} timesteps in the forecaster\")\n            self.forecaster = Forecaster(\n                num_bands=self.input_size, output_timesteps=num_output_timesteps, hparams=hparams,\n            )\n\n            self.forecaster_loss = F.smooth_l1_loss\n\n        self.classifier = Classifier(\n            input_size=self.input_size, hparams=hparams)\n        self.global_loss_function: Callable = F.binary_cross_entropy\n        self.local_loss_function: Callable = F.binary_cross_entropy\n\n    def forward(self, x: torch.Tensor) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        # To keep the ABC happy\n        return self.classifier(x)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n    def get_dataset(\n        self, subset: str, normalizing_dict: Optional[Dict] = None, cache: Optional[bool] = None,\n    ) -> CropDataset:\n        return CropDataset(\n            data_folder=self.data_folder,\n            subset=subset,\n            remove_b1_b10=self.hparams.remove_b1_b10,\n            normalizing_dict=normalizing_dict,\n            include_geowiki=self.hparams.include_geowiki if subset != \"testing\" else False,\n            cache=self.hparams.cache if cache is None else cache,\n            upsample=self.hparams.upsample if subset != \"testing\" else False,\n            noise_factor=self.hparams.noise_factor if subset != \"testing\" else 0,\n        )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.get_dataset(subset=\"training\"), shuffle=True, batch_size=self.hparams.batch_size,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.get_dataset(subset=\"validation\",\n                             normalizing_dict=self.normalizing_dict,),\n            batch_size=self.hparams.batch_size,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.get_dataset(subset=\"testing\",\n                             normalizing_dict=self.normalizing_dict,),\n            batch_size=self.hparams.batch_size,\n        )\n\n    def predict(\n        self,\n        path_to_file: Path,\n        with_forecaster: bool,\n        batch_size: int = 64,\n        add_ndvi: bool = True,\n        add_ndwi: bool = False,\n        nan_fill: float = 0,\n        days_per_timestep: int = 30,\n        local_head: bool = True,\n        use_gpu: bool = True,\n    ) -> xr.Dataset:\n\n        # check if a GPU is available, and if it is\n        # move the model onto the GPU\n        device: Optional[torch.device] = None\n        if use_gpu:\n            use_cuda = torch.cuda.is_available()\n            if not use_cuda:\n                print(\"No GPU - not using one\")\n            device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n            self.to(device)\n\n        self.eval()\n\n        input_data = tif_to_np(\n            path_to_file,\n            add_ndvi=add_ndvi,\n            add_ndwi=add_ndwi,\n            nan=nan_fill,\n            normalizing_dict=self.normalizing_dict,\n            days_per_timestep=days_per_timestep,\n        )\n\n        if with_forecaster:\n            input_data.x = input_data.x[:, : self.hparams.input_months, :]\n\n        predictions: List[np.ndarray] = []\n        cur_i = 0\n\n        pbar = tqdm(total=input_data.x.shape[0] - 1)\n        while cur_i < (input_data.x.shape[0] - 1):\n\n            batch_x_np = input_data.x[cur_i: cur_i + batch_size]\n            if self.hparams.remove_b1_b10:\n                batch_x_np = CropDataset._remove_bands(batch_x_np)\n            batch_x = torch.from_numpy(batch_x_np).float()\n\n            if use_gpu and (device is not None):\n                batch_x = batch_x.to(device)\n\n            with torch.no_grad():\n                if with_forecaster:\n                    batch_x_next = self.forecaster(batch_x)\n                    batch_x = torch.cat((batch_x, batch_x_next), dim=1)\n\n                batch_preds = self.classifier(batch_x)\n\n                if self.hparams.multi_headed:\n                    global_preds, local_preds = batch_preds\n\n                    if local_head:\n                        batch_preds = local_preds\n                    else:\n                        batch_preds = global_preds\n\n                # back to the CPU, if necessary\n                batch_preds = batch_preds.cpu()\n\n            predictions.append(cast(torch.Tensor, batch_preds).numpy())\n            cur_i += batch_size\n            pbar.update(batch_size)\n\n        all_preds = np.concatenate(predictions, axis=0)\n        if len(all_preds.shape) == 1:\n            all_preds = np.expand_dims(all_preds, axis=-1)\n\n        return preds_to_xr(all_preds, lats=input_data.lat, lons=input_data.lon,)\n\n    def _output_metrics(\n        self, preds: np.ndarray, labels: np.ndarray, prefix: str = \"\"\n    ) -> Dict[str, float]:\n\n        if len(preds) == 0:\n            # sometimes this happens in the warmup\n            return {}\n\n        output_dict: Dict[str, float] = {}\n        if not (labels == labels[0]).all():\n            # This can happen when lightning does its warm up on a subset of the\n            # validation data\n            output_dict[f\"{prefix}roc_auc_score\"] = roc_auc_score(\n                labels, preds)\n\n        preds = (preds > PROBABILITY_THRESHOLD).astype(int)\n\n        output_dict[f\"{prefix}precision_score\"] = precision_score(\n            labels, preds)\n        output_dict[f\"{prefix}recall_score\"] = recall_score(labels, preds)\n        output_dict[f\"{prefix}f1_score\"] = f1_score(labels, preds)\n        output_dict[f\"{prefix}accuracy\"] = accuracy_score(labels, preds)\n\n        return output_dict\n\n    def add_noise(self, x: torch.Tensor, training: bool) -> torch.Tensor:\n        if (self.hparams.noise_factor == 0) or (not training):\n            return x\n\n        # expect input to be of shape [timesteps, bands]\n        # and to be normalized with mean 0, std=1\n        # if its not, it means no norm_dict was passed, so lets\n        # just assume std=1\n        noise = torch.normal(0, 1, size=x.shape).float() * \\\n            self.hparams.noise_factor\n\n        # the added noise is the same per band, so that the temporal relationships\n        # are preserved\n        # noise_per_timesteps = noise.repeat(x.shape[0], 1)\n        return x + noise\n\n    def _split_preds_and_get_loss(\n        self, batch, add_preds: bool, loss_label: str, log_loss: bool, training: bool\n    ) -> Dict:\n\n        x, label, is_global = batch\n\n        input_to_encode = x[:, : self.hparams.input_months, :]\n\n        if self.hparams.forecast:\n            # we will predict every timestep except the first one\n            output_to_predict = x[:, 1:, :]\n            encoder_output = self.forecaster(input_to_encode)\n            encoder_loss = self.forecaster_loss(\n                encoder_output, output_to_predict)\n            loss: Union[float, torch.Tensor] = encoder_loss\n\n            final_encoded_input = torch.cat(\n                (\n                    (\n                        self.add_noise(input_to_encode, training),\n                        # -1 because the encoder output has no value for the 0th\n                        # timestep\n                        encoder_output[:, self.hparams.input_months - 1:, :],\n                    )\n                ),\n                dim=1,\n            )\n\n            output_dict = {}\n            if add_preds:\n                output_dict.update(\n                    {\"encoder_prediction\": encoder_output,\n                        \"encoder_target\": output_to_predict, }\n                )\n            if log_loss:\n                output_dict[\"log\"] = {}\n\n            # we now repeat label and is_global\n            x = torch.cat((self.add_noise(x, training),\n                          final_encoded_input), dim=0)\n            label = torch.cat((label, label), dim=0)\n            is_global = torch.cat((is_global, is_global), dim=0)\n        else:\n            loss = 0\n            output_dict = {}\n            if log_loss:\n                output_dict[\"log\"] = {}\n            x = self.add_noise(input_to_encode, training=training)\n\n        if self.hparams.multi_headed:\n            org_global_preds, local_preds = self.classifier(x)\n            global_preds = org_global_preds[is_global != 0]\n            global_labels = label[is_global != 0]\n\n            local_preds = local_preds[is_global == 0]\n            local_labels = label[is_global == 0]\n\n            if local_preds.shape[0] > 0:\n                local_loss = self.local_loss_function(\n                    local_preds.squeeze(-1), local_labels,)\n                loss += local_loss\n\n            if global_preds.shape[0] > 0:\n                global_loss = self.global_loss_function(\n                    global_preds.squeeze(-1), global_labels,)\n\n                num_local_labels = local_preds.shape[0]\n                if num_local_labels == 0:\n                    alpha = 1\n                else:\n                    ratio = global_preds.shape[0] / num_local_labels\n                    alpha = ratio / self.hparams.alpha\n                loss += alpha * global_loss\n\n            output_dict[loss_label] = loss\n            if log_loss:\n                output_dict[\"log\"][loss_label] = loss\n            if add_preds:\n                output_dict.update(\n                    {\n                        \"global_pred\": global_preds,\n                        \"global_label\": global_labels,\n                        \"kenya_pred\": local_preds,\n                        \"kenya_label\": local_labels,\n                    }\n                )\n            return output_dict\n        else:\n            preds = cast(torch.Tensor, self.classifier(x))\n\n            loss += self.global_loss_function(\n                input=preds.squeeze(-1), target=label,)\n\n            output_dict = {loss_label: loss}\n            if log_loss:\n                output_dict[\"log\"][loss_label] = loss\n            if add_preds:\n                output_dict.update({\"pred\": preds, \"label\": label})\n            return output_dict\n\n    def training_step(self, batch, batch_idx):\n        return self._split_preds_and_get_loss(\n            batch, add_preds=False, loss_label=\"loss\", log_loss=True, training=True\n        )\n\n    def validation_step(self, batch, batch_idx):\n        return self._split_preds_and_get_loss(\n            batch, add_preds=True, loss_label=\"val_loss\", log_loss=True, training=False\n        )\n\n    def test_step(self, batch, batch_idx):\n        return self._split_preds_and_get_loss(\n            batch, add_preds=True, loss_label=\"test_loss\", log_loss=True, training=False\n        )\n\n    @staticmethod\n    def _split_tensor(outputs, label) -> Tuple[np.ndarray, np.ndarray]:\n        encoded_all, unencoded_all = [], []\n        for x in outputs:\n            # the first half is unencoded, the second is encoded\n            total = x[label]\n            unencoded_all.append(total[: total.shape[0] // 2])\n            encoded_all.append(total[total.shape[0] // 2:])\n        return (\n            torch.cat(unencoded_all).detach().cpu().numpy(),\n            torch.cat(encoded_all).detach().cpu().numpy(),\n        )\n\n    def _interpretable_metrics(self, outputs, input_prefix: str, output_prefix: str) -> Dict:\n\n        output_dict = {}\n\n        if self.hparams.forecast:\n            u_labels, e_labels = self._split_tensor(\n                outputs, f\"{input_prefix}label\")\n\n            u_preds, e_preds = self._split_tensor(\n                outputs, f\"{input_prefix}pred\")\n        else:\n            u_preds = torch.cat([x[f\"{input_prefix}pred\"]\n                                for x in outputs]).detach().cpu().numpy()\n            u_labels = (\n                torch.cat([x[f\"{input_prefix}label\"]\n                          for x in outputs]).detach().cpu().numpy()\n            )\n\n        output_dict.update(\n            self._output_metrics(\n                u_preds, u_labels, f\"unencoded_{output_prefix}{input_prefix}\")\n        )\n\n        if self.hparams.forecast:\n            output_dict.update(\n                self._output_metrics(\n                    e_preds, e_labels, f\"encoded_{output_prefix}{input_prefix}\")\n            )\n\n        return output_dict\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        logs = {\"val_loss\": avg_loss}\n        if self.hparams.forecast:\n            encoder_pred = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_prediction\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n            encoder_target = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_target\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n\n            logs[\"val_encoder_mae\"] = mean_absolute_error(\n                encoder_target, encoder_pred)\n\n        if self.hparams.multi_headed:\n            logs.update(self._interpretable_metrics(\n                outputs, \"global_\", \"val_\"))\n            logs.update(self._interpretable_metrics(outputs, \"kenya_\", \"val_\"))\n        else:\n            logs.update(self._interpretable_metrics(outputs, \"\", \"val_\"))\n        return {\"val_loss\": avg_loss, \"log\": logs}\n\n    def test_epoch_end(self, outputs):\n\n        avg_loss = torch.stack([x[\"test_loss\"] for x in outputs]).mean().item()\n        output_dict = {\"val_loss\": avg_loss}\n\n        if self.hparams.forecast:\n            encoder_pred = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_prediction\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n            encoder_target = (\n                torch.cat(\n                    [torch.flatten(x[\"encoder_target\"], start_dim=1) for x in outputs], dim=0,\n                )\n                .detach()\n                .cpu()\n                .numpy()\n            )\n\n            output_dict[\"test_encoder_mae\"] = mean_absolute_error(\n                encoder_target, encoder_pred)\n\n        if self.hparams.multi_headed:\n            output_dict.update(self._interpretable_metrics(\n                outputs, \"global_\", \"test_\"))\n            output_dict.update(self._interpretable_metrics(\n                outputs, \"kenya_\", \"test_\"))\n        else:\n            output_dict.update(\n                self._interpretable_metrics(outputs, \"\", \"test_\"))\n\n        return {\"progress_bar\": output_dict}\n\n    @staticmethod\n    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n\n        parser_args: Dict[str, Tuple[Type, Any]] = {\n            # assumes this is being run from \"scripts\"\n            \"--data_folder\": (str, str(Path(\"../data\"))),\n            \"--learning_rate\": (float, 0.001),\n            \"--batch_size\": (int, 64),\n            \"--input_months\": (int, 5),\n            \"--alpha\": (float, 10),\n            \"--noise_factor\": (float, 0.1),\n        }\n\n        for key, val in parser_args.items():\n            parser.add_argument(key, type=val[0], default=val[1])\n\n        parser.add_argument(\"--remove_b1_b10\",\n                            dest=\"remove_b1_b10\", action=\"store_true\")\n        parser.add_argument(\n            \"--keep_b1_b10\", dest=\"remove_b1_b10\", action=\"store_false\")\n        parser.set_defaults(remove_b1_b10=True)\n\n        parser.add_argument(\"--forecast\", dest=\"forecast\", action=\"store_true\")\n        parser.add_argument(\"--do_not_forecast\",\n                            dest=\"forecast\", action=\"store_false\")\n        parser.set_defaults(forecast=True)\n\n        parser.add_argument(\"--cache\", dest=\"cache\", action=\"store_true\")\n        parser.add_argument(\"--do_not_cache\", dest=\"cache\",\n                            action=\"store_false\")\n        parser.set_defaults(cache=True)\n\n        parser.add_argument(\"--include_geowiki\",\n                            dest=\"include_geowiki\", action=\"store_true\")\n        parser.add_argument(\"--exclude_geowiki\",\n                            dest=\"include_geowiki\", action=\"store_false\")\n        parser.set_defaults(include_geowiki=True)\n\n        parser.add_argument(\"--upsample\", dest=\"upsample\", action=\"store_true\")\n        parser.add_argument(\"--do_not_upsample\",\n                            dest=\"upsample\", action=\"store_false\")\n        parser.set_defaults(upsample=True)\n\n        classifier_parser = Classifier.add_model_specific_args(parser)\n        return Forecaster.add_model_specific_args(classifier_parser)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_model.py\", line 3, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n",
  "history_begin_time" : 1655907395297,
  "history_end_time" : 1655907403268,
  "history_notes" : null,
  "history_process" : "9ardvx",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zjjsunsvl4l",
  "history_input" : "from argparse import Namespace\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n\ndef train_model(model: pl.LightningModule, hparams: Namespace) -> pl.LightningModule:\n    early_stop_callback = EarlyStopping(\n        monitor=\"val_loss\", min_delta=0.00, patience=hparams.patience, verbose=True, mode=\"min\",\n    )\n    trainer = pl.Trainer(\n        default_save_path=hparams.data_folder,\n        max_epochs=hparams.max_epochs,\n        early_stop_callback=early_stop_callback,\n    )\n    trainer.fit(model)\n\n    return model\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_train_funcs.py\", line 3, in <module>\n    import pytorch_lightning as pl\nModuleNotFoundError: No module named 'pytorch_lightning'\n",
  "history_begin_time" : 1655907396240,
  "history_end_time" : 1655907403270,
  "history_notes" : null,
  "history_process" : "o0vujj",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0or15c733fs",
  "history_input" : "from dataclasses import dataclass\nfrom datetime import datetime\nfrom pathlib import Path\nimport numpy as np\nimport xarray as xr\nimport pandas as pd\n\nfrom src_engineer_base import BaseEngineer\n\nfrom typing import cast, Dict, Optional, Tuple\n\n\n@dataclass\nclass TestInstance:\n    x: np.ndarray\n    lat: np.ndarray\n    lon: np.ndarray\n\n\ndef tif_to_np(\n    path_to_dataset: Path,\n    add_ndvi: bool,\n    add_ndwi: bool,\n    nan: float,\n    normalizing_dict: Optional[Dict[str, np.ndarray]],\n    days_per_timestep: int,\n) -> TestInstance:\n\n    _, start_date, _ = cast(\n        Tuple[str, datetime, datetime],\n        BaseEngineer.process_filename(path_to_dataset.name, include_extended_filenames=True),\n    )\n\n    x = BaseEngineer.load_tif(\n        path_to_dataset, days_per_timestep=days_per_timestep, start_date=start_date\n    )\n\n    lon, lat = np.meshgrid(x.x.values, x.y.values)\n    flat_lat, flat_lon = (\n        np.squeeze(lat.reshape(-1, 1), -1),\n        np.squeeze(lon.reshape(-1, 1), -1),\n    )\n\n    x_np = x.values\n    x_np = x_np.reshape(x_np.shape[0], x_np.shape[1], x_np.shape[2] * x_np.shape[3])\n    x_np = np.moveaxis(x_np, -1, 0)\n\n    if add_ndvi:\n        x_np = BaseEngineer.calculate_ndvi(x_np, num_dims=3)\n    if add_ndwi:\n        x_np = BaseEngineer.calculate_ndwi(x_np, num_dims=3)\n\n    x_np = BaseEngineer.maxed_nan_to_num(x_np, nan=nan)\n\n    if normalizing_dict is not None:\n        x_np = (x_np - normalizing_dict[\"mean\"]) / normalizing_dict[\"std\"]\n\n    return TestInstance(x=x_np, lat=flat_lat, lon=flat_lon)\n\n\ndef preds_to_xr(predictions: np.ndarray, lats: np.ndarray, lons: np.ndarray) -> xr.Dataset:\n\n    data_dict: Dict[str, np.ndarray] = {\"lat\": lats, \"lon\": lons}\n\n    for prediction_idx in range(predictions.shape[1]):\n        prediction_label = f\"prediction_{prediction_idx}\"\n        data_dict[prediction_label] = predictions[:, prediction_idx]\n\n    return pd.DataFrame(data=data_dict).set_index([\"lat\", \"lon\"]).to_xarray()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"src_models_utils.py\", line 4, in <module>\n    import numpy as np\nModuleNotFoundError: No module named 'numpy'\n",
  "history_begin_time" : 1655907397538,
  "history_end_time" : 1655907403272,
  "history_notes" : null,
  "history_process" : "bhdtil",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]
